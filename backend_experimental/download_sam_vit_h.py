"""
SAM vit_h ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆGPUç”¨ï¼‰
RTX 3060ã§ã®é«˜é€Ÿãƒ»é«˜ç²¾åº¦æ¨è«–ã®ãŸã‚
"""
import urllib.request
from pathlib import Path
import sys
import hashlib

def download_sam_vit_h():
    """SAM vit_h ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    url = "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
    output_path = Path("sam_vit_h_4b8939.pth")

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
    if output_path.exists():
        size_mb = output_path.stat().st_size / 1024 / 1024
        print(f"âœ… {output_path} already exists ({size_mb:.1f}MB)")

        # ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        expected_size = 2446  # ~2.4GBï¼ˆå®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºï¼‰
        if abs(size_mb - expected_size) > 50:
            print(f"âš ï¸ File size ({size_mb:.1f}MB) differs from expected ({expected_size}MB)")
            response = input("Re-download? (y/n): ")
            if response.lower() != 'y':
                return True
            print("Deleting existing file...")
            output_path.unlink()
        else:
            print("File size OK. Skipping download.")
            return True

    print()
    print("=" * 70)
    print("ğŸ“¥ Downloading SAM vit_h model for GPU inference")
    print("=" * 70)
    print(f"URL: {url}")
    print(f"Output: {output_path}")
    print(f"Size: ~2.56GB")
    print(f"Target GPU: RTX 3060 (12GB VRAM)")
    print()
    print("This will take 5-15 minutes depending on your internet speed...")
    print()

    def progress_hook(block_num, block_size, total_size):
        """é€²æ—ãƒãƒ¼ã®è¡¨ç¤º"""
        downloaded = block_num * block_size
        percent = min(100, downloaded / total_size * 100)
        mb_downloaded = downloaded / 1024 / 1024
        mb_total = total_size / 1024 / 1024

        bar_length = 50
        filled = int(bar_length * percent / 100)
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)

        # æ¨å®šæ®‹ã‚Šæ™‚é–“ï¼ˆç°¡æ˜“è¨ˆç®—ï¼‰
        if block_num > 0:
            speed_mbps = mb_downloaded / (block_num * 0.1)  # ç°¡æ˜“é€Ÿåº¦æ¨å®š
            remaining_mb = mb_total - mb_downloaded
            eta_sec = remaining_mb / speed_mbps if speed_mbps > 0 else 0
            eta_min = eta_sec / 60

            sys.stdout.write(
                f'\r[{bar}] {percent:.1f}% '
                f'({mb_downloaded:.1f}MB / {mb_total:.1f}MB) '
                f'ETA: {eta_min:.1f}min'
            )
        else:
            sys.stdout.write(f'\r[{bar}] {percent:.1f}% ({mb_downloaded:.1f}MB / {mb_total:.1f}MB)')

        sys.stdout.flush()

    try:
        urllib.request.urlretrieve(url, output_path, progress_hook)
        print()
        print()
        print(f"âœ… Download completed: {output_path}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        size_mb = output_path.stat().st_size / 1024 / 1024
        expected_size = 2446  # ~2.4GBï¼ˆå®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºï¼‰

        print(f"   File size: {size_mb:.1f}MB")

        if abs(size_mb - expected_size) > 50:
            print(f"âš ï¸ Warning: File size ({size_mb:.1f}MB) differs from expected ({expected_size}MB)")
            print(f"   The download may be corrupted. Please try again.")
            return False

        print(f"âœ… File size verified")
        return True

    except KeyboardInterrupt:
        print("\n\nâš ï¸ Download interrupted by user")
        if output_path.exists():
            print(f"Cleaning up partial download...")
            output_path.unlink()
        return False

    except Exception as e:
        print(f"\n\nâŒ Download failed: {e}")
        if output_path.exists():
            output_path.unlink()
        return False

def verify_model_file():
    """ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
    model_path = Path("sam_vit_h_4b8939.pth")

    if not model_path.exists():
        print("âŒ Model file not found")
        return False

    print()
    print("=" * 70)
    print("Verifying model file...")
    print("=" * 70)

    try:
        # PyTorchã§ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        import torch

        print("Loading model file with PyTorch...")
        state_dict = torch.load(model_path, map_location='cpu')

        if isinstance(state_dict, dict):
            num_keys = len(state_dict.keys())
            print(f"âœ… Model file is valid PyTorch checkpoint")
            print(f"   Number of parameters: {num_keys}")

            # ã‚µãƒ³ãƒ—ãƒ«ã‚­ãƒ¼ã‚’è¡¨ç¤º
            sample_keys = list(state_dict.keys())[:3]
            print(f"   Sample keys: {sample_keys}")

            return True
        else:
            print(f"âš ï¸ Unexpected format: {type(state_dict)}")
            return False

    except Exception as e:
        print(f"âŒ Model file verification failed: {e}")
        return False

if __name__ == "__main__":
    print()
    print("=" * 70)
    print("SAM vit_h Model Download for GPU (RTX 3060)")
    print("=" * 70)
    print()
    print("This script will download SAM vit_h checkpoint (~2.56GB)")
    print("Required for high-precision GPU inference on RTX 3060")
    print()

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
    success = download_sam_vit_h()

    if success:
        # æ¤œè¨¼
        verify_success = verify_model_file()

        print()
        print("=" * 70)
        if verify_success:
            print("ğŸ‰ Model ready for GPU inference!")
            print()
            print("Next steps:")
            print("1. Install PyTorch with CUDA:")
            print("   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
            print("2. Run check_cuda.py to verify GPU environment")
            print("3. Run test_sam_gpu.py to test SAM vit_h on GPU")
        else:
            print("âš ï¸ Model file may be corrupted")
            print("   Please delete sam_vit_h_4b8939.pth and run this script again")
        print("=" * 70)
    else:
        print()
        print("=" * 70)
        print("âŒ Download failed")
        print("   Please check your internet connection and try again")
        print("=" * 70)
