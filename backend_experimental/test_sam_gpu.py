"""
SAM vit_h GPUæ€§èƒ½ãƒ†ã‚¹ãƒˆï¼ˆRTX 3060ï¼‰
"""
import sys
import time
import numpy as np
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

def test_gpu_availability():
    """GPUåˆ©ç”¨å¯èƒ½æ€§ã®ç¢ºèª"""
    print("=" * 70)
    print("GPUç’°å¢ƒãƒã‚§ãƒƒã‚¯")
    print("=" * 70)

    try:
        import torch
    except ImportError:
        print("âŒ PyTorch not installed")
        print("   Install: pip install torch --index-url https://download.pytorch.org/whl/cu118")
        return False

    if not torch.cuda.is_available():
        print("âŒ CUDA is NOT available")
        print("   Please install PyTorch with CUDA support")
        return False

    print(f"âœ… CUDA available")
    print(f"   PyTorch version: {torch.__version__}")
    print(f"   CUDA version: {torch.version.cuda}")
    print(f"   GPU: {torch.cuda.get_device_name(0)}")

    props = torch.cuda.get_device_properties(0)
    total_vram = props.total_memory / 1024**3
    print(f"   VRAM: {total_vram:.2f} GB")

    return True

def test_sam_vit_h_loading():
    """SAM vit_h GPUèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 70)
    print("SAM vit_h GPUèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)

    try:
        from app.ai_engine.processors.sam_tracker_unified import SAMTrackerUnified
        import torch

        start_time = time.time()
        tracker = SAMTrackerUnified(model_type="vit_h", device="cuda")
        load_time = time.time() - start_time

        print(f"âœ… SAM vit_h loaded on GPU")
        print(f"   Loading time: {load_time:.2f}s")

        allocated_mb = torch.cuda.memory_allocated() / 1024**2
        print(f"   VRAM allocated: {allocated_mb:.1f}MB")

        return tracker

    except FileNotFoundError as e:
        print(f"âŒ Model checkpoint not found")
        print(f"   Run: python download_sam_vit_h.py")
        return None
    except Exception as e:
        print(f"âŒ Loading failed: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_inference_speed():
    """æ¨è«–é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print("\n" + "=" * 70)
    print("æ¨è«–é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯: GPU vs CPU")
    print("=" * 70)

    from app.ai_engine.processors.sam_tracker_unified import SAMTrackerUnified
    import torch

    # ãƒ†ã‚¹ãƒˆç”»åƒï¼ˆå®Ÿéš›ã®å‹•ç”»ã‚µã‚¤ã‚ºï¼‰
    test_image = np.random.randint(0, 255, (620, 1214, 3), dtype=np.uint8)
    test_point = [(607, 310)]

    # GPU ãƒ†ã‚¹ãƒˆ
    print("\nğŸ”¥ GPU (RTX 3060, vit_h) ãƒ†ã‚¹ãƒˆ:")
    try:
        tracker_gpu = SAMTrackerUnified(model_type="vit_h", device="cuda")
        tracker_gpu.set_image(test_image)

        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        _ = tracker_gpu.segment_with_point(test_point, [1])

        # 10å›å®Ÿè¡Œ
        times_gpu = []
        for i in range(10):
            start = time.time()
            result = tracker_gpu.segment_with_point(test_point, [1])
            elapsed = time.time() - start
            times_gpu.append(elapsed)

        avg_gpu = sum(times_gpu) / len(times_gpu)
        print(f"   å¹³å‡æ¨è«–æ™‚é–“: {avg_gpu*1000:.1f}ms")
        print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {1/avg_gpu:.1f} fps")

    except Exception as e:
        print(f"   âŒ GPUæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
        avg_gpu = None

    # CPU ãƒ†ã‚¹ãƒˆï¼ˆæ¯”è¼ƒç”¨ï¼‰
    print("\nğŸ’» CPU (vit_b) ãƒ†ã‚¹ãƒˆ:")
    try:
        tracker_cpu = SAMTrackerUnified(model_type="vit_b", device="cpu")
        tracker_cpu.set_image(test_image)

        _ = tracker_cpu.segment_with_point(test_point, [1])

        times_cpu = []
        for i in range(5):
            start = time.time()
            result = tracker_cpu.segment_with_point(test_point, [1])
            elapsed = time.time() - start
            times_cpu.append(elapsed)

        avg_cpu = sum(times_cpu) / len(times_cpu)
        print(f"   å¹³å‡æ¨è«–æ™‚é–“: {avg_cpu*1000:.1f}ms")
        print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {1/avg_cpu:.1f} fps")

    except Exception as e:
        print(f"   âŒ CPUæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
        avg_cpu = None

    # æ¯”è¼ƒ
    if avg_gpu and avg_cpu:
        speedup = avg_cpu / avg_gpu
        print("\n" + "=" * 70)
        print(f"âš¡ é«˜é€ŸåŒ–ç‡: {speedup:.1f}x")
        print(f"   GPU (vit_h): {avg_gpu*1000:.1f}ms")
        print(f"   CPU (vit_b): {avg_cpu*1000:.1f}ms")
        print("=" * 70)

        if speedup >= 5:
            print("ğŸ‰ RTX 3060ã§5å€ä»¥ä¸Šã®é«˜é€ŸåŒ–é”æˆï¼")
        elif speedup >= 3:
            print("âœ… 3å€ä»¥ä¸Šã®é«˜é€ŸåŒ–ã‚’ç¢ºèª")
        else:
            print("âš ï¸ æœŸå¾…ã‚ˆã‚Šä½é€Ÿã§ã™")

def test_batch_processing():
    """ãƒãƒƒãƒå‡¦ç†æ€§èƒ½ãƒ†ã‚¹ãƒˆï¼ˆæœ€é©åŒ–å‰å¾Œã®æ¯”è¼ƒï¼‰"""
    print("\n" + "=" * 70)
    print("ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆï¼ˆ113ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰- æœ€é©åŒ–åŠ¹æœæ¤œè¨¼")
    print("=" * 70)

    from app.ai_engine.processors.sam_tracker_unified import SAMTrackerUnified
    import torch

    # 113ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†
    frames = [np.random.randint(0, 255, (620, 1214, 3), dtype=np.uint8) for _ in range(113)]

    # å™¨å…·åˆæœŸåŒ–ãƒ‡ãƒ¼ã‚¿
    instruments = [{
        "id": 0,
        "name": "Test Instrument",
        "selection": {
            "type": "point",
            "data": [(607, 310)]
        },
        "color": "#FF0000"
    }]

    # GPU tracker
    tracker = SAMTrackerUnified(model_type="vit_h", device="cuda")
    tracker.initialize_instruments(frames[0], instruments)

    print(f"\nğŸ”¥ æœ€é©åŒ–ç‰ˆï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å†åˆ©ç”¨ï¼‰:")
    print(f"   Processing {len(frames)} frames...")
    start_time = time.time()

    # detect_batchã‚’ä½¿ç”¨ï¼ˆå†…éƒ¨ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å†åˆ©ç”¨ï¼‰
    results = tracker.detect_batch(frames)

    total_time = time.time() - start_time
    avg_fps = len(frames) / total_time

    print(f"\nâœ… ãƒãƒƒãƒå‡¦ç†å®Œäº†")
    print(f"   ç·å‡¦ç†æ™‚é–“: {total_time:.2f}s")
    print(f"   å¹³å‡FPS: {avg_fps:.1f} fps")
    print(f"   ãƒ•ãƒ¬ãƒ¼ãƒ ã‚ãŸã‚Š: {total_time/len(frames)*1000:.1f}ms")

    allocated_mb = torch.cuda.memory_allocated() / 1024**2
    reserved_mb = torch.cuda.memory_reserved() / 1024**2
    print(f"   VRAMä½¿ç”¨é‡: {allocated_mb:.1f}MB allocated, {reserved_mb:.1f}MB reserved")

    # æ€§èƒ½è©•ä¾¡
    print()
    print("=" * 70)
    ms_per_frame = total_time / len(frames) * 1000
    baseline_ms = 1122  # æœ€é©åŒ–å‰ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

    if ms_per_frame < 100:
        speedup = baseline_ms / ms_per_frame
        print(f"ğŸ‰ å¤§æˆåŠŸï¼ {speedup:.1f}å€ã®é«˜é€ŸåŒ–é”æˆï¼")
        print(f"   {baseline_ms}ms â†’ {ms_per_frame:.1f}ms/frame")
    elif ms_per_frame < 200:
        speedup = baseline_ms / ms_per_frame
        print(f"âœ… è‰¯å¥½ï¼ {speedup:.1f}å€ã®é«˜é€ŸåŒ–")
        print(f"   {baseline_ms}ms â†’ {ms_per_frame:.1f}ms/frame")
    else:
        print(f"âš ï¸ æœŸå¾…ã‚ˆã‚Šä½é€Ÿ: {ms_per_frame:.1f}ms/frame")
        print(f"   ç›®æ¨™: 50-100ms/frame")
    print("=" * 70)

if __name__ == "__main__":
    print("\nğŸ”¬ SAM vit_h GPUæ€§èƒ½ãƒ†ã‚¹ãƒˆï¼ˆRTX 3060ï¼‰\n")

    # Test 1: GPUç’°å¢ƒç¢ºèª
    if not test_gpu_availability():
        print("\nâŒ GPUç’°å¢ƒãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        sys.exit(1)

    # Test 2: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    tracker = test_sam_vit_h_loading()
    if not tracker:
        print("\nâŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)

    # Test 3: æ¨è«–é€Ÿåº¦æ¯”è¼ƒ
    test_inference_speed()

    # Test 4: ãƒãƒƒãƒå‡¦ç†
    test_batch_processing()

    print("\n" + "=" * 70)
    print("ğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("   RTX 3060ã§ã®GPUæ¨è«–ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
    print("=" * 70)
