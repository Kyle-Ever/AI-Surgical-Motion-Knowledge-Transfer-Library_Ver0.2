"""
SAM2 Video API ã‚’æ´»ç”¨ã—ãŸå™¨å…·ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°å®Ÿè£…ï¼ˆå®Ÿé¨“ç‰ˆï¼‰

ç‰¹å¾´:
- ãƒ“ãƒ‡ã‚ªå…¨ä½“ã®æ™‚é–“çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®
- Memory Bankã§ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³è€æ€§
- ä¸€è²«ã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆID
- è¤‡æ•°å™¨å…·ã®åŒæ™‚è¿½è·¡
"""

import cv2
import numpy as np
import torch
from typing import Dict, List, Any, Optional
import logging
from pathlib import Path
import asyncio
import base64
from io import BytesIO
from PIL import Image
from scipy.ndimage import median_filter

# SAM2ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from sam2.build_sam import build_sam2_video_predictor
    SAM2_AVAILABLE = True
except ImportError:
    SAM2_AVAILABLE = False
    logging.error("SAM2 library not available. Please install: pip install sam2")

logger = logging.getLogger(__name__)


class SAM2TrackerVideo:
    """
    SAM2 Video API ã‚’ä½¿ã£ãŸå™¨å…·ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°

    ç‰¹å¾´:
    - ãƒ“ãƒ‡ã‚ªå…¨ä½“ã®æ™‚é–“çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®
    - Memory Bankã§ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³è€æ€§
    - ä¸€è²«ã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆID
    - è¤‡æ•°å™¨å…·ã®åŒæ™‚è¿½è·¡
    """

    def __init__(
        self,
        model_type: str = "small",
        checkpoint_path: Optional[str] = None,
        device: str = "cpu"
    ):
        """
        åˆæœŸåŒ–

        Args:
            model_type: "tiny", "small", "base_plus", "large"
            checkpoint_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            device: "cpu" or "cuda"
        """
        self.model_type = model_type
        self.device = device
        self.predictor = None
        self.inference_state = None

        # GPUæ¤œå‡º
        if device == "cuda" and not torch.cuda.is_available():
            logger.warning("CUDA not available, using CPU")
            self.device = "cpu"
        elif device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"Auto-detected device: {self.device}")

        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        if SAM2_AVAILABLE:
            self._load_model(checkpoint_path)
        else:
            logger.error("SAM2 not available, tracker will not work")

    def _load_model(self, checkpoint_path: Optional[str]):
        """SAM2 Video Predictor ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆConfigãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¹è§£æ±ºï¼‰"""
        from app.core.config import settings

        if checkpoint_path is None:
            # Configã‹ã‚‰å–å¾—ï¼ˆçµ±ä¸€ã•ã‚ŒãŸãƒ‘ã‚¹è§£æ±ºï¼‰
            checkpoint_path = settings.get_sam2_video_checkpoint(self.model_type)
        else:
            checkpoint_path = Path(checkpoint_path)

        if not checkpoint_path.exists():
            logger.error(f"SAM2 checkpoint not found: {checkpoint_path}")
            raise FileNotFoundError(
                f"SAM2 checkpoint not found: {checkpoint_path}\n"
                f"Download from: https://dl.fbaipublicfiles.com/segment_anything_2/"
            )

        # Configãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å†…ç›¸å¯¾ãƒ‘ã‚¹ï¼‰
        config_path = settings.get_sam2_video_config(self.model_type)

        logger.info(f"Loading SAM2 Video Predictor: {self.model_type} on {self.device}")
        logger.info(f"  Checkpoint: {checkpoint_path}")
        logger.info(f"  Config: {config_path}")

        try:
            self.predictor = build_sam2_video_predictor(
                config_path,
                str(checkpoint_path),
                device=self.device
            )
            logger.info("SAM2 Video Predictor loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load SAM2: {e}")
            raise

    async def track_video(
        self,
        video_path: str,
        instruments: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ãƒ“ãƒ‡ã‚ªå…¨ä½“ã§å™¨å…·ã‚’è¿½è·¡

        Args:
            video_path: å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆMP4å½¢å¼ï¼‰
            instruments: å™¨å…·æƒ…å ±
                [
                    {
                        "id": 0,
                        "name": "forceps",
                        "selection": {
                            "type": "point" | "box" | "mask",
                            "data": [...coordinates...]
                        }
                    }
                ]

        Returns:
            è¿½è·¡çµæœ
                {
                    "instruments": [
                        {
                            "instrument_id": 0,
                            "name": "forceps",
                            "trajectory": [
                                {
                                    "frame_index": 0,
                                    "center": [x, y],
                                    "bbox": [x1, y1, x2, y2],
                                    "confidence": 0.95,
                                    "mask": np.ndarray
                                },
                                ...
                            ]
                        }
                    ]
                }
        """
        logger.info(f"[SAM2 Video API] Starting tracking: {len(instruments)} instruments")

        if not SAM2_AVAILABLE or self.predictor is None:
            logger.error("SAM2 not available")
            return {"instruments": []}

        # ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’å‹•ç”»ã‹ã‚‰å–å¾—
        frame_count = self._get_video_frame_count(video_path)
        logger.info(f"[SAM2 Video API] Video has {frame_count} frames")

        # 1. Inference stateåˆæœŸåŒ–
        await asyncio.get_event_loop().run_in_executor(
            None, self._initialize_state, video_path
        )

        # 2. åˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆFrame 0ï¼‰ã§å™¨å…·ã‚’ç™»éŒ²
        await asyncio.get_event_loop().run_in_executor(
            None, self._register_instruments, instruments
        )

        # 3. å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½è·¡ã‚’ä¼æ’­ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’æ¸¡ã™ï¼‰
        video_segments = await asyncio.get_event_loop().run_in_executor(
            None, self._propagate_tracking, frame_count
        )

        # 4. è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        trajectories = self._extract_trajectories(video_segments, instruments)

        logger.info(f"[SAM2 Video API] Tracking completed: {len(trajectories)} instruments tracked")

        return {"instruments": trajectories}

    def _initialize_state(self, video_path: str):
        """Inference stateã‚’åˆæœŸåŒ–"""
        logger.info(f"[SAM2 Video API] Initializing inference state with video: {video_path}")

        with torch.inference_mode():
            # ãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆMP4ï¼‰ã‚’æ¸¡ã™
            # SAM2 Video APIã¯MP4ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯JPEGãƒ•ã‚©ãƒ«ãƒ€ã‚’å—ã‘å…¥ã‚Œã‚‹
            self.inference_state = self.predictor.init_state(
                video_path=video_path,
                async_loading_frames=False
            )

        logger.info(f"[SAM2 Video API] Inference state initialized successfully")

    def _register_instruments(self, instruments: List[Dict[str, Any]]):
        """åˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ã§å™¨å…·ã‚’ç™»éŒ²"""
        logger.info(f"[SAM2 Video API] Registering {len(instruments)} instruments...")

        with torch.inference_mode():
            for inst in instruments:
                obj_id = inst["id"]
                # ãƒ‡ãƒãƒƒã‚°ï¼šç™»éŒ²ã™ã‚‹obj_idã®å‹ã¨å€¤ã‚’ç¢ºèª
                logger.info(f"[DEBUG] Registering obj_id={obj_id}, type={type(obj_id)}")
                selection = inst.get("selection", {})
                sel_type = selection.get("type")
                sel_data = selection.get("data")

                if sel_type == "point":
                    # ãƒã‚¤ãƒ³ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                    points = np.array(sel_data, dtype=np.float32)
                    labels = np.ones(len(points), dtype=np.int32)

                    self.predictor.add_new_points_or_box(
                        inference_state=self.inference_state,
                        frame_idx=0,
                        obj_id=obj_id,
                        points=points,
                        labels=labels
                    )
                    logger.info(f"[SAM2 Video API] Registered instrument {obj_id} with {len(points)} points")

                elif sel_type == "box":
                    # ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                    box = np.array(sel_data, dtype=np.float32)

                    self.predictor.add_new_points_or_box(
                        inference_state=self.inference_state,
                        frame_idx=0,
                        obj_id=obj_id,
                        box=box
                    )
                    logger.info(f"[SAM2 Video API] Registered instrument {obj_id} with box")

                elif sel_type == "mask":
                    # ãƒã‚¹ã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæœ€ã‚‚æ­£ç¢ºï¼‰
                    # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸPNGç”»åƒã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                    if isinstance(sel_data, str):
                        # Base64ãƒ‡ã‚³ãƒ¼ãƒ‰
                        mask_bytes = base64.b64decode(sel_data)
                        # PILã§ç”»åƒã¨ã—ã¦èª­ã¿è¾¼ã¿
                        mask_image = Image.open(BytesIO(mask_bytes))
                        # numpyé…åˆ—ã«å¤‰æ›ï¼ˆã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
                        mask = np.array(mask_image.convert('L'), dtype=np.uint8)
                        # ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ï¼ˆ0 or 255 â†’ 0 or 1ï¼‰
                        mask = (mask > 127).astype(np.uint8)
                        logger.info(f"[SAM2 Video API] Decoded mask shape: {mask.shape}")
                    else:
                        # ã™ã§ã«numpyé…åˆ—ã®å ´åˆï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
                        mask = np.array(sel_data, dtype=np.uint8)

                    self.predictor.add_new_mask(
                        inference_state=self.inference_state,
                        frame_idx=0,
                        obj_id=obj_id,
                        mask=mask
                    )
                    logger.info(f"[SAM2 Video API] Registered instrument {obj_id} with mask")

                else:
                    logger.warning(f"[SAM2 Video API] Unknown selection type: {sel_type}")

        logger.info("[SAM2 Video API] All instruments registered")

    def _propagate_tracking(self, total_frames: int) -> Dict[int, Dict[int, np.ndarray]]:
        """å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½è·¡ã‚’ä¼æ’­ï¼ˆğŸ†• Phase 4: å‹•çš„é–¾å€¤èª¿æ•´å¯¾å¿œï¼‰"""
        logger.info("[SAM2 Video API] Propagating tracking across video...")

        # è¨­å®šã‹ã‚‰é–¾å€¤ã‚’å–å¾—
        from app.core.config import settings
        base_threshold = settings.SAM2_MASK_THRESHOLD
        use_dynamic = settings.SAM2_ENABLE_DYNAMIC_THRESHOLD

        if use_dynamic:
            logger.info(f"[DYNAMIC THRESHOLD] Enabled: min={settings.SAM2_DYNAMIC_THRESHOLD_MIN}, max={settings.SAM2_DYNAMIC_THRESHOLD_MAX}")
            logger.info(f"[DYNAMIC THRESHOLD] Motion thresholds: slow={settings.SAM2_MOTION_THRESHOLD_SLOW}px, fast={settings.SAM2_MOTION_THRESHOLD_FAST}px")
        else:
            logger.info(f"[SAM2 Video API] Using fixed threshold: {base_threshold}")

        video_segments = {}
        frame_count = 0
        processed_frames = 0  # ğŸ†• é€²æ—è¿½è·¡ç”¨ã‚«ã‚¦ãƒ³ã‚¿

        # ğŸ†• Phase 4: å‰ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä¸­å¿ƒåº§æ¨™ã‚’è¨˜éŒ²ï¼ˆå‹•ãè¨ˆç®—ç”¨ï¼‰
        prev_centers = {}  # {obj_id: (x, y)}

        with torch.inference_mode():
            try:
                # SAM2ãŒå…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è‡ªå‹•è¿½è·¡
                for out_frame_idx, out_obj_ids, out_mask_logits in \
                        self.predictor.propagate_in_video(self.inference_state):
                    processed_frames += 1  # ğŸ†• ã‚«ã‚¦ãƒ³ãƒˆ

                    # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã§logitsã®æƒ…å ±ã‚’ç¢ºèª
                    if out_frame_idx == 0:
                        logger.info(f"[DEBUG] Frame 0 out_mask_logits: shape={out_mask_logits.shape}, dtype={out_mask_logits.dtype}, min={out_mask_logits.min().item():.4f}, max={out_mask_logits.max().item():.4f}")

                    masks = {}

                    for i, obj_id in enumerate(out_obj_ids):
                        # ğŸ†• Phase 4: å‹•çš„é–¾å€¤èª¿æ•´
                        if use_dynamic and obj_id in prev_centers:
                            # ãƒã‚¹ã‚¯ã‹ã‚‰ç¾åœ¨ã®ä¸­å¿ƒåº§æ¨™ã‚’è¨ˆç®—
                            mask_logits = out_mask_logits[i].cpu().numpy()
                            temp_mask = mask_logits > 0.0  # ä»®ãƒã‚¹ã‚¯ï¼ˆé–¾å€¤0ã§å…¨é ˜åŸŸï¼‰

                            if temp_mask.sum() > 0:
                                # ãƒã‚¹ã‚¯ã®æ¬¡å…ƒã‚’ç¢ºèªã—ã¦2æ¬¡å…ƒã«å¤‰æ›
                                temp_mask_2d = temp_mask.squeeze() if temp_mask.ndim > 2 else temp_mask
                                y_coords, x_coords = np.where(temp_mask_2d)
                                current_center = (np.mean(x_coords), np.mean(y_coords))

                                # å‰ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã®ç§»å‹•è·é›¢ã‚’è¨ˆç®—
                                prev_x, prev_y = prev_centers[obj_id]
                                motion_distance = np.sqrt(
                                    (current_center[0] - prev_x)**2 +
                                    (current_center[1] - prev_y)**2
                                )

                                # ç§»å‹•è·é›¢ã«å¿œã˜ã¦é–¾å€¤ã‚’èª¿æ•´
                                if motion_distance < settings.SAM2_MOTION_THRESHOLD_SLOW:
                                    # é™æ­¢ï½ä½é€Ÿ: é«˜é–¾å€¤ï¼ˆç²¾åº¦å„ªå…ˆï¼‰
                                    threshold = settings.SAM2_DYNAMIC_THRESHOLD_MAX
                                elif motion_distance < settings.SAM2_MOTION_THRESHOLD_FAST:
                                    # ä¸­é€Ÿ: ä¸­é–“é–¾å€¤
                                    threshold = (settings.SAM2_DYNAMIC_THRESHOLD_MIN + settings.SAM2_DYNAMIC_THRESHOLD_MAX) / 2
                                else:
                                    # é«˜é€Ÿ: ä½é–¾å€¤ï¼ˆè¿½è·¡å„ªå…ˆï¼‰
                                    threshold = settings.SAM2_DYNAMIC_THRESHOLD_MIN

                                # é–¾å€¤å¤‰æ›´æ™‚ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                                if processed_frames % 100 == 0 or motion_distance > settings.SAM2_MOTION_THRESHOLD_FAST:
                                    logger.debug(f"[DYNAMIC] Frame {out_frame_idx} obj {obj_id}: motion={motion_distance:.1f}px â†’ threshold={threshold:.2f}")

                                prev_centers[obj_id] = current_center
                            else:
                                # ãƒã‚¹ã‚¯ãŒç©ºã®å ´åˆã¯åŸºæœ¬é–¾å€¤ã‚’ä½¿ç”¨
                                threshold = base_threshold
                        else:
                            # å‹•çš„é–¾å€¤ç„¡åŠ¹ or åˆå›ãƒ•ãƒ¬ãƒ¼ãƒ 
                            threshold = base_threshold

                        # ãƒã‚¹ã‚¯ã‚’ãƒã‚¤ãƒŠãƒªåŒ–
                        mask = (out_mask_logits[i] > threshold).cpu().numpy()
                        masks[obj_id] = mask

                        # ğŸ†• Phase 4: æ¬¡ãƒ•ãƒ¬ãƒ¼ãƒ ç”¨ã«ä¸­å¿ƒåº§æ¨™ã‚’ä¿å­˜
                        if use_dynamic and mask.sum() > 0:
                            # ãƒã‚¹ã‚¯ã®æ¬¡å…ƒã‚’ç¢ºèªã—ã¦2æ¬¡å…ƒã«å¤‰æ›
                            mask_2d = mask.squeeze() if mask.ndim > 2 else mask
                            y_coords, x_coords = np.where(mask_2d)
                            prev_centers[obj_id] = (np.mean(x_coords), np.mean(y_coords))

                    # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã§masksè¾æ›¸ã®ã‚­ãƒ¼ã‚’ç¢ºèª
                    if out_frame_idx == 0:
                        logger.info(f"[DEBUG] Frame 0 mask keys: {list(masks.keys())}, types: {[type(k) for k in masks.keys()]}")
                        # ãƒã‚¤ãƒŠãƒªåŒ–å¾Œã®ãƒã‚¹ã‚¯æƒ…å ±
                        for obj_id in masks:
                            logger.info(f"[DEBUG] Frame 0 obj_id={obj_id}: mask shape={masks[obj_id].shape}, sum={masks[obj_id].sum()}")

                    # ğŸ› FIX: å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã§ãƒã‚¹ã‚¯ã‚’ä¿å­˜ï¼ˆifæ–‡ã®å¤–ï¼‰
                    video_segments[out_frame_idx] = masks
                    frame_count += 1

                    # ğŸ†• é€²æ—ãƒ­ã‚°å¼·åŒ–ï¼ˆ100ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ + è©³ç´°æƒ…å ±ï¼‰
                    if processed_frames % 100 == 0:
                        logger.warning(f"[PROPAGATION] Processed {processed_frames} frames, current frame_idx={out_frame_idx}, total expected={total_frames}")

            except Exception as e:
                logger.error(f"[PROPAGATION] Failed at frame {processed_frames}: {e}")
                import traceback
                logger.error(f"[PROPAGATION] Traceback: {traceback.format_exc()}")
                # éƒ¨åˆ†çš„ãªçµæœã‚’è¿”ã™ï¼ˆå®Œå…¨ã«å¤±æ•—ã™ã‚‹ã‚ˆã‚Šãƒã‚·ï¼‰
                logger.warning(f"[PROPAGATION] Returning partial results: {frame_count} frames")
                return video_segments

        logger.info(f"[SAM2 Video API] Tracking propagated to {frame_count} frames")
        return video_segments

    def _extract_trajectories(
        self,
        video_segments: Dict[int, Dict[int, np.ndarray]],
        instruments: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        logger.info("[SAM2 Video API] Extracting trajectories...")

        trajectories = []
        total_frames = len(video_segments.keys())
        logger.info(f"[DEBUG] Starting trajectory extraction: {total_frames} frames, {len(instruments)} instruments")

        for inst in instruments:
            obj_id = inst["id"]
            trajectory = []

            # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®å™¨å…·ã®ã¿è©³ç´°ãƒ­ã‚°
            is_first_inst = (inst == instruments[0])

            for frame_idx in sorted(video_segments.keys()):
                masks = video_segments[frame_idx]

                # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã§æ¤œç´¢çŠ¶æ³ã‚’ç¢ºèª
                if is_first_inst and frame_idx == sorted(video_segments.keys())[0]:
                    logger.info(f"[DEBUG] Looking for obj_id={obj_id} (type={type(obj_id)}) in masks with keys={list(masks.keys())}, types={[type(k) for k in masks.keys()]}")

                if obj_id not in masks:
                    # ã“ã®å™¨å…·ãŒæ¤œå‡ºã•ã‚Œãªã‹ã£ãŸãƒ•ãƒ¬ãƒ¼ãƒ 
                    continue

                mask = masks[obj_id]

                # ãƒã‚¹ã‚¯ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
                if mask.sum() == 0:
                    # ç©ºã®ãƒã‚¹ã‚¯
                    if is_first_inst and frame_idx < 5:  # æœ€åˆã®5ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿ãƒ­ã‚°
                        logger.info(f"[DEBUG] Frame {frame_idx}: Empty mask (sum=0) before normalization, shape={mask.shape}, dtype={mask.dtype}")
                    continue

                # ãƒã‚¹ã‚¯ã‚’2æ¬¡å…ƒã«æ­£è¦åŒ–
                original_shape = mask.shape
                if mask.ndim == 3:
                    # 3æ¬¡å…ƒã®å ´åˆ (B, H, W) â†’ (H, W): ãƒãƒƒãƒæ¬¡å…ƒã‚’å‰Šé™¤
                    mask = mask[0]
                elif mask.ndim == 4:
                    # 4æ¬¡å…ƒã®å ´åˆ (B, C, H, W) â†’ (H, W): ãƒãƒƒãƒã¨ãƒãƒ£ãƒ³ãƒãƒ«æ¬¡å…ƒã‚’å‰Šé™¤
                    mask = mask[0, 0]
                elif mask.ndim > 4:
                    raise ValueError(f"[SAM2 Video API] Unexpected mask dimension: {mask.ndim}, shape={mask.shape}")

                # æ­£è¦åŒ–å¾Œã«å†åº¦ç©ºãƒã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯
                if mask.sum() == 0:
                    # æ­£è¦åŒ–å¾Œã‚‚ç©ºã®ãƒã‚¹ã‚¯
                    if is_first_inst and frame_idx < 5:
                        logger.info(f"[DEBUG] Frame {frame_idx}: Empty mask (sum=0) after normalization, original_shape={original_shape}, normalized_shape={mask.shape}")
                    continue

                # ğŸ†• æœ€å°é¢ç©ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆãƒã‚¤ã‚ºé™¤å»ï¼‰
                from app.core.config import settings
                min_area = settings.SAM2_MIN_MASK_AREA
                mask_area = mask.sum()
                if mask_area < min_area:
                    if is_first_inst and frame_idx < 5:
                        logger.info(f"[QUALITY] Frame {frame_idx}: Small mask filtered (area={mask_area} < {min_area})")
                    continue

                # ãƒ‡ãƒãƒƒã‚°ï¼šåˆå›ã®ã¿ãƒã‚¹ã‚¯æƒ…å ±ã‚’å‡ºåŠ›ï¼ˆæ‹¡å¼µï¼šæœ€åˆã®3ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰
                if frame_idx < 3 and is_first_inst:
                    if original_shape != mask.shape:
                        logger.info(f"[DEBUG] Frame {frame_idx}: Normalized mask: {original_shape} â†’ {mask.shape}")
                    logger.info(f"[DEBUG] Frame {frame_idx}: Mask info - shape={mask.shape}, dtype={mask.dtype}, sum={mask.sum()}, min={mask.min()}, max={mask.max()}")

                # é‡å¿ƒè¨ˆç®—
                y_coords, x_coords = np.where(mask)

                # ç©ºé…åˆ—ãƒã‚§ãƒƒã‚¯ï¼ˆå¿µã®ãŸã‚ï¼‰
                if len(x_coords) == 0 or len(y_coords) == 0:
                    continue
                center_x = float(np.mean(x_coords))
                center_y = float(np.mean(y_coords))

                # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹
                x_min, x_max = float(x_coords.min()), float(x_coords.max())
                y_min, y_max = float(y_coords.min()), float(y_coords.max())

                # ğŸ†• æ”¹å–„ã•ã‚ŒãŸä¿¡é ¼åº¦è¨ˆç®—ï¼ˆæ‰‹è¡“å™¨å…·æœ€é©åŒ–ç‰ˆï¼‰
                # ãƒã‚¹ã‚¯ã®å“è³ªã«åŸºã¥ã„ãŸä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0.0ï½1.0ï¼‰
                mask_area = mask.sum()
                bbox_width = x_max - x_min
                bbox_height = y_max - y_min
                bbox_area = bbox_width * bbox_height
                aspect_ratio = bbox_height / bbox_width if bbox_width > 0 else 1.0

                # 1. BBoxå……å¡«ç‡ã®æ­£è¦åŒ–ï¼ˆå™¨å…·å½¢çŠ¶ã«å¿œã˜ãŸæœŸå¾…å€¤ã§è£œæ­£ï¼‰
                fill_ratio = mask_area / bbox_area if bbox_area > 0 else 0.0

                # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‹ã‚‰æœŸå¾…fill_ratioã‚’è¨ˆç®—
                if aspect_ratio >= 3.0:
                    # æ¥µç´°é•·å™¨å…·: ãƒã‚¹ã‚¯ãŒBBoxã®30%ã‚’åŸ‹ã‚ã‚Œã°ååˆ†
                    expected_fill = 0.3
                elif aspect_ratio >= 1.5:
                    # ä¸­ç¨‹åº¦ã®ç´°é•·ã•: 50%æœŸå¾…
                    expected_fill = 0.5
                else:
                    # æ­£æ–¹å½¢ã«è¿‘ã„: 70%å¿…è¦ï¼ˆãƒã‚¤ã‚ºã®å¯èƒ½æ€§ï¼‰
                    expected_fill = 0.7

                # æ­£è¦åŒ–ï¼ˆæœŸå¾…å€¤ã§å‰²ã‚‹ï¼‰
                fill_ratio_normalized = min(1.0, fill_ratio / expected_fill) if expected_fill > 0 else 0.0

                # 2. ã‚µã‚¤ã‚ºã®å¦¥å½“æ€§ï¼ˆæ¥µç«¯ã«å°ã•ã„/å¤§ãã„ãƒã‚¹ã‚¯ã¯ä½ã„ä¿¡é ¼åº¦ï¼‰
                from app.core.config import settings
                min_area = settings.SAM2_MIN_MASK_AREA  # ä¾‹: 500
                max_area = mask.shape[0] * mask.shape[1] * 0.5  # ç”»åƒã®50%ä»¥ä¸Šã¯ä¸è‡ªç„¶

                if mask_area < min_area:
                    size_score = mask_area / min_area  # 0.0ï½1.0
                elif mask_area > max_area:
                    size_score = max_area / mask_area  # 1.0æœªæº€
                else:
                    size_score = 1.0  # å¦¥å½“ãªã‚µã‚¤ã‚º

                # 3. å½¢çŠ¶ã®å¦¥å½“æ€§ï¼ˆæ‰‹è¡“å™¨å…·ã¯ç´°é•·ã„å½¢çŠ¶ãŒå¤šã„ï¼‰
                # ç†æƒ³çš„ãªã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”: 1.5ï½15ï¼ˆå¹…åºƒã„å™¨å…·ã«å¯¾å¿œï¼‰
                if 1.5 <= aspect_ratio <= 15.0:
                    shape_score = 1.0  # ç†æƒ³çš„ãªç¯„å›²
                elif aspect_ratio < 1.5:
                    # æ­£æ–¹å½¢ã«è¿‘ã„ï¼ˆãƒã‚¤ã‚ºã®å¯èƒ½æ€§ï¼‰
                    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”0.75ã§0.5ã€1.5ã§1.0ã®ç·šå½¢ã‚¹ã‚³ã‚¢
                    shape_score = 0.5 + 0.5 * (aspect_ratio / 1.5)
                else:
                    # æ¥µç«¯ã«ç´°é•·ã„ï¼ˆã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”15ä»¥ä¸Šï¼‰
                    # 15ã§1.0ã€65ã§0.5ã®ç·šå½¢æ¸›å°‘
                    shape_score = max(0.5, 1.0 - (aspect_ratio - 15.0) / 50.0)

                # æœ€çµ‚çš„ãªä¿¡é ¼åº¦ï¼šæ‰‹è¡“å™¨å…·ã«æœ€é©åŒ–ã—ãŸé‡ã¿é…åˆ†
                confidence = (
                    fill_ratio_normalized * 0.3 +  # å……å¡«ç‡: 30%ï¼ˆæ­£è¦åŒ–æ¸ˆã¿ï¼‰
                    size_score * 0.2 +              # ã‚µã‚¤ã‚º: 20%ï¼ˆè£œåŠ©çš„ï¼‰
                    shape_score * 0.5               # å½¢çŠ¶: 50%ï¼ˆæœ€é‡è¦ç‰¹å¾´ï¼‰
                )
                confidence = min(1.0, max(0.0, confidence))  # 0.0ï½1.0ã«ã‚¯ãƒªãƒƒãƒ—

                trajectory.append({
                    "frame_index": int(frame_idx),
                    "center": [center_x, center_y],
                    "bbox": [x_min, y_min, x_max, y_max],
                    "confidence": float(confidence),
                    "mask": mask  # å¿…è¦ã«å¿œã˜ã¦ä¿å­˜
                })

            trajectories.append({
                "instrument_id": obj_id,
                "name": inst.get("name", f"instrument_{obj_id}"),
                "trajectory": trajectory
            })

            logger.info(f"[SAM2 Video API] Extracted trajectory for {inst.get('name', 'Unknown')}: {len(trajectory)} frames")

            # ãƒ‡ãƒãƒƒã‚°ï¼šç©ºã®trajectoryã®å ´åˆã€è©³ç´°ã‚’å‡ºåŠ›
            if len(trajectory) == 0:
                logger.warning(f"[DEBUG] Zero-length trajectory! obj_id={obj_id}, instruments_count={len(instruments)}, video_segments_frames={len(video_segments.keys())}")

        # ğŸ†• Phase 2: æ™‚é–“çš„å¹³æ»‘åŒ–ã‚’é©ç”¨
        from app.core.config import settings
        if settings.SAM2_ENABLE_TEMPORAL_SMOOTHING and len(trajectories) > 0:
            logger.info("[SAM2 Video API] Applying temporal smoothing...")
            trajectories = self._apply_temporal_smoothing(trajectories, settings.SAM2_SMOOTHING_WINDOW)
            logger.info("[SAM2 Video API] Temporal smoothing applied")

        return trajectories

    def _bbox_to_sam_format(self, bbox: List[float]) -> np.ndarray:
        """BBoxã‚’SAMå½¢å¼ã«å¤‰æ›"""
        return np.array(bbox, dtype=np.float32)

    def _center_to_sam_format(self, center: List[float]) -> np.ndarray:
        """ä¸­å¿ƒç‚¹ã‚’SAMå½¢å¼ã«å¤‰æ›"""
        return np.array([center], dtype=np.float32)

    def _mask_to_bbox(self, mask: np.ndarray) -> List[float]:
        """ãƒã‚¹ã‚¯ã‹ã‚‰BBoxã‚’æŠ½å‡º"""
        if mask.sum() == 0:
            return [0, 0, 0, 0]

        y_coords, x_coords = np.where(mask)
        x_min, x_max = float(x_coords.min()), float(x_coords.max())
        y_min, y_max = float(y_coords.min()), float(y_coords.max())

        return [x_min, y_min, x_max, y_max]

    def _calculate_mask_center(self, mask: np.ndarray) -> List[float]:
        """ãƒã‚¹ã‚¯ã®ä¸­å¿ƒã‚’è¨ˆç®—"""
        if mask.sum() == 0:
            return [0.0, 0.0]

        y_coords, x_coords = np.where(mask)
        center_x = float(np.mean(x_coords))
        center_y = float(np.mean(y_coords))

        return [center_x, center_y]

    def _calculate_mask_confidence(self, mask: np.ndarray) -> float:
        """ãƒã‚¹ã‚¯ã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        if mask.size == 0:
            return 0.0

        area = mask.sum()
        total_pixels = mask.shape[0] * mask.shape[1]
        confidence = min(1.0, area / total_pixels)

        return float(confidence)

    def _get_video_frame_count(self, video_path: str) -> int:
        """
        å‹•ç”»ã®ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’å–å¾—

        Args:
            video_path: å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Returns:
            ãƒ•ãƒ¬ãƒ¼ãƒ æ•°
        """
        import cv2

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {video_path}")

        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        cap.release()

        logger.info(f"[SAM2 Video API] Detected {frame_count} frames in video")
        return frame_count

    def _apply_temporal_smoothing(
        self,
        trajectories: List[Dict[str, Any]],
        window_size: int = 5
    ) -> List[Dict[str, Any]]:
        """
        æ™‚é–“çš„å¹³æ»‘åŒ–ã‚’é©ç”¨ã—ã¦ã‚¸ãƒƒã‚¿ãƒ¼ã‚’å‰Šæ¸›

        Args:
            trajectories: è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            window_size: å¹³æ»‘åŒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆå¥‡æ•°æ¨å¥¨ï¼‰

        Returns:
            å¹³æ»‘åŒ–ã•ã‚ŒãŸè»Œè·¡ãƒ‡ãƒ¼ã‚¿
        """
        smoothed_trajectories = []

        for traj_data in trajectories:
            trajectory = traj_data["trajectory"]

            # è»Œè·¡ãŒçŸ­ã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if len(trajectory) < window_size:
                logger.warning(f"[SMOOTHING] Trajectory too short ({len(trajectory)} frames), skipping smoothing")
                smoothed_trajectories.append(traj_data)
                continue

            # ä¸­å¿ƒåº§æ¨™ã‚’æŠ½å‡º
            centers = np.array([frame["center"] for frame in trajectory])  # (N, 2)
            bboxes = np.array([frame["bbox"] for frame in trajectory])    # (N, 4)

            # ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ï¼ˆå„æ¬¡å…ƒç‹¬ç«‹ã«ï¼‰
            smoothed_centers = np.zeros_like(centers)
            smoothed_bboxes = np.zeros_like(bboxes)

            for dim in range(2):  # x, y
                smoothed_centers[:, dim] = median_filter(centers[:, dim], size=window_size, mode='nearest')

            for dim in range(4):  # x_min, y_min, x_max, y_max
                smoothed_bboxes[:, dim] = median_filter(bboxes[:, dim], size=window_size, mode='nearest')

            # å¹³æ»‘åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§æ›´æ–°
            smoothed_trajectory = []
            for i, frame in enumerate(trajectory):
                smoothed_frame = frame.copy()
                smoothed_frame["center"] = smoothed_centers[i].tolist()
                smoothed_frame["bbox"] = smoothed_bboxes[i].tolist()
                smoothed_trajectory.append(smoothed_frame)

            smoothed_trajectories.append({
                "instrument_id": traj_data["instrument_id"],
                "name": traj_data["name"],
                "trajectory": smoothed_trajectory
            })

            logger.info(f"[SMOOTHING] Applied median filter (window={window_size}) to {traj_data['name']}: {len(smoothed_trajectory)} frames")

        return smoothed_trajectories
