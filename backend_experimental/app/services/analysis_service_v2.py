"""
Analysis Service V2 - Clean architecture implementation
"""
import asyncio
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime
import cv2
import numpy as np
import json
import pytz

from app.models import SessionLocal
from sqlalchemy.orm import Session
from app.models.analysis import AnalysisResult, AnalysisStatus, get_jst_now
from app.models.video import Video, VideoType
from app.core.websocket import manager
from app.core.config import settings
from app.ai_engine.processors.skeleton_detector import HandSkeletonDetector
from app.ai_engine.processors.sam_tracker_unified import SAMTrackerUnified
from app.ai_engine.processors.sam2_tracker import SAM2Tracker
from app.ai_engine.processors.sam2_tracker_video import SAM2TrackerVideo  # å®Ÿé¨“ç‰ˆ
from app.ai_engine.processors.gaze_analyzer import GazeAnalyzer  # è¦–ç·šè§£æ
from .metrics_calculator import MetricsCalculator
from .frame_extraction_service import FrameExtractionService, ExtractionConfig, ExtractionResult

logger = logging.getLogger(__name__)


def convert_numpy_types(obj):
    """
    Convert numpy types to Python native types for JSON serialization

    Args:
        obj: Object potentially containing numpy types

    Returns:
        Object with all numpy types converted to Python native types
    """
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    return obj


class AnalysisServiceV2:
    """
    ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ãè§£æã‚µãƒ¼ãƒ“ã‚¹
    è²¬å‹™ã®åˆ†é›¢ã¨æ‹¡å¼µæ€§ã‚’é‡è¦–
    """

    def __init__(self):
        self.detectors = {}
        self.video_info = {}
        self.warnings = []  # Phase 2.2: è­¦å‘Šåé›†ç”¨
        self.tracking_stats = {}  # Phase 2.2: ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆåé›†ç”¨
        # SAM2ä½¿ç”¨ãƒ•ãƒ©ã‚°ï¼ˆç’°å¢ƒå¤‰æ•° USE_SAM2=true ã§æœ‰åŠ¹åŒ–ï¼‰
        self.use_sam2 = getattr(settings, 'USE_SAM2', False)
        # ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºã‚µãƒ¼ãƒ“ã‚¹
        self.frame_extraction_service = FrameExtractionService(
            ExtractionConfig(
                target_fps=getattr(settings, 'FRAME_EXTRACTION_FPS', 15),
                use_round=True  # round()ã‚’ä½¿ç”¨ã—ã¦frame_skipè¨ˆç®—
            )
        )
        self.extraction_result: Optional[ExtractionResult] = None  # æŠ½å‡ºçµæœã‚’ä¿æŒ
        # è¦–ç·šè§£æã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ï¼ˆé…å»¶åˆæœŸåŒ–ï¼‰
        self.gaze_analyzer: Optional[GazeAnalyzer] = None

    async def analyze_video(
        self,
        video_id: str,
        analysis_id: str,
        instruments: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        å‹•ç”»è§£æã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

        Args:
            video_id: å‹•ç”»ID
            analysis_id: è§£æID
            instruments: å™¨å…·å®šç¾©ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            è§£æçµæœã®è¾æ›¸
        """
        logger.info(f"[ANALYSIS] === Starting V2 analysis ===")
        logger.info(f"[ANALYSIS] video_id: {video_id}")
        logger.info(f"[ANALYSIS] analysis_id: {analysis_id}")
        logger.info(f"[ANALYSIS] instruments: {instruments}")

        db = SessionLocal()
        try:
            # 1. è§£æãƒ¬ã‚³ãƒ¼ãƒ‰ã¨å‹•ç”»æƒ…å ±ã®å–å¾—
            analysis_result = db.query(AnalysisResult).filter(
                AnalysisResult.id == analysis_id
            ).first()

            if not analysis_result:
                raise ValueError(f"Analysis not found: {analysis_id}")

            video = db.query(Video).filter(Video.id == video_id).first()
            if not video:
                raise ValueError(f"Video not found: {video_id}")

            # Convert relative path to absolute path from backend directory
            video_path = Path(video.file_path)
            if not video_path.is_absolute():
                # Assume file_path is relative to backend directory
                backend_dir = Path(__file__).parent.parent.parent  # backend/
                video_path = backend_dir / video_path

            if not video_path.exists():
                raise FileNotFoundError(f"Video file not found: {video_path}")

            # 2. å‹•ç”»ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãå‡¦ç†æˆ¦ç•¥ã®æ±ºå®š
            video_type = video.video_type
            logger.info(f"[ANALYSIS] Video type: {video_type}")
            logger.info(f"[ANALYSIS] Video path: {video.file_path}")

            # 2.1 è¦–ç·šè§£æã®å ´åˆã¯å°‚ç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¸ï¼ˆæ—¢å­˜æ©Ÿèƒ½ã¨å®Œå…¨åˆ†é›¢ï¼‰
            if video_type == VideoType.EYE_GAZE:
                logger.info(f"[ANALYSIS] Routing to eye gaze analysis pipeline")
                return await self._analyze_eye_gaze(video, analysis_result, analysis_id, db)

            # 3. å‹•ç”»æƒ…å ±ã®å–å¾—ï¼ˆæ—¢å­˜æ©Ÿèƒ½ã¯ã“ã“ã‹ã‚‰ç¶™ç¶šï¼‰
            logger.info(f"[ANALYSIS] Getting video info...")
            await self._update_status(analysis_result, "initialization", db, progress=5)
            self.video_info = self._get_video_info(str(video_path))
            logger.info(f"[ANALYSIS] Video info retrieved")
            await self._update_status(analysis_result, "initialization", db, progress=10)
            logger.info(f"[ANALYSIS] Status updated to initialization")

            # 4. ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºï¼ˆæ–°ã—ã„FrameExtractionServiceã‚’ä½¿ç”¨ï¼‰
            logger.info(f"[ANALYSIS] Starting frame extraction...")
            await self._update_status(analysis_result, "frame_extraction", db, progress=15)

            # æ–°ã—ã„ã‚µãƒ¼ãƒ“ã‚¹ã§ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º
            loop = asyncio.get_event_loop()
            self.extraction_result = await loop.run_in_executor(
                None,
                self.frame_extraction_service.extract_frames,
                str(video_path)
            )

            frames = self.extraction_result.frames
            logger.info(f"[ANALYSIS] {self.extraction_result}")
            logger.info(f"[ANALYSIS] Extracted {len(frames)} frames, "
                       f"effective_fps={self.extraction_result.effective_fps:.2f}, "
                       f"frame_skip={self.extraction_result.frame_skip}")
            await self._update_status(analysis_result, "frame_extraction", db, progress=30)

            # 5. æ¤œå‡ºå‡¦ç†ã®å®Ÿè¡Œ
            logger.info(f"[ANALYSIS] Starting detection...")
            await self._update_status(analysis_result, "skeleton_detection", db, progress=35)
            detection_results = await self._run_detection(
                frames, video_type, video_id, instruments, video_path
            )
            logger.info(f"[ANALYSIS] Detection completed with {len(detection_results) if detection_results else 0} results")
            await self._update_status(analysis_result, "instrument_detection", db, progress=60)

            # 6. ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            await self._update_status(analysis_result, "motion_analysis", db, progress=70)
            metrics = await self._calculate_metrics(detection_results)
            await self._update_status(analysis_result, "motion_analysis", db, progress=80)

            # 7. ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            await self._update_status(analysis_result, "report_generation", db, progress=85)
            scores = await self._calculate_scores(metrics)

            # 8. çµæœã®ä¿å­˜
            await self._update_status(analysis_result, "report_generation", db, progress=90)
            await self._save_results(
                analysis_result, detection_results, metrics, scores, db
            )
            await self._update_status(analysis_result, "report_generation", db, progress=95)

            # 9. å®Œäº†é€šçŸ¥
            await self._update_status(analysis_result, "completed", db, progress=100)

            return {
                'status': 'success',
                'video_id': video_id,
                'analysis_id': analysis_id,
                'detection_results': detection_results,
                'metrics': metrics,
                'scores': scores
            }

        except Exception as e:
            logger.error(f"[ANALYSIS] Analysis failed: {str(e)}")
            logger.error(f"[ANALYSIS] Error type: {type(e).__name__}")
            import traceback
            error_traceback = traceback.format_exc()
            logger.error(f"[ANALYSIS] Traceback: {error_traceback}")

            if analysis_result:
                # Phase 2.2: ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è©³ç´°ã«è¨˜éŒ²
                analysis_result.status = AnalysisStatus.FAILED
                analysis_result.error_message = f"{type(e).__name__}: {str(e)}"

                # åé›†ã—ãŸè­¦å‘ŠãŒã‚ã‚Œã°ä¿å­˜
                if self.warnings:
                    analysis_result.warnings = json.dumps(self.warnings)
                    logger.info(f"[ANALYSIS] Saved {len(self.warnings)} warnings to DB")

                # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆãŒã‚ã‚Œã°ä¿å­˜
                if self.tracking_stats:
                    analysis_result.tracking_stats = json.dumps(self.tracking_stats)
                    logger.info(f"[ANALYSIS] Saved tracking stats to DB: {list(self.tracking_stats.keys())}")

                db.commit()

                # WebSocketã§è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’é€ä¿¡
                try:
                    await manager.send_progress(analysis_id, {
                        "type": "error",
                        "error_type": type(e).__name__,
                        "message": str(e),
                        "warnings_count": len(self.warnings),
                        "tracking_stats": self.tracking_stats
                    })
                except:
                    pass  # WebSocketé€ä¿¡å¤±æ•—ã¯ç„¡è¦–

            raise
        finally:
            db.close()
            # æ¤œå‡ºå™¨ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            for detector in self.detectors.values():
                if hasattr(detector, 'close'):
                    detector.close()

    def _get_video_info(self, video_path: str) -> Dict:
        """å‹•ç”»æƒ…å ±ã‚’å–å¾—"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {video_path}")

        info = {
            'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
            'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
            'fps': cap.get(cv2.CAP_PROP_FPS),
            'total_frames': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
            'duration': 0
        }

        # FPSãŒä¸æ­£ãªå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if info['fps'] <= 0:
            logger.warning(f"[ANALYSIS] Invalid FPS ({info['fps']}), using default 30fps")
            info['fps'] = 30.0

        # å‹•ç”»ã®é•·ã•ã‚’æ­£ç¢ºã«è¨ˆç®—
        info['duration'] = info['total_frames'] / info['fps']

        cap.release()
        logger.info(f"[ANALYSIS] Video info: {info}")
        return info

    # _extract_frames ãƒ¡ã‚½ãƒƒãƒ‰ã¯å‰Šé™¤ - FrameExtractionServiceã‚’ä½¿ç”¨

    def _convert_instruments_format(self, instruments: List[Dict]) -> List[Dict]:
        """
        ä¿å­˜ã•ã‚ŒãŸinstrumentså½¢å¼ã‚’SAMTrackerUnifiedç”¨ã«å¤‰æ›

        Input: [{"name": str, "bbox": [x,y,w,h], "frame_number": int, "mask": str}]
        Output: [{"id": int, "name": str, "selection": {"type": "mask"|"box", "data": ...}, "color": str}]

        Args:
            instruments: ä¿å­˜ã•ã‚ŒãŸinstrumentså½¢å¼ã®ãƒªã‚¹ãƒˆ

        Returns:
            SAMTrackerUnifiedç”¨ã«å¤‰æ›ã•ã‚ŒãŸinstrumentsãƒªã‚¹ãƒˆ
        """
        converted = []
        colors = ["#FF0000", "#00FF00", "#0000FF", "#FFFF00", "#FF00FF"]

        for idx, inst in enumerate(instruments):
            # ãƒã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯å„ªå…ˆçš„ã«ä½¿ç”¨ï¼ˆæœ€ã‚‚æ­£ç¢ºï¼‰
            if "mask" in inst and inst["mask"]:
                logger.info(f"[ANALYSIS] Instrument {idx} using mask-based initialization")
                converted.append({
                    "id": idx,
                    "name": inst.get("name", f"Instrument {idx + 1}"),
                    "selection": {
                        "type": "mask",
                        "data": inst["mask"]  # base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒã‚¹ã‚¯
                    },
                    "color": inst.get("color", colors[idx % len(colors)])
                })
            # ãƒã‚¹ã‚¯ãŒãªã„å ´åˆã¯bboxã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            elif "bbox" in inst:
                x, y, w, h = inst["bbox"]
                bbox_xyxy = [x, y, x + w, y + h]
                logger.info(f"[ANALYSIS] Instrument {idx} using box-based initialization (fallback)")
                converted.append({
                    "id": idx,
                    "name": inst.get("name", f"Instrument {idx + 1}"),
                    "selection": {
                        "type": "box",
                        "data": bbox_xyxy
                    },
                    "color": inst.get("color", colors[idx % len(colors)])
                })
            elif "selection" in inst and inst["selection"].get("type") == "box":
                # æ—¢ã«å¤‰æ›æ¸ˆã¿ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
                bbox_xyxy = inst["selection"]["data"]
                logger.info(f"[ANALYSIS] Instrument {idx} using pre-converted box format")
                converted.append({
                    "id": idx,
                    "name": inst.get("name", f"Instrument {idx + 1}"),
                    "selection": {
                        "type": "box",
                        "data": bbox_xyxy
                    },
                    "color": inst.get("color", colors[idx % len(colors)])
                })
            # ğŸ†• pointsãƒªã‚¹ãƒˆã‹ã‚‰bboxã‚’è¨ˆç®—
            elif "points" in inst and inst["points"]:
                points = inst["points"]
                if len(points) >= 2:  # æœ€ä½2ç‚¹å¿…è¦
                    xs = [p[0] for p in points]
                    ys = [p[1] for p in points]
                    x_min, y_min = min(xs), min(ys)
                    x_max, y_max = max(xs), max(ys)
                    bbox_xyxy = [x_min, y_min, x_max, y_max]
                    logger.info(f"[ANALYSIS] Instrument {idx} using points-to-box conversion ({len(points)} points)")
                    converted.append({
                        "id": idx,
                        "name": inst.get("name", f"Instrument {idx + 1}"),
                        "selection": {
                            "type": "box",
                            "data": bbox_xyxy
                        },
                        "color": inst.get("color", colors[idx % len(colors)])
                    })
                else:
                    logger.warning(f"[ANALYSIS] Instrument {idx} has insufficient points ({len(points)})")
            else:
                logger.warning(f"[ANALYSIS] Instrument {idx} has no valid bbox, mask, or points, skipping")
                continue

        logger.info(f"[ANALYSIS] Converted {len(converted)} instruments from saved format to SAM format")
        return converted

    async def _run_detection(
        self,
        frames: List[np.ndarray],
        video_type: VideoType,
        video_id: str,
        instruments: Optional[List[Dict]],
        video_path: Path
    ) -> Dict[str, Any]:
        """
        å‹•ç”»ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãæ¤œå‡ºå‡¦ç†ã®å®Ÿè¡Œ

        Args:
            frames: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆ
            video_type: å‹•ç”»ã‚¿ã‚¤ãƒ—
            video_id: å‹•ç”»ID
            instruments: å™¨å…·å®šç¾©
            video_path: å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ï¼‰

        Returns:
            æ¤œå‡ºçµæœ
        """
        logger.info(f"[ANALYSIS] _run_detection started: video_type={video_type}, frames={len(frames)}")
        results = {
            'skeleton_data': [],
            'instrument_data': []
        }

        # å‹•ç”»ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãæ¤œå‡ºå™¨ã®é¸æŠ
        if video_type in [VideoType.EXTERNAL, VideoType.EXTERNAL_NO_INSTRUMENTS]:
            # éª¨æ ¼æ¤œå‡ºã®ã¿
            logger.info(f"[ANALYSIS] Running MediaPipe detection only (no instruments) for video_type: {video_type}")
            detector = HandSkeletonDetector(min_detection_confidence=0.1)
            self.detectors['mediapipe'] = detector

            logger.info(f"[ANALYSIS] Starting MediaPipe batch detection on {len(frames)} frames")
            skeleton_results = detector.detect_batch(frames)
            logger.info(f"[ANALYSIS] MediaPipe detection completed, got {len(skeleton_results)} results")

            # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®çµæœã‚’ç¢ºèª
            if skeleton_results and len(skeleton_results) > 0:
                first_result = skeleton_results[0]
                # å‹ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹
                if isinstance(first_result, dict):
                    logger.info(f"[ANALYSIS] First skeleton result: detected={first_result.get('detected')}, hands={len(first_result.get('hands', []))}")
                else:
                    logger.warning(f"[ANALYSIS] First skeleton result is not dict: type={type(first_result)}")

            results['skeleton_data'] = self._format_skeleton_data(skeleton_results)
            logger.info(f"[ANALYSIS] Formatted {len(results['skeleton_data'])} skeleton data points")

        elif video_type == VideoType.EXTERNAL_WITH_INSTRUMENTS:
            # éª¨æ ¼æ¤œå‡ºã¨å™¨å…·æ¤œå‡ºã®ä¸¡æ–¹
            logger.info("[ANALYSIS] Running both MediaPipe and SAM detection")

            # MediaPipeæ¤œå‡º
            mediapipe_detector = HandSkeletonDetector(min_detection_confidence=0.1)
            self.detectors['mediapipe'] = mediapipe_detector
            skeleton_results = mediapipe_detector.detect_batch(frames)

            # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®çµæœã‚’ç¢ºèª
            if skeleton_results and len(skeleton_results) > 0:
                first_result = skeleton_results[0]
                if isinstance(first_result, dict):
                    logger.info(f"[ANALYSIS] First skeleton result: detected={first_result.get('detected')}, hands={len(first_result.get('hands', []))}")
                else:
                    logger.warning(f"[ANALYSIS] First skeleton result is not dict: type={type(first_result)}")

            results['skeleton_data'] = self._format_skeleton_data(skeleton_results)

            # SAMæ¤œå‡ºï¼ˆä¸€æœ¬åŒ–å®Ÿè£…ï¼‰
            device = getattr(settings, 'SAM_DEVICE', 'cpu')
            if device == 'auto':
                import torch
                device = 'cuda' if torch.cuda.is_available() else 'cpu'

            fps = self.video_info.get('fps', 30.0)
            target_fps = 5.0  # ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºæ™‚ã®FPS

            # å®Ÿé¨“ç‰ˆ: SAM2 Video APIã‚’ä½¿ç”¨ã™ã‚‹ã‹ç¢ºèª
            use_video_api = getattr(settings, 'USE_SAM2_VIDEO_API', False)

            if self.use_sam2 and use_video_api:
                # ğŸ§ª å®Ÿé¨“ç‰ˆ: SAM2 Video API
                logger.info(f"[EXPERIMENTAL] SAM2 Video API: {settings.SAM2_VIDEO_MODEL_TYPE}, device={device}")

                # Configã‹ã‚‰è‡ªå‹•çš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆğŸ†• Phase 3: model_typeæŒ‡å®šï¼‰
                sam_detector = SAM2TrackerVideo(
                    model_type=settings.SAM2_VIDEO_MODEL_TYPE,
                    device=device
                )
                logger.info("[EXPERIMENTAL] SAM2 Video API: Memory Bank + Temporal Context enabled")

                # Video APIã¯å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¸€åº¦ã«å‡¦ç†
                if instruments and len(instruments) > 0 and len(frames) > 0:
                    # ä¿å­˜å½¢å¼ã‹ã‚‰SAMå½¢å¼ã«å¤‰æ›
                    instruments_converted = self._convert_instruments_format(instruments)
                    logger.info(f"[EXPERIMENTAL] Tracking {len(instruments_converted)} instruments across {len(frames)} frames...")

                    # ğŸ†• å™¨å…·åˆæœŸåŒ–å†…å®¹ã‚’è©³ç´°ã«ãƒ­ã‚°
                    for idx, inst in enumerate(instruments_converted):
                        logger.info(f"[INSTRUMENT INIT] [{idx}] id={inst['id']}, name={inst['name']}, selection_type={inst['selection']['type']}")

                    # Video APIã§è¿½è·¡ï¼ˆvideo_pathã¯æ—¢ã«çµ¶å¯¾ãƒ‘ã‚¹ï¼‰
                    logger.info(f"[SAM2 VIDEO] Starting video tracking: path={video_path}")
                    logger.info(f"[SAM2 VIDEO] Video total frames: {self.video_info.get('total_frames', 'unknown')}")
                    logger.info(f"[SAM2 VIDEO] Video duration: {self.video_info.get('duration', 'unknown')}s")
                    logger.info(f"[SAM2 VIDEO] Instruments: {len(instruments_converted)}")

                    try:
                        tracking_result = await sam_detector.track_video(
                            str(video_path),
                            instruments_converted
                        )
                        logger.info(f"[SAM2 VIDEO] Tracking completed successfully")
                    except Exception as e:
                        logger.error(f"[SAM2 VIDEO] Tracking failed: {e}")
                        import traceback
                        logger.error(f"[SAM2 VIDEO] Traceback: {traceback.format_exc()}")
                        raise

                    # çµæœã‚’ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã®å½¢å¼ã«å¤‰æ›
                    # é‡è¦: extraction_resultã‚’ä½¿ç”¨ã—ã¦æ­£ç¢ºãªãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°
                    if self.extraction_result:
                        instrument_results = self._convert_video_api_result(
                            tracking_result,
                            total_frames=len(frames),
                            extraction_result=self.extraction_result
                        )
                    else:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        instrument_results = self._convert_video_api_result(tracking_result, len(frames))
                    logger.info(f"[EXPERIMENTAL] SAM2 Video API completed: {len(instrument_results)} frames processed")
                else:
                    logger.warning("[EXPERIMENTAL] No instruments provided for Video API tracking")
                    instrument_results = []

            elif self.use_sam2:
                # æ—¢å­˜ã®SAM2ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½å‡¦ç†ï¼‰
                logger.info(f"[ANALYSIS] Creating SAM2Tracker with model=small, device={device}")
                sam_detector = SAM2Tracker(model_type="small", device=device)
                logger.info("[ANALYSIS] SAM2 enabled for higher accuracy (+2% Dice, -21% HD95)")

                self.detectors['sam'] = sam_detector

                # å™¨å…·ã®åˆæœŸåŒ–
                if instruments and len(instruments) > 0 and len(frames) > 0:
                    try:
                        # ä¿å­˜å½¢å¼ã‹ã‚‰SAMå½¢å¼ã«å¤‰æ›
                        instruments_converted = self._convert_instruments_format(instruments)
                        logger.info(f"[ANALYSIS] Initializing {len(instruments_converted)} instruments from user selection")
                        sam_detector.initialize_instruments(frames[0], instruments_converted)
                    except Exception as e:
                        logger.error(f"[ANALYSIS] Failed to initialize instruments from user selection: {e}")
                        logger.info("[ANALYSIS] Falling back to automatic instrument detection")
                        sam_detector.auto_detect_instruments(frames[0], max_instruments=5)
                elif len(frames) > 0:
                    logger.info("[ANALYSIS] No user selection, using automatic instrument detection")
                    sam_detector.auto_detect_instruments(frames[0], max_instruments=5)
                else:
                    logger.warning("[ANALYSIS] No frames available for instrument initialization")

                logger.info(f"[ANALYSIS] Running SAM detect_batch on {len(frames)} frames...")
                instrument_results = sam_detector.detect_batch(frames)
                logger.info(f"[ANALYSIS] SAM detection completed, got {len(instrument_results)} results")
            else:
                # SAM1ï¼ˆæ—¢å­˜å®Ÿè£…ï¼‰
                logger.info(f"[ANALYSIS] Creating SAMTrackerUnified with model=vit_h, device={device}, instruments={len(instruments) if instruments else 0}, fps={fps}, target_fps={target_fps}")
                sam_detector = SAMTrackerUnified(model_type="vit_h", device=device)

                self.detectors['sam'] = sam_detector

                # å™¨å…·ã®åˆæœŸåŒ–
                if instruments and len(instruments) > 0 and len(frames) > 0:
                    try:
                        instruments_converted = self._convert_instruments_format(instruments)
                        logger.info(f"[ANALYSIS] Initializing {len(instruments_converted)} instruments from user selection")
                        sam_detector.initialize_instruments(frames[0], instruments_converted)
                    except Exception as e:
                        logger.error(f"[ANALYSIS] Failed to initialize instruments from user selection: {e}")
                        logger.info("[ANALYSIS] Falling back to automatic instrument detection")
                        sam_detector.auto_detect_instruments(frames[0], max_instruments=5)
                elif len(frames) > 0:
                    logger.info("[ANALYSIS] No user selection, using automatic instrument detection")
                    sam_detector.auto_detect_instruments(frames[0], max_instruments=5)
                else:
                    logger.warning("[ANALYSIS] No frames available for instrument initialization")

                logger.info(f"[ANALYSIS] Running SAM detect_batch on {len(frames)} frames...")
                instrument_results = sam_detector.detect_batch(frames)
                logger.info(f"[ANALYSIS] SAM detection completed, got {len(instrument_results)} results")

            # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®çµæœã‚’ç¢ºèª
            if instrument_results and len(instrument_results) > 0:
                first_result = instrument_results[0]
                # å‹ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹
                if isinstance(first_result, dict):
                    logger.info(f"[ANALYSIS] First instrument result: detected={first_result.get('detected')}, instruments={len(first_result.get('instruments', []))}")
                else:
                    logger.warning(f"[ANALYSIS] First instrument result is not dict: type={type(first_result)}")

            results['instrument_data'] = self._format_instrument_data(instrument_results)
            logger.info(f"[ANALYSIS] Formatted instrument data: {len(results['instrument_data'])} frames with detections")

            # Phase 2.2: ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆã‚’åé›†
            self._collect_tracking_stats(sam_detector, instrument_results)

        elif video_type == VideoType.INTERNAL:
            # å†…è¦–é¡ï¼šå™¨å…·æ¤œå‡ºã®ã¿
            logger.info("[ANALYSIS] Running SAM detection only (internal camera)")
            device = getattr(settings, 'SAM_DEVICE', 'cpu')
            if device == 'auto':
                import torch
                device = 'cuda' if torch.cuda.is_available() else 'cpu'

            fps = self.video_info.get('fps', 30.0)
            target_fps = 5.0  # ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºæ™‚ã®FPS

            # SAM2ã¾ãŸã¯SAM1ã‚’é¸æŠ
            if self.use_sam2:
                logger.info(f"[ANALYSIS] Creating SAM2Tracker with model=small, device={device}")
                detector = SAM2Tracker(model_type="small", device=device)
                logger.info("[ANALYSIS] SAM2 enabled for higher accuracy (+2% Dice, -21% HD95)")
            else:
                logger.info(f"[ANALYSIS] Creating SAMTrackerUnified with model=vit_h, device={device}")
                # GPUå¯¾å¿œ: vit_hãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆRTX 3060ã§é«˜é€Ÿãƒ»é«˜ç²¾åº¦ï¼‰
                detector = SAMTrackerUnified(model_type="vit_h", device=device)

            self.detectors['sam'] = detector

            # å™¨å…·ã®åˆæœŸåŒ–
            if instruments and len(instruments) > 0 and len(frames) > 0:
                try:
                    # ä¿å­˜å½¢å¼ã‹ã‚‰SAMå½¢å¼ã«å¤‰æ›
                    instruments_converted = self._convert_instruments_format(instruments)
                    logger.info(f"[ANALYSIS] Initializing {len(instruments_converted)} instruments from user selection")
                    detector.initialize_instruments(frames[0], instruments_converted)
                except Exception as e:
                    logger.error(f"[ANALYSIS] Failed to initialize instruments from user selection: {e}")
                    logger.info("[ANALYSIS] Falling back to automatic instrument detection")
                    detector.auto_detect_instruments(frames[0], max_instruments=5)
            elif len(frames) > 0:
                logger.info("[ANALYSIS] No user selection, using automatic instrument detection for INTERNAL video")
                detector.auto_detect_instruments(frames[0], max_instruments=5)
            else:
                logger.warning("[ANALYSIS] No frames available for instrument initialization")

            instrument_results = detector.detect_batch(frames)
            results['instrument_data'] = self._format_instrument_data(instrument_results)

            # Phase 2.2: ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆã‚’åé›†
            self._collect_tracking_stats(detector, instrument_results)

        else:
            logger.warning(f"Unknown video type: {video_type}, defaulting to MediaPipe only")
            detector = HandSkeletonDetector(min_detection_confidence=0.1)
            self.detectors['mediapipe'] = detector

            skeleton_results = detector.detect_batch(frames)

            # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®çµæœã‚’ç¢ºèª
            if skeleton_results and len(skeleton_results) > 0:
                first_result = skeleton_results[0]
                if isinstance(first_result, dict):
                    logger.info(f"[ANALYSIS] First skeleton result: detected={first_result.get('detected')}, hands={len(first_result.get('hands', []))}")
                else:
                    logger.warning(f"[ANALYSIS] First skeleton result is not dict: type={type(first_result)}")

            results['skeleton_data'] = self._format_skeleton_data(skeleton_results)

        return results

    def _format_skeleton_data(self, raw_results: List[Dict]) -> List[Dict]:
        """
        éª¨æ ¼æ¤œå‡ºçµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰äº’æ›å½¢å¼ï¼‰

        extraction_resultã®frame_indicesã¨timestampsã‚’ä½¿ç”¨ã—ã¦æ­£ç¢ºãªãƒãƒƒãƒ”ãƒ³ã‚°
        """
        from collections import defaultdict

        # extraction_resultãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not self.extraction_result:
            logger.error("[ANALYSIS] extraction_result not available, using fallback")
            fps = self.video_info.get('fps', 30)
            target_fps = getattr(settings, 'FRAME_EXTRACTION_FPS', 15)
            frame_skip = max(1, int(fps / target_fps))

            frames_dict = defaultdict(list)
            for result in raw_results:
                if not isinstance(result, dict):
                    continue
                if result.get('detected'):
                    if 'frame_index' not in result:
                        raise ValueError(f"Missing frame_index in skeleton result")
                    frame_idx = result['frame_index']
                    actual_frame_number = frame_idx * frame_skip
                    timestamp = actual_frame_number / fps if fps > 0 else frame_idx / 30.0

                    for hand in result.get('hands', []):
                        hand_data = {
                            'hand_type': hand.get('hand_type', hand.get('label', 'Unknown')),
                            'landmarks': hand.get('landmarks', {}),
                            'palm_center': hand.get('palm_center', {}),
                            'finger_angles': hand.get('finger_angles', {}),
                            'hand_openness': hand.get('hand_openness', 0.0)
                        }
                        frames_dict[actual_frame_number].append(hand_data)

            formatted = []
            for frame_number in sorted(frames_dict.keys()):
                timestamp = frame_number / fps if fps > 0 else frame_number / 30.0
                formatted.append({
                    'frame': frame_number,
                    'frame_number': frame_number,
                    'timestamp': timestamp,
                    'hands': frames_dict[frame_number]
                })
            return formatted

        # æ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯: extraction_resultã‚’ä½¿ç”¨
        logger.info(f"[ANALYSIS] _format_skeleton_data using extraction_result: "
                   f"{len(self.extraction_result.frame_indices)} frame_indices, "
                   f"{len(self.extraction_result.timestamps)} timestamps")

        frames_dict = defaultdict(list)
        for result in raw_results:
            if not isinstance(result, dict):
                logger.warning(f"Skipping non-dict result: type={type(result)}")
                continue
            if result.get('detected'):
                # Fail Fast: frame_indexãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
                if 'frame_index' not in result:
                    error_msg = f"Missing frame_index in skeleton detection result. Result keys: {list(result.keys())}"
                    logger.error(error_msg)
                    raise ValueError(f"skeleton_detector.detect_batch() must include frame_index in results. {error_msg}")

                frame_idx = result['frame_index']

                # extraction_resultã‹ã‚‰æ­£ç¢ºãªå€¤ã‚’å–å¾—
                if frame_idx >= len(self.extraction_result.frame_indices):
                    logger.warning(f"[ANALYSIS] Frame {frame_idx} exceeds extraction_result length")
                    continue

                actual_frame_number = self.extraction_result.frame_indices[frame_idx]
                timestamp = self.extraction_result.timestamps[frame_idx]

                for hand in result.get('hands', []):
                    hand_data = {
                        'hand_type': hand.get('hand_type', hand.get('label', 'Unknown')),
                        'landmarks': hand.get('landmarks', {}),
                        'palm_center': hand.get('palm_center', {}),
                        'finger_angles': hand.get('finger_angles', {}),
                        'hand_openness': hand.get('hand_openness', 0.0)
                    }
                    frames_dict[actual_frame_number].append(hand_data)

        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰å½¢å¼ã«å¤‰æ›: 1ãƒ•ãƒ¬ãƒ¼ãƒ  = 1ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°ã®æ‰‹ã‚’å«ã‚€ï¼‰
        formatted = []
        for frame_number in sorted(frames_dict.keys()):
            # extraction_resultã‹ã‚‰å¯¾å¿œã™ã‚‹ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
            frame_idx = self.extraction_result.frame_indices.index(frame_number) if frame_number in self.extraction_result.frame_indices else None
            if frame_idx is not None:
                timestamp = self.extraction_result.timestamps[frame_idx]
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                fps = self.video_info.get('fps', 30)
                timestamp = frame_number / fps

            formatted.append({
                'frame': frame_number,
                'frame_number': frame_number,
                'timestamp': timestamp,
                'hands': frames_dict[frame_number]
            })

        logger.info(f"Formatted {len(formatted)} skeleton frames with hands data")
        return formatted

    def _format_instrument_data(self, raw_results: List[Dict]) -> List[Dict]:
        """
        å™¨å…·æ¤œå‡ºçµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

        extraction_resultã®frame_indicesã¨timestampsã‚’ä½¿ç”¨ã—ã¦æ­£ç¢ºãªãƒãƒƒãƒ”ãƒ³ã‚°
        """
        formatted = []

        # extraction_resultãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not self.extraction_result:
            logger.error("[ANALYSIS] extraction_result not available for instrument data, using fallback")
            fps = self.video_info.get('fps', 30)
            target_fps = getattr(settings, 'FRAME_EXTRACTION_FPS', 15)
            frame_skip = max(1, int(fps / target_fps))

            for frame_idx, result in enumerate(raw_results):
                if not isinstance(result, dict):
                    continue
                actual_frame_number = frame_idx * frame_skip
                timestamp = actual_frame_number / fps if fps > 0 else frame_idx / 30.0
                instruments = result.get('instruments', result.get('detections', []))
                formatted.append({
                    'frame_number': actual_frame_number,
                    'timestamp': timestamp,
                    'detections': instruments
                })
            return formatted

        # æ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯: extraction_resultã‚’ä½¿ç”¨
        logger.info(f"[ANALYSIS] _format_instrument_data using extraction_result: "
                   f"{len(self.extraction_result.frame_indices)} frame_indices")

        for frame_idx, result in enumerate(raw_results):
            if not isinstance(result, dict):
                logger.warning(f"[ANALYSIS] Skipping non-dict instrument result: type={type(result)}")
                continue

            if frame_idx >= len(self.extraction_result.frame_indices):
                logger.warning(f"[ANALYSIS] Instrument frame {frame_idx} exceeds extraction_result length")
                break

            # extraction_resultã‹ã‚‰æ­£ç¢ºãªå€¤ã‚’å–å¾—
            actual_frame_number = self.extraction_result.frame_indices[frame_idx]
            timestamp = self.extraction_result.timestamps[frame_idx]

            # SAM2 Video APIã¯'instruments'ã‚­ãƒ¼ã€SAMTrackerUnifiedã¯'detections'ã‚­ãƒ¼ã‚’ä½¿ã†
            instruments = result.get('instruments', result.get('detections', []))

            # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã¨æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç¢ºèª
            if frame_idx == 0 or frame_idx >= 110:
                logger.info(f"[ANALYSIS] Instrument frame {frame_idx}: "
                          f"actual_frame={actual_frame_number}, "
                          f"timestamp={timestamp:.3f}s, "
                          f"instruments_count={len(instruments)}")

            formatted.append({
                'frame_number': actual_frame_number,
                'timestamp': timestamp,
                'detections': instruments
            })

        logger.info(f"Formatted {len(formatted)} instrument detections with correct timestamps")
        return formatted

    async def _calculate_metrics(self, detection_results: Dict) -> Dict:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        metrics = {}

        # éª¨æ ¼ãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if detection_results.get('skeleton_data'):
            calculator = MetricsCalculator(fps=self.video_info.get('fps', 30))
            metrics['skeleton_metrics'] = calculator.calculate_all_metrics(
                detection_results['skeleton_data']
            )

        # å™¨å…·ãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆå°†æ¥çš„ã«å®Ÿè£…ï¼‰
        if detection_results.get('instrument_data'):
            metrics['instrument_metrics'] = {
                'total_detections': len(detection_results['instrument_data'])
            }

        logger.info(f"Calculated metrics: {list(metrics.keys())}")
        return metrics

    async def _calculate_scores(self, metrics: Dict) -> Dict:
        """ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        scores = {
            'overall_score': 0,
            'efficiency_score': 0,
            'smoothness_score': 0,
            'accuracy_score': 0
        }

        if 'skeleton_metrics' in metrics:
            skeleton_metrics = metrics['skeleton_metrics']

            # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå°†æ¥çš„ã«æ”¹å–„ï¼‰
            if 'velocity' in skeleton_metrics:
                avg_velocity = skeleton_metrics['velocity'].get('average', 0)
                scores['efficiency_score'] = min(100, avg_velocity * 10)

            if 'jerk' in skeleton_metrics:
                avg_jerk = skeleton_metrics['jerk'].get('average', 0)
                scores['smoothness_score'] = max(0, 100 - avg_jerk * 5)

            # ç·åˆã‚¹ã‚³ã‚¢
            scores['overall_score'] = (
                scores['efficiency_score'] * 0.4 +
                scores['smoothness_score'] * 0.6
            )

        logger.info(f"Calculated scores: {scores}")
        return scores

    async def _save_results(
        self,
        analysis_result: AnalysisResult,
        detection_results: Dict,
        metrics: Dict,
        scores: Dict,
        db
    ):
        """çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆnumpyå‹å¤‰æ›ã¨ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ä»˜ãï¼‰"""
        skeleton_data = detection_results.get('skeleton_data', [])
        instrument_data = detection_results.get('instrument_data', [])

        logger.info(f"[ANALYSIS] _save_results: skeleton_data length = {len(skeleton_data)}")
        logger.info(f"[ANALYSIS] _save_results: instrument_data length = {len(instrument_data)}")

        # skeleton_dataã¯å³åº§ã«å‹å¤‰æ›ï¼ˆåœ§ç¸®ä¸è¦ï¼‰
        logger.info(f"[ANALYSIS] Converting skeleton_data numpy types...")
        skeleton_data = convert_numpy_types(skeleton_data)

        # instrument_dataã¯åœ§ç¸®ã—ã¦ã‹ã‚‰å‹å¤‰æ›ï¼ˆmaskâ†’contourå¤‰æ›ãŒå¿…è¦ï¼‰
        if instrument_data:
            logger.info(f"[ANALYSIS] Compressing instrument_data (maskâ†’contour)...")
            instrument_data = self._compress_instrument_data(instrument_data)
            compressed_size = len(json.dumps(instrument_data))
            logger.info(f"[ANALYSIS] Compressed instrument_data: {compressed_size} characters")

        # åœ§ç¸®å¾Œã«å‹å¤‰æ›ï¼ˆæ®‹ã‚Šã®numpyå‹ã‚’Pythonå‹ã«ï¼‰
        logger.info(f"[ANALYSIS] Converting remaining numpy types...")
        instrument_data = convert_numpy_types(instrument_data)
        metrics = convert_numpy_types(metrics)
        scores = convert_numpy_types(scores)

        analysis_result.skeleton_data = skeleton_data
        analysis_result.instrument_data = instrument_data
        analysis_result.motion_analysis = metrics
        analysis_result.scores = scores
        analysis_result.total_frames = self.video_info.get('total_frames', 0)
        analysis_result.status = AnalysisStatus.COMPLETED
        # JSTæ™‚åˆ»ã§ä¿å­˜
        jst = pytz.timezone('Asia/Tokyo')
        analysis_result.completed_at = datetime.now(jst).replace(tzinfo=None)
        analysis_result.progress = 100

        # Phase 2.2: ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆã¨è­¦å‘Šã‚’ä¿å­˜
        if self.tracking_stats:
            analysis_result.tracking_stats = json.dumps(self.tracking_stats)
            logger.info(f"[ANALYSIS] Saved tracking_stats: {list(self.tracking_stats.keys())}")

        if self.warnings:
            analysis_result.warnings = json.dumps(self.warnings)
            logger.info(f"[ANALYSIS] Saved {len(self.warnings)} warnings")

        db.commit()
        logger.info(f"[ANALYSIS] Results saved for analysis_id: {analysis_result.id}")

    def _collect_tracking_stats(self, detector, instrument_results: List[Dict]):
        """
        ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆã‚’åé›†ã™ã‚‹ï¼ˆPhase 2.2ï¼‰

        Args:
            detector: SAMTrackerUnifiedã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            instrument_results: å™¨å…·æ¤œå‡ºçµæœ
        """
        try:
            # SAMTrackerUnifiedã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
            if hasattr(detector, 'get_tracking_stats'):
                tracker_stats = detector.get_tracking_stats()

                # å™¨å…·ã”ã¨ã®çµ±è¨ˆ
                for inst_key, inst_stats in tracker_stats.get('instruments', {}).items():
                    if inst_key not in self.tracking_stats:
                        self.tracking_stats[inst_key] = {}

                    self.tracking_stats[inst_key]['max_lost_count'] = inst_stats.get('lost_frames', 0)
                    self.tracking_stats[inst_key]['last_score'] = inst_stats.get('last_score', 0.0)
                    self.tracking_stats[inst_key]['trajectory_length'] = inst_stats.get('trajectory_length', 0)

            # å†æ¤œå‡ºã‚¤ãƒ™ãƒ³ãƒˆã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            re_detection_count = {}
            for frame_data in instrument_results:
                if isinstance(frame_data, dict):
                    detections = frame_data.get('detections', [])
                    for detection in detections:
                        if detection.get('redetected'):
                            track_id = detection.get('track_id', 0)
                            inst_key = f"instrument_{track_id}"

                            if inst_key not in re_detection_count:
                                re_detection_count[inst_key] = 0
                            re_detection_count[inst_key] += 1

            # å†æ¤œå‡ºã‚«ã‚¦ãƒ³ãƒˆã‚’tracking_statsã«è¿½åŠ 
            for inst_key, count in re_detection_count.items():
                if inst_key not in self.tracking_stats:
                    self.tracking_stats[inst_key] = {}
                self.tracking_stats[inst_key]['re_detections'] = count

            # ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã¨æ¤œå‡ºãƒ•ãƒ¬ãƒ¼ãƒ æ•°
            total_frames = len(instrument_results)
            detected_frames = sum(
                1 for frame_data in instrument_results
                if isinstance(frame_data, dict) and len(frame_data.get('detections', [])) > 0
            )

            self.tracking_stats['summary'] = {
                'total_frames': total_frames,
                'detected_frames': detected_frames,
                'detection_rate': detected_frames / total_frames if total_frames > 0 else 0
            }

            logger.info(f"[ANALYSIS] Collected tracking stats: {list(self.tracking_stats.keys())}")

        except Exception as e:
            logger.warning(f"[ANALYSIS] Failed to collect tracking stats: {e}")

    def _compress_instrument_data(self, instrument_data: List[Dict]) -> List[Dict]:
        """
        å¤§å®¹é‡ã®å™¨å…·è¿½è·¡ãƒ‡ãƒ¼ã‚¿ã‚’åœ§ç¸®

        æ³¨æ„ï¼šã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯_format_instrument_dataã®å‡ºåŠ›ã‚’å—ã‘å–ã‚‹
        _format_instrument_dataã¯ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã™ã‚‹ï¼š
        {
            'frame_number': int,
            'timestamp': float,
            'detections': [  # â† 'instruments'ã§ã¯ãªã'detections'
                {
                    'id': int,
                    'name': str,
                    'center': [x, y],
                    'bbox': [x1, y1, x2, y2],
                    'confidence': float,
                    'mask': array (optional)
                }
            ]
        }

        Args:
            instrument_data: å™¨å…·è¿½è·¡ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ

        Returns:
            åœ§ç¸®ã•ã‚ŒãŸå™¨å…·è¿½è·¡ãƒ‡ãƒ¼ã‚¿
        """
        if not instrument_data:
            return []

        total_frames = len(instrument_data)
        logger.info(f"[ANALYSIS] Compressing {total_frames} frames of instrument data")

        # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ§‹é€ ã‚’ç¢ºèª
        if total_frames > 0:
            first_frame = instrument_data[0]
            logger.info(f"[ANALYSIS] Compression input - First frame keys: {list(first_frame.keys())}")
            detections_key = 'detections' if 'detections' in first_frame else 'instruments'
            det_count = len(first_frame.get(detections_key, []))
            logger.info(f"[ANALYSIS] Compression input - First frame {detections_key} count: {det_count}")
            if det_count > 0:
                first_det = first_frame[detections_key][0]
                logger.info(f"[ANALYSIS] Compression input - First detection keys: {list(first_det.keys())}")

        # maskãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»ã—ã¦åœ§ç¸®
        compressed_data = []

        # ãƒ‡ãƒãƒƒã‚°: åœ§ç¸®é–‹å§‹æ™‚ã®ãƒ­ã‚°
        logger.warning(f"[CONTOUR_DEBUG] Starting compression, total frames: {len(instrument_data)}")

        for frame_idx, frame_data in enumerate(instrument_data):
            # _format_instrument_dataãŒä½¿ç”¨ã™ã‚‹ã‚­ãƒ¼åã«å¯¾å¿œ
            compressed_frame = {
                'frame_number': frame_data.get('frame_number'),  # frame_index ã§ã¯ãªã frame_number
                'timestamp': frame_data.get('timestamp', 0.0),
                'detections': []  # instruments ã§ã¯ãªã detections
            }

            # SAM2 Video APIã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œ
            detections = frame_data.get('detections', [])

            # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã§å¿…ãšãƒ­ã‚°å‡ºåŠ›
            if frame_idx == 0:
                logger.warning(f"[CONTOUR_DEBUG] Frame 0 has {len(detections)} detections")

            for det_idx, det in enumerate(detections):
                # ãƒ‡ãƒãƒƒã‚°: detectionã®æ§‹é€ ã‚’ç¢ºèªï¼ˆWARNINGãƒ¬ãƒ™ãƒ«ã§ç¢ºå®Ÿã«å‡ºåŠ›ï¼‰
                if frame_idx == 0 and det_idx == 0:
                    logger.warning(f"[CONTOUR_DEBUG] First detection keys: {list(det.keys())}")
                    logger.warning(f"[CONTOUR_DEBUG] First detection has 'mask': {'mask' in det}")
                    if 'mask' in det:
                        mask_data = det.get('mask')
                        logger.warning(f"[CONTOUR_DEBUG] Mask type: {type(mask_data)}, is None: {mask_data is None}")
                        if isinstance(mask_data, np.ndarray):
                            logger.warning(f"[CONTOUR_DEBUG] Mask shape: {mask_data.shape}, dtype: {mask_data.dtype}, sum: {mask_data.sum()}")

                compressed_det = {
                    'id': det.get('id'),
                    'name': det.get('name', ''),
                    'center': det.get('center', []),
                    'bbox': det.get('bbox', []),
                    'confidence': det.get('confidence', 0.0),
                    'contour': self._extract_mask_contour(det.get('mask'))  # ãƒã‚¹ã‚¯è¼ªéƒ­ã‚’æŠ½å‡º
                    # maské…åˆ—ã¯é™¤å¤–ï¼ˆ588MBâ†’æ•°ç™¾KBï¼‰ã€ä»£ã‚ã‚Šã«contouråº§æ¨™ã‚’ä¿å­˜
                }

                compressed_frame['detections'].append(compressed_det)

            compressed_data.append(compressed_frame)

        # åœ§ç¸®çµæœã‚’ç¢ºèª
        frames_with_dets = sum(1 for f in compressed_data if len(f.get('detections', [])) > 0)
        logger.info(f"[ANALYSIS] After mask removal: {frames_with_dets}/{total_frames} frames have detections")

        # 500KBè¶…éã®å ´åˆã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§å‰Šæ¸›
        compressed_json = json.dumps(compressed_data)
        compressed_size = len(compressed_json)
        logger.info(f"[ANALYSIS] Compressed data size: {compressed_size} characters")

        if compressed_size > 500000:
            logger.warning(f"[ANALYSIS] Still too large ({compressed_size} chars), sampling frames...")
            summary_data = []

            # æœ€åˆã®10ãƒ•ãƒ¬ãƒ¼ãƒ 
            summary_data.extend(compressed_data[:10])

            # 10ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«
            for i in range(10, total_frames - 10, 10):
                summary_data.append(compressed_data[i])

            # æœ€å¾Œã®10ãƒ•ãƒ¬ãƒ¼ãƒ 
            if total_frames > 20:
                summary_data.extend(compressed_data[-10:])

            sampled_frames_with_dets = sum(1 for f in summary_data if len(f.get('detections', [])) > 0)
            logger.info(f"[ANALYSIS] Sampled {len(summary_data)} frames from {total_frames} total")
            logger.info(f"[ANALYSIS] Sampled data: {sampled_frames_with_dets}/{len(summary_data)} frames have detections")
            return summary_data

        return compressed_data

    async def _update_status(
        self,
        analysis_result: AnalysisResult,
        status: str,
        db,
        progress: int = None
    ):
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ã¨WebSocketé€šçŸ¥ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        analysis_result.current_step = status
        if progress is not None:
            analysis_result.progress = progress

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’DBã®statusãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã‚‚åæ˜ 
        if status == "completed":
            analysis_result.status = AnalysisStatus.COMPLETED
        elif status in ["initialization", "frame_extraction", "skeleton_detection",
                       "instrument_detection", "motion_analysis", "report_generation"]:
            analysis_result.status = AnalysisStatus.PROCESSING

        db.commit()

        # WebSocketé€šçŸ¥ï¼ˆè©³ç´°æƒ…å ±ä»˜ãï¼‰
        await manager.send_progress(
            analysis_result.id,
            {
                'type': 'status_update',
                'status': status,
                'current_step': status,
                'progress': progress or analysis_result.progress,
                'message': self._get_step_message(status, progress)
            }
        )

        logger.info(f"Updated status: {status}, progress: {progress}")

    def _get_step_message(self, step: str, progress: int = None) -> str:
        """å„ã‚¹ãƒ†ãƒƒãƒ—ã®èª¬æ˜ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™"""
        messages = {
            "initialization": "å‹•ç”»æƒ…å ±ã‚’å–å¾—ã—ã¦ã„ã¾ã™...",
            "frame_extraction": "ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™...",
            "skeleton_detection": "éª¨æ ¼ã‚’æ¤œå‡ºã—ã¦ã„ã¾ã™...",
            "instrument_detection": "å™¨å…·ã‚’èªè­˜ã—ã¦ã„ã¾ã™...",
            "motion_analysis": "ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è§£æã—ã¦ã„ã¾ã™...",
            "report_generation": "ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...",
            "completed": "è§£æãŒå®Œäº†ã—ã¾ã—ãŸï¼"
        }
        return messages.get(step, f"å‡¦ç†ä¸­... ({progress}%)" if progress else "å‡¦ç†ä¸­...")

    def _convert_video_api_result(
        self,
        tracking_result: Dict[str, Any],
        total_frames: int,
        extraction_result: Optional[ExtractionResult] = None
    ) -> List[Dict[str, Any]]:
        """
        SAM2 Video APIã®çµæœã‚’æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ï¼‰ã«å¤‰æ›

        Args:
            tracking_result: Video APIã®çµæœ
            total_frames: æŠ½å‡ºã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ æ•°
            extraction_result: ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºçµæœï¼ˆframe_indicesãƒãƒƒãƒ”ãƒ³ã‚°ç”¨ï¼‰

        Returns:
            ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã®çµæœãƒªã‚¹ãƒˆ
        """
        logger.info("[EXPERIMENTAL] Converting Video API result to frame-based format...")

        # ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã®çµæœã‚’åˆæœŸåŒ–
        frame_results = [
            {"detected": False, "instruments": []}
            for _ in range(total_frames)
        ]

        # å„å™¨å…·ã®è»Œè·¡ã‚’ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã«åˆ†é…
        instruments_data = tracking_result.get("instruments", [])

        # extraction_resultãŒã‚ã‚‹å ´åˆã€å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·â†’æŠ½å‡ºã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        video_frame_to_extract_idx = {}
        if extraction_result:
            for idx, video_frame_num in enumerate(extraction_result.frame_indices):
                video_frame_to_extract_idx[video_frame_num] = idx
            logger.info(f"[EXPERIMENTAL] Created frame mapping: {len(video_frame_to_extract_idx)} video frames â†’ extract indices")

        for inst_data in instruments_data:
            inst_id = inst_data["instrument_id"]
            inst_name = inst_data["name"]
            trajectory = inst_data["trajectory"]

            for point_idx, point in enumerate(trajectory):
                # SAM2 Video APIã¯å‹•ç”»å†…ã®å®Ÿéš›ã®ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã‚’è¿”ã™
                video_frame_idx = point["frame_index"]

                # æŠ½å‡ºã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
                if extraction_result and video_frame_idx in video_frame_to_extract_idx:
                    extract_idx = video_frame_to_extract_idx[video_frame_idx]
                else:
                    # ğŸ› FIX: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å‰Šé™¤ã—ã¦ã‚¹ã‚­ãƒƒãƒ—
                    # å•é¡Œ: SAM2ãŒé€£ç¶šãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·(0,1,2,3...)ã‚’è¿”ã™ã®ã«å¯¾ã—ã€
                    #       extraction_resultã«ã¯é–“å¼•ãå¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ (0,2,4,6...)ã®ã¿å«ã¾ã‚Œã‚‹
                    # çµæœ: video_frame=1ãŒextract_idx=1ã«é…ç½®ã•ã‚Œã€video_frame=2ã‚‚
                    #       extract_idx=1ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã‚‹ã“ã¨ã§é‡è¤‡ãŒç™ºç”Ÿã—ã¦ã„ãŸ
                    if point_idx < 5:  # æœ€åˆã®æ•°å›ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                        logger.info(f"[SKIP] Inst {inst_id}, Point {point_idx}: video_frame={video_frame_idx} not in extraction mapping, skipping")
                    continue  # ã“ã®trajectory pointã‚’ã‚¹ã‚­ãƒƒãƒ—

                if 0 <= extract_idx < total_frames:
                    frame_results[extract_idx]["detected"] = True
                    frame_results[extract_idx]["instruments"].append({
                        "id": inst_id,
                        "name": inst_name,
                        "center": point["center"],
                        "bbox": point["bbox"],
                        "confidence": point["confidence"],
                        "mask": point.get("mask")
                    })

        # æ¤œå‡ºãŒã‚ã£ãŸãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        detected_frames = sum(1 for fr in frame_results if fr["detected"])
        logger.info(f"[EXPERIMENTAL] Converted to frame-based format: {detected_frames}/{total_frames} frames with detections")

        return frame_results

    def _extract_mask_contour(self, mask: np.ndarray) -> List[List[int]]:
        """
        ãƒã‚¹ã‚¯ã‹ã‚‰è¼ªéƒ­åº§æ¨™ã‚’æŠ½å‡ºï¼ˆè»½é‡åŒ–ï¼‰

        Args:
            mask: numpyé…åˆ—ã®ãƒã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ (H, W)

        Returns:
            è¼ªéƒ­åº§æ¨™ã®ãƒªã‚¹ãƒˆ [[x, y], [x, y], ...]
        """
        # ãƒ‡ãƒãƒƒã‚°: maskã®å‹ã¨å†…å®¹ã‚’ç¢ºèªï¼ˆWARNINGãƒ¬ãƒ™ãƒ«ã§ç¢ºå®Ÿã«å‡ºåŠ›ï¼‰
        logger.warning(f"[CONTOUR_DEBUG] _extract_mask_contour called, mask type: {type(mask)}, is None: {mask is None}")

        if mask is None:
            logger.warning("[CONTOUR_DEBUG] Mask is None, returning empty contour")
            return []

        if not isinstance(mask, np.ndarray):
            logger.warning(f"[CONTOUR_DEBUG] Mask is not numpy array (type: {type(mask)}), returning empty contour")
            return []

        try:
            # ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ã«å¤‰æ›
            if mask.dtype == np.float32 or mask.dtype == np.float64:
                binary_mask = (mask > 0.5).astype(np.uint8)
            else:
                binary_mask = mask.astype(np.uint8)

            # ãƒã‚¹ã‚¯ãŒç©ºã®å ´åˆ
            if binary_mask.sum() == 0:
                return []

            # è¼ªéƒ­æŠ½å‡º
            contours, _ = cv2.findContours(
                binary_mask,
                cv2.RETR_EXTERNAL,
                cv2.CHAIN_APPROX_SIMPLE
            )

            if len(contours) == 0:
                return []

            # æœ€å¤§ã®è¼ªéƒ­ã‚’ä½¿ç”¨
            largest_contour = max(contours, key=cv2.contourArea)

            # åº§æ¨™æ•°ã‚’å‰Šæ¸›ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºè»½é‡åŒ–: 0.3%ã®ç²¾åº¦ã§è¿‘ä¼¼ï¼‰
            epsilon = 0.003 * cv2.arcLength(largest_contour, True)
            approx = cv2.approxPolyDP(largest_contour, epsilon, True)

            # [[x, y], [x, y], ...] å½¢å¼ã«å¤‰æ›
            contour_points = approx.reshape(-1, 2).tolist()

            logger.debug(f"[CONTOUR] Extracted {len(contour_points)} points from mask shape={mask.shape}")

            return contour_points

        except Exception as e:
            logger.warning(f"[CONTOUR] Failed to extract contour: {e}")
            return []

    async def _analyze_eye_gaze(
        self,
        video: Video,
        analysis_result: AnalysisResult,
        analysis_id: str,
        db: Session
    ) -> Dict[str, Any]:
        """
        è¦–ç·šè§£æå°‚ç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆæ—¢å­˜æ©Ÿèƒ½ã¨å®Œå…¨ç‹¬ç«‹ï¼‰

        Args:
            video: Video model
            analysis_result: AnalysisResult model
            analysis_id: è§£æID
            db: Database session

        Returns:
            è§£æçµæœè¾æ›¸
        """
        logger.info(f"[GAZE] === Starting Eye Gaze Analysis ===")
        logger.info(f"[GAZE] video_id: {video.id}")
        logger.info(f"[GAZE] analysis_id: {analysis_id}")

        try:
            # å‹•ç”»ãƒ‘ã‚¹ã®å–å¾—
            video_path = Path(video.file_path)
            if not video_path.is_absolute():
                backend_dir = Path(__file__).parent.parent.parent
                video_path = backend_dir / video_path

            if not video_path.exists():
                raise FileNotFoundError(f"Video file not found: {video_path}")

            logger.info(f"[GAZE] Video path: {video_path}")

            # 1. GazeAnalyzeråˆæœŸåŒ–ï¼ˆé…å»¶åˆæœŸåŒ–ï¼‰
            await self._update_status(analysis_result, "initialization", db, progress=5)
            if self.gaze_analyzer is None:
                logger.info("[GAZE] Initializing GazeAnalyzer...")
                loop = asyncio.get_event_loop()
                self.gaze_analyzer = await loop.run_in_executor(
                    None,
                    lambda: GazeAnalyzer(device="auto")
                )
                logger.info("[GAZE] GazeAnalyzer initialized")
            await self._update_status(analysis_result, "initialization", db, progress=10)

            # 2. ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºï¼ˆæ—¢å­˜FrameExtractionServiceå†åˆ©ç”¨ï¼‰
            logger.info("[GAZE] Starting frame extraction...")
            await self._update_status(analysis_result, "frame_extraction", db, progress=15)

            loop = asyncio.get_event_loop()
            extraction_result = await loop.run_in_executor(
                None,
                self.frame_extraction_service.extract_frames,
                str(video_path)
            )

            frames = extraction_result.frames
            logger.info(f"[GAZE] Extracted {len(frames)} frames")
            await self._update_status(analysis_result, "frame_extraction", db, progress=30)

            # 3. å„ãƒ•ãƒ¬ãƒ¼ãƒ ã§è¦–ç·šè§£æ
            logger.info("[GAZE] Starting gaze analysis...")
            await self._update_status(analysis_result, "gaze_detection", db, progress=35)

            gaze_results = []
            total_fixations = 0
            attention_hotspots = {}  # {(x, y): count}

            # è§£æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
            gaze_params = {
                'center_bias_weight': 0.6,
                'saccade_radius': 60,
                'ior_decay': 0.9,
                'add_corner_seeds': True,
                'num_fixations': 8,
                'gamma': 1.2,
                'blur_sigma': 5,
                'alpha': 0.6,
                'heat_threshold': 0.1,
                'circle_size': 6,
                'line_thickness': 2,
                'show_numbers': False
            }

            for idx, frame in enumerate(frames):
                # é€²æ—æ›´æ–°ï¼ˆ35% â†’ 85%ï¼‰
                progress = 35 + int((idx / len(frames)) * 50)
                if idx % 10 == 0:  # 10ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«æ›´æ–°
                    await self._update_status(analysis_result, "gaze_detection", db, progress=progress)
                    await manager.send_progress(analysis_id, {
                        "type": "progress",
                        "step": "gaze_detection",
                        "progress": progress,
                        "message": f"è¦–ç·šè§£æä¸­: {idx}/{len(frames)} ãƒ•ãƒ¬ãƒ¼ãƒ "
                    })

                try:
                    # GazeAnalyzerã§è§£æï¼ˆåŒæœŸé–¢æ•°ãªã®ã§executorã§å®Ÿè¡Œï¼‰
                    result = await loop.run_in_executor(
                        None,
                        self.gaze_analyzer.analyze_frame,
                        frame,
                        gaze_params
                    )

                    # å›ºè¦–ç‚¹ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    fixations = result['fixations']
                    total_fixations += len(fixations)

                    # ãƒ›ãƒƒãƒˆã‚¹ãƒãƒƒãƒˆé›†è¨ˆ
                    for fx, fy in fixations:
                        # 20x20ãƒ”ã‚¯ã‚»ãƒ«ã®ã‚°ãƒªãƒƒãƒ‰ã«ä¸¸ã‚ã‚‹
                        grid_x = (fx // 20) * 20
                        grid_y = (fy // 20) * 20
                        key = (grid_x, grid_y)
                        attention_hotspots[key] = attention_hotspots.get(key, 0) + 1

                    # çµæœã‚’ä¿å­˜
                    gaze_results.append({
                        'frame_index': idx,
                        'timestamp': extraction_result.timestamps[idx],
                        'fixations': fixations,
                        'stats': result['stats']
                    })

                except Exception as e:
                    logger.warning(f"[GAZE] Frame {idx} analysis failed: {e}")
                    gaze_results.append({
                        'frame_index': idx,
                        'timestamp': extraction_result.timestamps[idx],
                        'fixations': [],
                        'stats': {'max_value': 0, 'mean_value': 0, 'high_attention_ratio': 0}
                    })

            await self._update_status(analysis_result, "gaze_detection", db, progress=85)
            logger.info(f"[GAZE] Gaze analysis completed for {len(gaze_results)} frames")

            # 4. ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
            await self._update_status(analysis_result, "report_generation", db, progress=90)

            # ãƒˆãƒƒãƒ—5ãƒ›ãƒƒãƒˆã‚¹ãƒãƒƒãƒˆ
            top_hotspots = sorted(attention_hotspots.items(), key=lambda x: x[1], reverse=True)[:5]
            top_hotspot_coords = [list(coord) for coord, _ in top_hotspots]

            summary = {
                'total_frames': len(frames),
                'total_fixations': total_fixations,
                'average_fixations_per_frame': total_fixations / len(frames) if frames else 0,
                'attention_hotspots': top_hotspot_coords,
                'effective_fps': extraction_result.effective_fps,
                'total_duration': extraction_result.timestamps[-1] if extraction_result.timestamps else 0
            }

            # 5. çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            gaze_data = {
                'frames': convert_numpy_types(gaze_results),
                'summary': convert_numpy_types(summary),
                'params': convert_numpy_types(gaze_params)
            }

            # Ensure all data is JSON serializable
            analysis_result.gaze_data = convert_numpy_types(gaze_data)
            analysis_result.total_frames = len(frames)
            analysis_result.status = AnalysisStatus.COMPLETED
            analysis_result.completed_at = get_jst_now()

            db.commit()
            db.refresh(analysis_result)

            await self._update_status(analysis_result, "completed", db, progress=100)
            await manager.send_progress(analysis_id, {
                "type": "complete",
                "step": "completed",
                "progress": 100,
                "message": "è¦–ç·šè§£æãŒå®Œäº†ã—ã¾ã—ãŸ"
            })

            logger.info(f"[GAZE] === Eye Gaze Analysis Completed ===")
            logger.info(f"[GAZE] Total fixations: {total_fixations}")
            logger.info(f"[GAZE] Average fixations/frame: {summary['average_fixations_per_frame']:.2f}")

            return {
                'status': 'success',
                'video_id': video.id,
                'analysis_id': analysis_id,
                'gaze_data': gaze_data
            }

        except Exception as e:
            logger.error(f"[GAZE] Eye gaze analysis failed: {str(e)}")
            logger.error(f"[GAZE] Error type: {type(e).__name__}")
            import traceback
            error_traceback = traceback.format_exc()
            logger.error(f"[GAZE] Traceback: {error_traceback}")

            analysis_result.status = AnalysisStatus.FAILED
            analysis_result.error_message = f"{type(e).__name__}: {str(e)}"
            db.commit()

            await manager.send_progress(analysis_id, {
                "type": "error",
                "step": "failed",
                "message": f"è¦–ç·šè§£æã‚¨ãƒ©ãƒ¼: {str(e)}"
            })

            raise