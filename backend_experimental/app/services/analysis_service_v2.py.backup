"""
Analysis Service V2 - Clean architecture implementation
"""
import asyncio
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime
import cv2
import numpy as np
import json

from app.models import SessionLocal
from app.models.analysis import AnalysisResult, AnalysisStatus
from app.models.video import Video, VideoType
from app.core.websocket import manager
from app.core.config import settings
from app.ai_engine.processors.skeleton_detector import HandSkeletonDetector
from app.ai_engine.processors.sam_tracker_unified import SAMTrackerUnified
from app.ai_engine.processors.sam2_tracker import SAM2Tracker
from app.ai_engine.processors.sam2_tracker_video import SAM2TrackerVideo  # å®Ÿé¨“ç‰ˆ
from .metrics_calculator import MetricsCalculator

logger = logging.getLogger(__name__)


def convert_numpy_types(obj):
    """
    Convert numpy types to Python native types for JSON serialization

    Args:
        obj: Object potentially containing numpy types

    Returns:
        Object with all numpy types converted to Python native types
    """
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    return obj


class AnalysisServiceV2:
    """
    ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ãè§£æã‚µãƒ¼ãƒ“ã‚¹
    è²¬å‹™ã®åˆ†é›¢ã¨æ‹¡å¼µæ€§ã‚’é‡è¦–
    """

    def __init__(self):
        self.detectors = {}
        self.video_info = {}
        self.warnings = []  # Phase 2.2: è­¦å‘Šåé›†ç”¨
        self.tracking_stats = {}  # Phase 2.2: ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆåé›†ç”¨
        # SAM2ä½¿ç”¨ãƒ•ãƒ©ã‚°ï¼ˆç’°å¢ƒå¤‰æ•° USE_SAM2=true ã§æœ‰åŠ¹åŒ–ï¼‰
        self.use_sam2 = getattr(settings, 'USE_SAM2', False)

    async def analyze_video(
        self,
        video_id: str,
        analysis_id: str,
        instruments: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        å‹•ç”»è§£æã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

        Args:
            video_id: å‹•ç”»ID
            analysis_id: è§£æID
            instruments: å™¨å…·å®šç¾©ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            è§£æçµæœã®è¾æ›¸
        """
        logger.info(f"[ANALYSIS] === Starting V2 analysis ===")
        logger.info(f"[ANALYSIS] video_id: {video_id}")
        logger.info(f"[ANALYSIS] analysis_id: {analysis_id}")
        logger.info(f"[ANALYSIS] instruments: {instruments}")

        db = SessionLocal()
        try:
            # 1. è§£æãƒ¬ã‚³ãƒ¼ãƒ‰ã¨å‹•ç”»æƒ…å ±ã®å–å¾—
            analysis_result = db.query(AnalysisResult).filter(
                AnalysisResult.id == analysis_id
            ).first()

            if not analysis_result:
                raise ValueError(f"Analysis not found: {analysis_id}")

            video = db.query(Video).filter(Video.id == video_id).first()
            if not video:
                raise ValueError(f"Video not found: {video_id}")

            # Convert relative path to absolute path from backend directory
            video_path = Path(video.file_path)
            if not video_path.is_absolute():
                # Assume file_path is relative to backend directory
                backend_dir = Path(__file__).parent.parent.parent  # backend/
                video_path = backend_dir / video_path

            if not video_path.exists():
                raise FileNotFoundError(f"Video file not found: {video_path}")

            # 2. å‹•ç”»ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãå‡¦ç†æˆ¦ç•¥ã®æ±ºå®š
            video_type = video.video_type
            logger.info(f"[ANALYSIS] Video type: {video_type}")
            logger.info(f"[ANALYSIS] Video path: {video.file_path}")

            # 3. å‹•ç”»æƒ…å ±ã®å–å¾—
            logger.info(f"[ANALYSIS] Getting video info...")
            await self._update_status(analysis_result, "initialization", db, progress=5)
            self.video_info = self._get_video_info(str(video_path))
            logger.info(f"[ANALYSIS] Video info retrieved")
            await self._update_status(analysis_result, "initialization", db, progress=10)
            logger.info(f"[ANALYSIS] Status updated to initialization")

            # 4. ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º
            logger.info(f"[ANALYSIS] Starting frame extraction...")
            await self._update_status(analysis_result, "frame_extraction", db, progress=15)
            frames = await self._extract_frames(video_path)
            logger.info(f"[ANALYSIS] Extracted {len(frames)} frames")
            await self._update_status(analysis_result, "frame_extraction", db, progress=30)

            # 5. æ¤œå‡ºå‡¦ç†ã®å®Ÿè¡Œ
            logger.info(f"[ANALYSIS] Starting detection...")
            await self._update_status(analysis_result, "skeleton_detection", db, progress=35)
            detection_results = await self._run_detection(
                frames, video_type, video_id, instruments, video_path
            )
            logger.info(f"[ANALYSIS] Detection completed with {len(detection_results) if detection_results else 0} results")
            await self._update_status(analysis_result, "instrument_detection", db, progress=60)

            # 6. ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            await self._update_status(analysis_result, "motion_analysis", db, progress=70)
            metrics = await self._calculate_metrics(detection_results)
            await self._update_status(analysis_result, "motion_analysis", db, progress=80)

            # 7. ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            await self._update_status(analysis_result, "report_generation", db, progress=85)
            scores = await self._calculate_scores(metrics)

            # 8. çµæœã®ä¿å­˜
            await self._update_status(analysis_result, "report_generation", db, progress=90)
            await self._save_results(
                analysis_result, detection_results, metrics, scores, db
            )
            await self._update_status(analysis_result, "report_generation", db, progress=95)

            # 9. å®Œäº†é€šçŸ¥
            await self._update_status(analysis_result, "completed", db, progress=100)

            return {
                'status': 'success',
                'video_id': video_id,
                'analysis_id': analysis_id,
                'detection_results': detection_results,
                'metrics': metrics,
                'scores': scores
            }

        except Exception as e:
            logger.error(f"[ANALYSIS] Analysis failed: {str(e)}")
            logger.error(f"[ANALYSIS] Error type: {type(e).__name__}")
            import traceback
            error_traceback = traceback.format_exc()
            logger.error(f"[ANALYSIS] Traceback: {error_traceback}")

            if analysis_result:
                # Phase 2.2: ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è©³ç´°ã«è¨˜éŒ²
                analysis_result.status = AnalysisStatus.FAILED
                analysis_result.error_message = f"{type(e).__name__}: {str(e)}"

                # åé›†ã—ãŸè­¦å‘ŠãŒã‚ã‚Œã°ä¿å­˜
                if self.warnings:
                    analysis_result.warnings = json.dumps(self.warnings)
                    logger.info(f"[ANALYSIS] Saved {len(self.warnings)} warnings to DB")

                # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆãŒã‚ã‚Œã°ä¿å­˜
                if self.tracking_stats:
                    analysis_result.tracking_stats = json.dumps(self.tracking_stats)
                    logger.info(f"[ANALYSIS] Saved tracking stats to DB: {list(self.tracking_stats.keys())}")

                db.commit()

                # WebSocketã§è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’é€ä¿¡
                try:
                    await manager.send_update(analysis_id, {
                        "type": "error",
                        "error_type": type(e).__name__,
                        "message": str(e),
                        "warnings_count": len(self.warnings),
                        "tracking_stats": self.tracking_stats
                    })
                except:
                    pass  # WebSocketé€ä¿¡å¤±æ•—ã¯ç„¡è¦–

            raise
        finally:
            db.close()
            # æ¤œå‡ºå™¨ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            for detector in self.detectors.values():
                if hasattr(detector, 'close'):
                    detector.close()

    def _get_video_info(self, video_path: str) -> Dict:
        """å‹•ç”»æƒ…å ±ã‚’å–å¾—"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {video_path}")

        info = {
            'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
            'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
            'fps': cap.get(cv2.CAP_PROP_FPS),
            'total_frames': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
            'duration': 0
        }

        # FPSãŒä¸æ­£ãªå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if info['fps'] <= 0:
            logger.warning(f"[ANALYSIS] Invalid FPS ({info['fps']}), using default 30fps")
            info['fps'] = 30.0

        # å‹•ç”»ã®é•·ã•ã‚’æ­£ç¢ºã«è¨ˆç®—
        info['duration'] = info['total_frames'] / info['fps']

        cap.release()
        logger.info(f"[ANALYSIS] Video info: {info}")
        return info

    async def _extract_frames(self, video_path: Path, target_fps: int = 5) -> List[np.ndarray]:
        """
        å‹•ç”»ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡º

        Args:
            video_path: å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            target_fps: æŠ½å‡ºã™ã‚‹FPSï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5fpsï¼‰

        Returns:
            ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆ
        """
        logger.info(f"[ANALYSIS] _extract_frames started: video_path={video_path}, target_fps={target_fps}")
        frames = []
        cap = cv2.VideoCapture(str(video_path))

        if not cap.isOpened():
            logger.error(f"[ANALYSIS] Cannot open video: {video_path}")
            raise ValueError(f"Cannot open video: {video_path}")

        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        logger.info(f"[ANALYSIS] Video: fps={fps}, total_frames={total_frames}")

        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—æ•°ã‚’è¨ˆç®—
        frame_skip = max(1, int(fps / target_fps))
        logger.info(f"[ANALYSIS] Frame skip: {frame_skip}")

        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç›´æ¥æŒ‡å®šä½ç½®ã‹ã‚‰èª­ã¿å–ã‚‹æ–¹å¼ã«å¤‰æ›´
        frame_indices = list(range(0, total_frames, frame_skip))
        logger.info(f"[ANALYSIS] Will extract {len(frame_indices)} frames from total {total_frames} frames")  # Updated

        extracted_count = 0
        failed_frames = []

        for idx, frame_idx in enumerate(frame_indices):
            # æŒ‡å®šãƒ•ãƒ¬ãƒ¼ãƒ ã«ã‚¸ãƒ£ãƒ³ãƒ—
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()

            if ret and frame is not None:
                frames.append(frame)
                extracted_count += 1

                # é€²æ—ãƒ­ã‚°ï¼ˆ10ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ï¼‰
                if extracted_count % 10 == 0 or extracted_count == len(frame_indices):
                    progress = (idx + 1) / len(frame_indices) * 100
                    logger.info(f"[ANALYSIS] Extracted {extracted_count}/{len(frame_indices)} frames ({progress:.1f}%)")
            else:
                failed_frames.append(frame_idx)
                logger.warning(f"[ANALYSIS] Failed to read frame {frame_idx}")

        if failed_frames:
            logger.warning(f"[ANALYSIS] Failed to read {len(failed_frames)} frames: {failed_frames[:10]}...")

        cap.release()
        logger.info(f"[ANALYSIS] Frame extraction completed: {extracted_count} frames extracted")
        return frames

    def _convert_instruments_format(self, instruments: List[Dict]) -> List[Dict]:
        """
        ä¿å­˜ã•ã‚ŒãŸinstrumentså½¢å¼ã‚’SAMTrackerUnifiedç”¨ã«å¤‰æ›

        Input: [{"name": str, "bbox": [x,y,w,h], "frame_number": int, "mask": str}]
        Output: [{"id": int, "name": str, "selection": {"type": "mask"|"box", "data": ...}, "color": str}]

        Args:
            instruments: ä¿å­˜ã•ã‚ŒãŸinstrumentså½¢å¼ã®ãƒªã‚¹ãƒˆ

        Returns:
            SAMTrackerUnifiedç”¨ã«å¤‰æ›ã•ã‚ŒãŸinstrumentsãƒªã‚¹ãƒˆ
        """
        converted = []
        colors = ["#FF0000", "#00FF00", "#0000FF", "#FFFF00", "#FF00FF"]

        for idx, inst in enumerate(instruments):
            # ãƒã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯å„ªå…ˆçš„ã«ä½¿ç”¨ï¼ˆæœ€ã‚‚æ­£ç¢ºï¼‰
            if "mask" in inst and inst["mask"]:
                logger.info(f"[ANALYSIS] Instrument {idx} using mask-based initialization")
                converted.append({
                    "id": idx,
                    "name": inst.get("name", f"Instrument {idx + 1}"),
                    "selection": {
                        "type": "mask",
                        "data": inst["mask"]  # base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒã‚¹ã‚¯
                    },
                    "color": inst.get("color", colors[idx % len(colors)])
                })
            # ãƒã‚¹ã‚¯ãŒãªã„å ´åˆã¯bboxã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            elif "bbox" in inst:
                x, y, w, h = inst["bbox"]
                bbox_xyxy = [x, y, x + w, y + h]
                logger.info(f"[ANALYSIS] Instrument {idx} using box-based initialization (fallback)")
                converted.append({
                    "id": idx,
                    "name": inst.get("name", f"Instrument {idx + 1}"),
                    "selection": {
                        "type": "box",
                        "data": bbox_xyxy
                    },
                    "color": inst.get("color", colors[idx % len(colors)])
                })
            elif "selection" in inst and inst["selection"].get("type") == "box":
                # æ—¢ã«å¤‰æ›æ¸ˆã¿ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
                bbox_xyxy = inst["selection"]["data"]
                logger.info(f"[ANALYSIS] Instrument {idx} using pre-converted box format")
                converted.append({
                    "id": idx,
                    "name": inst.get("name", f"Instrument {idx + 1}"),
                    "selection": {
                        "type": "box",
                        "data": bbox_xyxy
                    },
                    "color": inst.get("color", colors[idx % len(colors)])
                })
            else:
                logger.warning(f"[ANALYSIS] Instrument {idx} has no valid bbox or mask, skipping")
                continue

        logger.info(f"[ANALYSIS] Converted {len(converted)} instruments from saved format to SAM format")
        return converted

    async def _run_detection(
        self,
        frames: List[np.ndarray],
        video_type: VideoType,
        video_id: str,
        instruments: Optional[List[Dict]],
        video_path: Path
    ) -> Dict[str, Any]:
        """
        å‹•ç”»ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãæ¤œå‡ºå‡¦ç†ã®å®Ÿè¡Œ

        Args:
            frames: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆ
            video_type: å‹•ç”»ã‚¿ã‚¤ãƒ—
            video_id: å‹•ç”»ID
            instruments: å™¨å…·å®šç¾©
            video_path: å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ï¼‰

        Returns:
            æ¤œå‡ºçµæœ
        """
        logger.info(f"[ANALYSIS] _run_detection started: video_type={video_type}, frames={len(frames)}")
        results = {
            'skeleton_data': [],
            'instrument_data': []
        }

        # å‹•ç”»ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãæ¤œå‡ºå™¨ã®é¸æŠ
        if video_type in [VideoType.EXTERNAL, VideoType.EXTERNAL_NO_INSTRUMENTS]:
            # éª¨æ ¼æ¤œå‡ºã®ã¿
            logger.info(f"[ANALYSIS] Running MediaPipe detection only (no instruments) for video_type: {video_type}")
            detector = HandSkeletonDetector(min_detection_confidence=0.1)
            self.detectors['mediapipe'] = detector

            logger.info(f"[ANALYSIS] Starting MediaPipe batch detection on {len(frames)} frames")
            skeleton_results = detector.detect_batch(frames)
            logger.info(f"[ANALYSIS] MediaPipe detection completed, got {len(skeleton_results)} results")

            # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®çµæœã‚’ç¢ºèª
            if skeleton_results and len(skeleton_results) > 0:
                first_result = skeleton_results[0]
                # å‹ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹
                if isinstance(first_result, dict):
                    logger.info(f"[ANALYSIS] First skeleton result: detected={first_result.get('detected')}, hands={len(first_result.get('hands', []))}")
                else:
                    logger.warning(f"[ANALYSIS] First skeleton result is not dict: type={type(first_result)}")

            results['skeleton_data'] = self._format_skeleton_data(skeleton_results)
            logger.info(f"[ANALYSIS] Formatted {len(results['skeleton_data'])} skeleton data points")

        elif video_type == VideoType.EXTERNAL_WITH_INSTRUMENTS:
            # éª¨æ ¼æ¤œå‡ºã¨å™¨å…·æ¤œå‡ºã®ä¸¡æ–¹
            logger.info("[ANALYSIS] Running both MediaPipe and SAM detection")

            # MediaPipeæ¤œå‡º
            mediapipe_detector = HandSkeletonDetector(min_detection_confidence=0.1)
            self.detectors['mediapipe'] = mediapipe_detector
            skeleton_results = mediapipe_detector.detect_batch(frames)

            # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®çµæœã‚’ç¢ºèª
            if skeleton_results and len(skeleton_results) > 0:
                first_result = skeleton_results[0]
                if isinstance(first_result, dict):
                    logger.info(f"[ANALYSIS] First skeleton result: detected={first_result.get('detected')}, hands={len(first_result.get('hands', []))}")
                else:
                    logger.warning(f"[ANALYSIS] First skeleton result is not dict: type={type(first_result)}")

            results['skeleton_data'] = self._format_skeleton_data(skeleton_results)

            # SAMæ¤œå‡ºï¼ˆä¸€æœ¬åŒ–å®Ÿè£…ï¼‰
            device = getattr(settings, 'SAM_DEVICE', 'cpu')
            if device == 'auto':
                import torch
                device = 'cuda' if torch.cuda.is_available() else 'cpu'

            fps = self.video_info.get('fps', 30.0)
            target_fps = 5.0  # ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºæ™‚ã®FPS

            # å®Ÿé¨“ç‰ˆ: SAM2 Video APIã‚’ä½¿ç”¨ã™ã‚‹ã‹ç¢ºèª
            use_video_api = getattr(settings, 'USE_SAM2_VIDEO_API', False)

            if self.use_sam2 and use_video_api:
                # ğŸ§ª å®Ÿé¨“ç‰ˆ: SAM2 Video API
                logger.info(f"[EXPERIMENTAL] SAM2 Video API: {settings.SAM2_VIDEO_MODEL_TYPE}, device={device}")

                # Configã‹ã‚‰è‡ªå‹•çš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
                sam_detector = SAM2TrackerVideo(device=device)
                logger.info("[EXPERIMENTAL] SAM2 Video API: Memory Bank + Temporal Context enabled")

                # Video APIã¯å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¸€åº¦ã«å‡¦ç†
                if instruments and len(instruments) > 0 and len(frames) > 0:
                    # ä¿å­˜å½¢å¼ã‹ã‚‰SAMå½¢å¼ã«å¤‰æ›
                    instruments_converted = self._convert_instruments_format(instruments)
                    logger.info(f"[EXPERIMENTAL] Tracking {len(instruments_converted)} instruments across {len(frames)} frames...")

                    # Video APIã§è¿½è·¡ï¼ˆvideo_pathã¯æ—¢ã«çµ¶å¯¾ãƒ‘ã‚¹ï¼‰
                    tracking_result = await sam_detector.track_video(
                        str(video_path),
                        instruments_converted
                    )

                    # çµæœã‚’ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã®å½¢å¼ã«å¤‰æ›
                    instrument_results = self._convert_video_api_result(tracking_result, len(frames))
                    logger.info(f"[EXPERIMENTAL] SAM2 Video API completed: {len(instrument_results)} frames processed")
                else:
                    logger.warning("[EXPERIMENTAL] No instruments provided for Video API tracking")
                    instrument_results = []

            elif self.use_sam2:
                # æ—¢å­˜ã®SAM2ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½å‡¦ç†ï¼‰
                logger.info(f"[ANALYSIS] Creating SAM2Tracker with model=small, device={device}")
                sam_detector = SAM2Tracker(model_type="small", device=device)
                logger.info("[ANALYSIS] SAM2 enabled for higher accuracy (+2% Dice, -21% HD95)")

                self.detectors['sam'] = sam_detector

                # å™¨å…·ã®åˆæœŸåŒ–
                if instruments and len(instruments) > 0 and len(frames) > 0:
                    try:
                        # ä¿å­˜å½¢å¼ã‹ã‚‰SAMå½¢å¼ã«å¤‰æ›
                        instruments_converted = self._convert_instruments_format(instruments)
                        logger.info(f"[ANALYSIS] Initializing {len(instruments_converted)} instruments from user selection")
                        sam_detector.initialize_instruments(frames[0], instruments_converted)
                    except Exception as e:
                        logger.error(f"[ANALYSIS] Failed to initialize instruments from user selection: {e}")
                        logger.info("[ANALYSIS] Falling back to automatic instrument detection")
                        sam_detector.auto_detect_instruments(frames[0], max_instruments=5)
                elif len(frames) > 0:
                    logger.info("[ANALYSIS] No user selection, using automatic instrument detection")
                    sam_detector.auto_detect_instruments(frames[0], max_instruments=5)
                else:
                    logger.warning("[ANALYSIS] No frames available for instrument initialization")

                logger.info(f"[ANALYSIS] Running SAM detect_batch on {len(frames)} frames...")
                instrument_results = sam_detector.detect_batch(frames)
                logger.info(f"[ANALYSIS] SAM detection completed, got {len(instrument_results)} results")
            else:
                # SAM1ï¼ˆæ—¢å­˜å®Ÿè£…ï¼‰
                logger.info(f"[ANALYSIS] Creating SAMTrackerUnified with model=vit_h, device={device}, instruments={len(instruments) if instruments else 0}, fps={fps}, target_fps={target_fps}")
                sam_detector = SAMTrackerUnified(model_type="vit_h", device=device)

                self.detectors['sam'] = sam_detector

                # å™¨å…·ã®åˆæœŸåŒ–
                if instruments and len(instruments) > 0 and len(frames) > 0:
                    try:
                        instruments_converted = self._convert_instruments_format(instruments)
                        logger.info(f"[ANALYSIS] Initializing {len(instruments_converted)} instruments from user selection")
                        sam_detector.initialize_instruments(frames[0], instruments_converted)
                    except Exception as e:
                        logger.error(f"[ANALYSIS] Failed to initialize instruments from user selection: {e}")
                        logger.info("[ANALYSIS] Falling back to automatic instrument detection")
                        sam_detector.auto_detect_instruments(frames[0], max_instruments=5)
                elif len(frames) > 0:
                    logger.info("[ANALYSIS] No user selection, using automatic instrument detection")
                    sam_detector.auto_detect_instruments(frames[0], max_instruments=5)
                else:
                    logger.warning("[ANALYSIS] No frames available for instrument initialization")

                logger.info(f"[ANALYSIS] Running SAM detect_batch on {len(frames)} frames...")
                instrument_results = sam_detector.detect_batch(frames)
                logger.info(f"[ANALYSIS] SAM detection completed, got {len(instrument_results)} results")

            # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®çµæœã‚’ç¢ºèª
            if instrument_results and len(instrument_results) > 0:
                first_result = instrument_results[0]
                # å‹ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹
                if isinstance(first_result, dict):
                    logger.info(f"[ANALYSIS] First instrument result: detected={first_result.get('detected')}, instruments={len(first_result.get('instruments', []))}")
                else:
                    logger.warning(f"[ANALYSIS] First instrument result is not dict: type={type(first_result)}")

            results['instrument_data'] = self._format_instrument_data(instrument_results)
            logger.info(f"[ANALYSIS] Formatted instrument data: {len(results['instrument_data'])} frames with detections")

            # Phase 2.2: ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆã‚’åé›†
            self._collect_tracking_stats(sam_detector, instrument_results)

        elif video_type == VideoType.INTERNAL:
            # å†…è¦–é¡ï¼šå™¨å…·æ¤œå‡ºã®ã¿
            logger.info("[ANALYSIS] Running SAM detection only (internal camera)")
            device = getattr(settings, 'SAM_DEVICE', 'cpu')
            if device == 'auto':
                import torch
                device = 'cuda' if torch.cuda.is_available() else 'cpu'

            fps = self.video_info.get('fps', 30.0)
            target_fps = 5.0  # ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºæ™‚ã®FPS

            # SAM2ã¾ãŸã¯SAM1ã‚’é¸æŠ
            if self.use_sam2:
                logger.info(f"[ANALYSIS] Creating SAM2Tracker with model=small, device={device}")
                detector = SAM2Tracker(model_type="small", device=device)
                logger.info("[ANALYSIS] SAM2 enabled for higher accuracy (+2% Dice, -21% HD95)")
            else:
                logger.info(f"[ANALYSIS] Creating SAMTrackerUnified with model=vit_h, device={device}")
                # GPUå¯¾å¿œ: vit_hãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆRTX 3060ã§é«˜é€Ÿãƒ»é«˜ç²¾åº¦ï¼‰
                detector = SAMTrackerUnified(model_type="vit_h", device=device)

            self.detectors['sam'] = detector

            # å™¨å…·ã®åˆæœŸåŒ–
            if instruments and len(instruments) > 0 and len(frames) > 0:
                try:
                    # ä¿å­˜å½¢å¼ã‹ã‚‰SAMå½¢å¼ã«å¤‰æ›
                    instruments_converted = self._convert_instruments_format(instruments)
                    logger.info(f"[ANALYSIS] Initializing {len(instruments_converted)} instruments from user selection")
                    detector.initialize_instruments(frames[0], instruments_converted)
                except Exception as e:
                    logger.error(f"[ANALYSIS] Failed to initialize instruments from user selection: {e}")
                    logger.info("[ANALYSIS] Falling back to automatic instrument detection")
                    detector.auto_detect_instruments(frames[0], max_instruments=5)
            elif len(frames) > 0:
                logger.info("[ANALYSIS] No user selection, using automatic instrument detection for INTERNAL video")
                detector.auto_detect_instruments(frames[0], max_instruments=5)
            else:
                logger.warning("[ANALYSIS] No frames available for instrument initialization")

            instrument_results = detector.detect_batch(frames)
            results['instrument_data'] = self._format_instrument_data(instrument_results)

            # Phase 2.2: ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆã‚’åé›†
            self._collect_tracking_stats(detector, instrument_results)

        else:
            logger.warning(f"Unknown video type: {video_type}, defaulting to MediaPipe only")
            detector = HandSkeletonDetector(min_detection_confidence=0.1)
            self.detectors['mediapipe'] = detector

            skeleton_results = detector.detect_batch(frames)

            # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®çµæœã‚’ç¢ºèª
            if skeleton_results and len(skeleton_results) > 0:
                first_result = skeleton_results[0]
                if isinstance(first_result, dict):
                    logger.info(f"[ANALYSIS] First skeleton result: detected={first_result.get('detected')}, hands={len(first_result.get('hands', []))}")
                else:
                    logger.warning(f"[ANALYSIS] First skeleton result is not dict: type={type(first_result)}")

            results['skeleton_data'] = self._format_skeleton_data(skeleton_results)

        return results

    def _format_skeleton_data(self, raw_results: List[Dict]) -> List[Dict]:
        """éª¨æ ¼æ¤œå‡ºçµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰äº’æ›å½¢å¼ï¼‰"""
        from collections import defaultdict

        # ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        frames_dict = defaultdict(list)
        fps = self.video_info.get('fps', 30)
        target_fps = 5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æŠ½å‡ºãƒ¬ãƒ¼ãƒˆ
        frame_skip = max(1, int(fps / target_fps))

        for result in raw_results:
            # å‹ãƒã‚§ãƒƒã‚¯
            if not isinstance(result, dict):
                logger.warning(f"Skipping non-dict result: type={type(result)}")
                continue
            if result.get('detected'):
                # Fail Fast: frame_indexãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
                if 'frame_index' not in result:
                    error_msg = f"Missing frame_index in skeleton detection result. Result keys: {list(result.keys())}"
                    logger.error(error_msg)
                    raise ValueError(f"skeleton_detector.detect_batch() must include frame_index in results. {error_msg}")

                frame_idx = result['frame_index']
                # å®Ÿéš›ã®ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã‚’è¨ˆç®—
                actual_frame_number = frame_idx * frame_skip
                timestamp = actual_frame_number / fps if fps > 0 else frame_idx / 30.0

                for hand in result.get('hands', []):
                    hand_data = {
                        'hand_type': hand.get('hand_type', hand.get('label', 'Unknown')),
                        'landmarks': hand.get('landmarks', {}),
                        'palm_center': hand.get('palm_center', {}),
                        'finger_angles': hand.get('finger_angles', {}),
                        'hand_openness': hand.get('hand_openness', 0.0)
                    }
                    frames_dict[actual_frame_number].append(hand_data)

        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰å½¢å¼ã«å¤‰æ›: 1ãƒ•ãƒ¬ãƒ¼ãƒ  = 1ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°ã®æ‰‹ã‚’å«ã‚€ï¼‰
        formatted = []
        for frame_number in sorted(frames_dict.keys()):
            timestamp = frame_number / fps if fps > 0 else frame_number / 30.0
            formatted.append({
                'frame': frame_number,
                'frame_number': frame_number,
                'timestamp': timestamp,
                'hands': frames_dict[frame_number]
            })

        logger.info(f"Formatted {len(formatted)} skeleton frames with hands data")
        return formatted

    def _format_instrument_data(self, raw_results: List[Dict]) -> List[Dict]:
        """å™¨å…·æ¤œå‡ºçµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        formatted = []
        fps = self.video_info.get('fps', 30)  # å®Ÿéš›ã®å‹•ç”»FPS
        target_fps = 5  # ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºæ™‚ã®FPS

        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—ã‚’è¨ˆç®—ï¼ˆskeleton_dataã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        frame_skip = max(1, int(fps / target_fps))

        # ãƒ‡ãƒãƒƒã‚°ï¼šFPSæƒ…å ±ã‚’ç¢ºèª
        logger.info(f"[ANALYSIS] _format_instrument_data: fps={fps}, target_fps={target_fps}, frame_skip={frame_skip}")
        logger.info(f"[ANALYSIS] self.video_info: {self.video_info}")

        for frame_idx, result in enumerate(raw_results):
            # å‹ãƒã‚§ãƒƒã‚¯
            if not isinstance(result, dict):
                logger.warning(f"[ANALYSIS] Skipping non-dict instrument result: type={type(result)}")
                continue

            # å®Ÿéš›ã®ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã‚’è¨ˆç®—ï¼ˆskeleton_dataã¨åŒæ§˜ï¼‰
            actual_frame_number = frame_idx * frame_skip

            # æ­£ã—ã„ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨ˆç®—ï¼ˆå®Ÿéš›ã®FPSã§å‰²ã‚‹ï¼‰
            timestamp = actual_frame_number / fps if fps > 0 else frame_idx / 30.0

            # SAM2 Video APIã¯'instruments'ã‚­ãƒ¼ã€SAMTrackerUnifiedã¯'detections'ã‚­ãƒ¼ã‚’ä½¿ã†
            instruments = result.get('instruments', result.get('detections', []))

            # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã¨æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç¢ºèª
            if frame_idx == 0 or frame_idx >= 110:
                logger.info(f"[ANALYSIS] Instrument frame {frame_idx}: actual_frame={actual_frame_number}, timestamp={timestamp:.2f}s, instruments_count={len(instruments)}, detected={result.get('detected', False)}")
                if frame_idx == 0 and len(instruments) > 0:
                    logger.info(f"[ANALYSIS] First instrument result: detected={result.get('detected')}, instruments={len(instruments)}, first_inst={instruments[0]}")

            formatted.append({
                'frame_number': actual_frame_number,  # å®Ÿéš›ã®ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·
                'timestamp': timestamp,  # æ­£ã—ã„ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
                'detections': instruments  # SAM2ã§ã¯'instruments'ã€SAMTrackerUnifiedã§ã¯'detections'
            })

        logger.info(f"Formatted {len(formatted)} instrument detections with correct timestamps")
        return formatted

    async def _calculate_metrics(self, detection_results: Dict) -> Dict:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        metrics = {}

        # éª¨æ ¼ãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if detection_results.get('skeleton_data'):
            calculator = MetricsCalculator(fps=self.video_info.get('fps', 30))
            metrics['skeleton_metrics'] = calculator.calculate_all_metrics(
                detection_results['skeleton_data']
            )

        # å™¨å…·ãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆå°†æ¥çš„ã«å®Ÿè£…ï¼‰
        if detection_results.get('instrument_data'):
            metrics['instrument_metrics'] = {
                'total_detections': len(detection_results['instrument_data'])
            }

        logger.info(f"Calculated metrics: {list(metrics.keys())}")
        return metrics

    async def _calculate_scores(self, metrics: Dict) -> Dict:
        """ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        scores = {
            'overall_score': 0,
            'efficiency_score': 0,
            'smoothness_score': 0,
            'accuracy_score': 0
        }

        if 'skeleton_metrics' in metrics:
            skeleton_metrics = metrics['skeleton_metrics']

            # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå°†æ¥çš„ã«æ”¹å–„ï¼‰
            if 'velocity' in skeleton_metrics:
                avg_velocity = skeleton_metrics['velocity'].get('average', 0)
                scores['efficiency_score'] = min(100, avg_velocity * 10)

            if 'jerk' in skeleton_metrics:
                avg_jerk = skeleton_metrics['jerk'].get('average', 0)
                scores['smoothness_score'] = max(0, 100 - avg_jerk * 5)

            # ç·åˆã‚¹ã‚³ã‚¢
            scores['overall_score'] = (
                scores['efficiency_score'] * 0.4 +
                scores['smoothness_score'] * 0.6
            )

        logger.info(f"Calculated scores: {scores}")
        return scores

    async def _save_results(
        self,
        analysis_result: AnalysisResult,
        detection_results: Dict,
        metrics: Dict,
        scores: Dict,
        db
    ):
        """çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆnumpyå‹å¤‰æ›ã¨ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ä»˜ãï¼‰"""
        skeleton_data = detection_results.get('skeleton_data', [])
        instrument_data = detection_results.get('instrument_data', [])

        logger.info(f"[ANALYSIS] _save_results: skeleton_data length = {len(skeleton_data)}")
        logger.info(f"[ANALYSIS] _save_results: instrument_data length = {len(instrument_data)}")

        # numpyå‹ã‚’Pythonæ¨™æº–å‹ã«å¤‰æ›
        logger.info(f"[ANALYSIS] Converting numpy types...")
        skeleton_data = convert_numpy_types(skeleton_data)
        instrument_data = convert_numpy_types(instrument_data)
        metrics = convert_numpy_types(metrics)
        scores = convert_numpy_types(scores)

        # instrument_dataã®ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ã«å¿œã˜ã¦åœ§ç¸®
        instrument_data_str = json.dumps(instrument_data)
        data_size = len(instrument_data_str)
        logger.info(f"[ANALYSIS] Instrument data size: {data_size} characters")

        if data_size > 500000:  # 500KBè¶…é
            logger.warning(f"[ANALYSIS] Instrument data too large ({data_size} chars), compressing...")
            instrument_data = self._compress_instrument_data(instrument_data)
            compressed_size = len(json.dumps(instrument_data))
            logger.info(f"[ANALYSIS] Compressed to {compressed_size} characters ({100 * compressed_size / data_size:.1f}%)")

        analysis_result.skeleton_data = skeleton_data
        analysis_result.instrument_data = instrument_data
        analysis_result.motion_analysis = metrics
        analysis_result.scores = scores
        analysis_result.total_frames = self.video_info.get('total_frames', 0)
        analysis_result.status = AnalysisStatus.COMPLETED
        analysis_result.completed_at = datetime.now()
        analysis_result.progress = 100

        # Phase 2.2: ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆã¨è­¦å‘Šã‚’ä¿å­˜
        if self.tracking_stats:
            analysis_result.tracking_stats = json.dumps(self.tracking_stats)
            logger.info(f"[ANALYSIS] Saved tracking_stats: {list(self.tracking_stats.keys())}")

        if self.warnings:
            analysis_result.warnings = json.dumps(self.warnings)
            logger.info(f"[ANALYSIS] Saved {len(self.warnings)} warnings")

        db.commit()
        logger.info(f"[ANALYSIS] Results saved for analysis_id: {analysis_result.id}")

    def _collect_tracking_stats(self, detector, instrument_results: List[Dict]):
        """
        ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆã‚’åé›†ã™ã‚‹ï¼ˆPhase 2.2ï¼‰

        Args:
            detector: SAMTrackerUnifiedã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            instrument_results: å™¨å…·æ¤œå‡ºçµæœ
        """
        try:
            # SAMTrackerUnifiedã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
            if hasattr(detector, 'get_tracking_stats'):
                tracker_stats = detector.get_tracking_stats()

                # å™¨å…·ã”ã¨ã®çµ±è¨ˆ
                for inst_key, inst_stats in tracker_stats.get('instruments', {}).items():
                    if inst_key not in self.tracking_stats:
                        self.tracking_stats[inst_key] = {}

                    self.tracking_stats[inst_key]['max_lost_count'] = inst_stats.get('lost_frames', 0)
                    self.tracking_stats[inst_key]['last_score'] = inst_stats.get('last_score', 0.0)
                    self.tracking_stats[inst_key]['trajectory_length'] = inst_stats.get('trajectory_length', 0)

            # å†æ¤œå‡ºã‚¤ãƒ™ãƒ³ãƒˆã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            re_detection_count = {}
            for frame_data in instrument_results:
                if isinstance(frame_data, dict):
                    detections = frame_data.get('detections', [])
                    for detection in detections:
                        if detection.get('redetected'):
                            track_id = detection.get('track_id', 0)
                            inst_key = f"instrument_{track_id}"

                            if inst_key not in re_detection_count:
                                re_detection_count[inst_key] = 0
                            re_detection_count[inst_key] += 1

            # å†æ¤œå‡ºã‚«ã‚¦ãƒ³ãƒˆã‚’tracking_statsã«è¿½åŠ 
            for inst_key, count in re_detection_count.items():
                if inst_key not in self.tracking_stats:
                    self.tracking_stats[inst_key] = {}
                self.tracking_stats[inst_key]['re_detections'] = count

            # ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã¨æ¤œå‡ºãƒ•ãƒ¬ãƒ¼ãƒ æ•°
            total_frames = len(instrument_results)
            detected_frames = sum(
                1 for frame_data in instrument_results
                if isinstance(frame_data, dict) and len(frame_data.get('detections', [])) > 0
            )

            self.tracking_stats['summary'] = {
                'total_frames': total_frames,
                'detected_frames': detected_frames,
                'detection_rate': detected_frames / total_frames if total_frames > 0 else 0
            }

            logger.info(f"[ANALYSIS] Collected tracking stats: {list(self.tracking_stats.keys())}")

        except Exception as e:
            logger.warning(f"[ANALYSIS] Failed to collect tracking stats: {e}")

    def _compress_instrument_data(self, instrument_data: List[Dict]) -> List[Dict]:
        """
        å¤§å®¹é‡ã®å™¨å…·è¿½è·¡ãƒ‡ãƒ¼ã‚¿ã‚’åœ§ç¸®

        æ³¨æ„ï¼šã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯_format_instrument_dataã®å‡ºåŠ›ã‚’å—ã‘å–ã‚‹
        _format_instrument_dataã¯ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã™ã‚‹ï¼š
        {
            'frame_number': int,
            'timestamp': float,
            'detections': [  # â† 'instruments'ã§ã¯ãªã'detections'
                {
                    'id': int,
                    'name': str,
                    'center': [x, y],
                    'bbox': [x1, y1, x2, y2],
                    'confidence': float,
                    'mask': array (optional)
                }
            ]
        }

        Args:
            instrument_data: å™¨å…·è¿½è·¡ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ

        Returns:
            åœ§ç¸®ã•ã‚ŒãŸå™¨å…·è¿½è·¡ãƒ‡ãƒ¼ã‚¿
        """
        if not instrument_data:
            return []

        total_frames = len(instrument_data)
        logger.info(f"[ANALYSIS] Compressing {total_frames} frames of instrument data")

        # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ§‹é€ ã‚’ç¢ºèª
        if total_frames > 0:
            first_frame = instrument_data[0]
            logger.info(f"[ANALYSIS] Compression input - First frame keys: {list(first_frame.keys())}")
            detections_key = 'detections' if 'detections' in first_frame else 'instruments'
            det_count = len(first_frame.get(detections_key, []))
            logger.info(f"[ANALYSIS] Compression input - First frame {detections_key} count: {det_count}")
            if det_count > 0:
                first_det = first_frame[detections_key][0]
                logger.info(f"[ANALYSIS] Compression input - First detection keys: {list(first_det.keys())}")

        # maskãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»ã—ã¦åœ§ç¸®
        compressed_data = []

        # ãƒ‡ãƒãƒƒã‚°: åœ§ç¸®é–‹å§‹æ™‚ã®ãƒ­ã‚°
        logger.warning(f"[CONTOUR_DEBUG] Starting compression, total frames: {len(instrument_data)}")

        for frame_idx, frame_data in enumerate(instrument_data):
            # _format_instrument_dataãŒä½¿ç”¨ã™ã‚‹ã‚­ãƒ¼åã«å¯¾å¿œ
            compressed_frame = {
                'frame_number': frame_data.get('frame_number'),  # frame_index ã§ã¯ãªã frame_number
                'timestamp': frame_data.get('timestamp', 0.0),
                'detections': []  # instruments ã§ã¯ãªã detections
            }

            # SAM2 Video APIã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œ
            detections = frame_data.get('detections', [])

            # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã§å¿…ãšãƒ­ã‚°å‡ºåŠ›
            if frame_idx == 0:
                logger.warning(f"[CONTOUR_DEBUG] Frame 0 has {len(detections)} detections")

            for det_idx, det in enumerate(detections):
                # ãƒ‡ãƒãƒƒã‚°: detectionã®æ§‹é€ ã‚’ç¢ºèªï¼ˆWARNINGãƒ¬ãƒ™ãƒ«ã§ç¢ºå®Ÿã«å‡ºåŠ›ï¼‰
                if frame_idx == 0 and det_idx == 0:
                    logger.warning(f"[CONTOUR_DEBUG] First detection keys: {list(det.keys())}")
                    logger.warning(f"[CONTOUR_DEBUG] First detection has 'mask': {'mask' in det}")
                    if 'mask' in det:
                        mask_data = det.get('mask')
                        logger.warning(f"[CONTOUR_DEBUG] Mask type: {type(mask_data)}, is None: {mask_data is None}")
                        if isinstance(mask_data, np.ndarray):
                            logger.warning(f"[CONTOUR_DEBUG] Mask shape: {mask_data.shape}, dtype: {mask_data.dtype}, sum: {mask_data.sum()}")

                compressed_det = {
                    'id': det.get('id'),
                    'name': det.get('name', ''),
                    'center': det.get('center', []),
                    'bbox': det.get('bbox', []),
                    'confidence': det.get('confidence', 0.0),
                    'contour': self._extract_mask_contour(det.get('mask'))  # ãƒã‚¹ã‚¯è¼ªéƒ­ã‚’æŠ½å‡º
                    # maské…åˆ—ã¯é™¤å¤–ï¼ˆ588MBâ†’æ•°ç™¾KBï¼‰ã€ä»£ã‚ã‚Šã«contouråº§æ¨™ã‚’ä¿å­˜
                }

                compressed_frame['detections'].append(compressed_det)

            compressed_data.append(compressed_frame)

        # åœ§ç¸®çµæœã‚’ç¢ºèª
        frames_with_dets = sum(1 for f in compressed_data if len(f.get('detections', [])) > 0)
        logger.info(f"[ANALYSIS] After mask removal: {frames_with_dets}/{total_frames} frames have detections")

        # 500KBè¶…éã®å ´åˆã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§å‰Šæ¸›
        compressed_json = json.dumps(compressed_data)
        compressed_size = len(compressed_json)
        logger.info(f"[ANALYSIS] Compressed data size: {compressed_size} characters")

        if compressed_size > 500000:
            logger.warning(f"[ANALYSIS] Still too large ({compressed_size} chars), sampling frames...")
            summary_data = []

            # æœ€åˆã®10ãƒ•ãƒ¬ãƒ¼ãƒ 
            summary_data.extend(compressed_data[:10])

            # 10ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«
            for i in range(10, total_frames - 10, 10):
                summary_data.append(compressed_data[i])

            # æœ€å¾Œã®10ãƒ•ãƒ¬ãƒ¼ãƒ 
            if total_frames > 20:
                summary_data.extend(compressed_data[-10:])

            sampled_frames_with_dets = sum(1 for f in summary_data if len(f.get('detections', [])) > 0)
            logger.info(f"[ANALYSIS] Sampled {len(summary_data)} frames from {total_frames} total")
            logger.info(f"[ANALYSIS] Sampled data: {sampled_frames_with_dets}/{len(summary_data)} frames have detections")
            return summary_data

        return compressed_data

    async def _update_status(
        self,
        analysis_result: AnalysisResult,
        status: str,
        db,
        progress: int = None
    ):
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ã¨WebSocketé€šçŸ¥ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        analysis_result.current_step = status
        if progress is not None:
            analysis_result.progress = progress

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’DBã®statusãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã‚‚åæ˜ 
        if status == "completed":
            analysis_result.status = AnalysisStatus.COMPLETED
        elif status in ["initialization", "frame_extraction", "skeleton_detection",
                       "instrument_detection", "motion_analysis", "report_generation"]:
            analysis_result.status = AnalysisStatus.PROCESSING

        db.commit()

        # WebSocketé€šçŸ¥ï¼ˆè©³ç´°æƒ…å ±ä»˜ãï¼‰
        await manager.send_progress(
            analysis_result.id,
            {
                'type': 'status_update',
                'status': status,
                'current_step': status,
                'progress': progress or analysis_result.progress,
                'message': self._get_step_message(status, progress)
            }
        )

        logger.info(f"Updated status: {status}, progress: {progress}")

    def _get_step_message(self, step: str, progress: int = None) -> str:
        """å„ã‚¹ãƒ†ãƒƒãƒ—ã®èª¬æ˜ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™"""
        messages = {
            "initialization": "å‹•ç”»æƒ…å ±ã‚’å–å¾—ã—ã¦ã„ã¾ã™...",
            "frame_extraction": "ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™...",
            "skeleton_detection": "éª¨æ ¼ã‚’æ¤œå‡ºã—ã¦ã„ã¾ã™...",
            "instrument_detection": "å™¨å…·ã‚’èªè­˜ã—ã¦ã„ã¾ã™...",
            "motion_analysis": "ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è§£æã—ã¦ã„ã¾ã™...",
            "report_generation": "ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...",
            "completed": "è§£æãŒå®Œäº†ã—ã¾ã—ãŸï¼"
        }
        return messages.get(step, f"å‡¦ç†ä¸­... ({progress}%)" if progress else "å‡¦ç†ä¸­...")

    def _convert_video_api_result(
        self,
        tracking_result: Dict[str, Any],
        total_frames: int
    ) -> List[Dict[str, Any]]:
        """
        SAM2 Video APIã®çµæœã‚’æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ï¼‰ã«å¤‰æ›

        Args:
            tracking_result: Video APIã®çµæœ
                {
                    "instruments": [
                        {
                            "instrument_id": 0,
                            "name": "forceps",
                            "trajectory": [
                                {"frame_index": 0, "center": [x, y], "bbox": [...], ...},
                                ...
                            ]
                        }
                    ]
                }
            total_frames: ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°

        Returns:
            ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã®çµæœãƒªã‚¹ãƒˆ
                [
                    {
                        "detected": True,
                        "instruments": [
                            {"id": 0, "center": [x, y], "bbox": [...], ...},
                            ...
                        ]
                    },
                    ...
                ]
        """
        logger.info("[EXPERIMENTAL] Converting Video API result to frame-based format...")

        # ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã®çµæœã‚’åˆæœŸåŒ–
        frame_results = [
            {"detected": False, "instruments": []}
            for _ in range(total_frames)
        ]

        # å„å™¨å…·ã®è»Œè·¡ã‚’ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã«åˆ†é…
        instruments_data = tracking_result.get("instruments", [])

        for inst_data in instruments_data:
            inst_id = inst_data["instrument_id"]
            inst_name = inst_data["name"]
            trajectory = inst_data["trajectory"]

            for point in trajectory:
                frame_idx = point["frame_index"]

                if 0 <= frame_idx < total_frames:
                    frame_results[frame_idx]["detected"] = True
                    frame_results[frame_idx]["instruments"].append({
                        "id": inst_id,
                        "name": inst_name,
                        "center": point["center"],
                        "bbox": point["bbox"],
                        "confidence": point["confidence"],
                        "mask": point.get("mask")
                    })

        # æ¤œå‡ºãŒã‚ã£ãŸãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        detected_frames = sum(1 for fr in frame_results if fr["detected"])
        logger.info(f"[EXPERIMENTAL] Converted to frame-based format: {detected_frames}/{total_frames} frames with detections")

        return frame_results

    def _extract_mask_contour(self, mask: np.ndarray) -> List[List[int]]:
        """
        ãƒã‚¹ã‚¯ã‹ã‚‰è¼ªéƒ­åº§æ¨™ã‚’æŠ½å‡ºï¼ˆè»½é‡åŒ–ï¼‰

        Args:
            mask: numpyé…åˆ—ã®ãƒã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ (H, W)

        Returns:
            è¼ªéƒ­åº§æ¨™ã®ãƒªã‚¹ãƒˆ [[x, y], [x, y], ...]
        """
        # ãƒ‡ãƒãƒƒã‚°: maskã®å‹ã¨å†…å®¹ã‚’ç¢ºèªï¼ˆWARNINGãƒ¬ãƒ™ãƒ«ã§ç¢ºå®Ÿã«å‡ºåŠ›ï¼‰
        logger.warning(f"[CONTOUR_DEBUG] _extract_mask_contour called, mask type: {type(mask)}, is None: {mask is None}")

        if mask is None:
            logger.warning("[CONTOUR_DEBUG] Mask is None, returning empty contour")
            return []

        if not isinstance(mask, np.ndarray):
            logger.warning(f"[CONTOUR_DEBUG] Mask is not numpy array (type: {type(mask)}), returning empty contour")
            return []

        try:
            # ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ã«å¤‰æ›
            if mask.dtype == np.float32 or mask.dtype == np.float64:
                binary_mask = (mask > 0.5).astype(np.uint8)
            else:
                binary_mask = mask.astype(np.uint8)

            # ãƒã‚¹ã‚¯ãŒç©ºã®å ´åˆ
            if binary_mask.sum() == 0:
                return []

            # è¼ªéƒ­æŠ½å‡º
            contours, _ = cv2.findContours(
                binary_mask,
                cv2.RETR_EXTERNAL,
                cv2.CHAIN_APPROX_SIMPLE
            )

            if len(contours) == 0:
                return []

            # æœ€å¤§ã®è¼ªéƒ­ã‚’ä½¿ç”¨
            largest_contour = max(contours, key=cv2.contourArea)

            # åº§æ¨™æ•°ã‚’å‰Šæ¸›ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºè»½é‡åŒ–: 0.3%ã®ç²¾åº¦ã§è¿‘ä¼¼ï¼‰
            epsilon = 0.003 * cv2.arcLength(largest_contour, True)
            approx = cv2.approxPolyDP(largest_contour, epsilon, True)

            # [[x, y], [x, y], ...] å½¢å¼ã«å¤‰æ›
            contour_points = approx.reshape(-1, 2).tolist()

            logger.debug(f"[CONTOUR] Extracted {len(contour_points)} points from mask shape={mask.shape}")

            return contour_points

        except Exception as e:
            logger.warning(f"[CONTOUR] Failed to extract contour: {e}")
            return []