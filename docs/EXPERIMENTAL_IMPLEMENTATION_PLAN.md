# å®Ÿé¨“ç‰ˆå®Ÿè£…è¨ˆç”»æ›¸ï¼šSAM2 Video APIçµ±åˆã«ã‚ˆã‚‹å™¨å…·ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ç²¾åº¦å‘ä¸Š

**ä½œæˆæ—¥**: 2025-10-11
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: v1.0
**ç›®çš„**: å™¨å…·ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®å®Ÿé¨“çš„å®Ÿè£…

---

## ğŸ“‹ ç›®æ¬¡

1. [å®Ÿè£…æ¦‚è¦](#1-å®Ÿè£…æ¦‚è¦)
2. [ç¾çŠ¶åˆ†æã¨èª²é¡Œ](#2-ç¾çŠ¶åˆ†æã¨èª²é¡Œ)
3. [æ”¹å–„æˆ¦ç•¥](#3-æ”¹å–„æˆ¦ç•¥)
4. [æŠ€è¡“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#4-æŠ€è¡“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
5. [å®Ÿè£…è©³ç´°](#5-å®Ÿè£…è©³ç´°)
6. [ãƒ†ã‚¹ãƒˆè¨ˆç”»](#6-ãƒ†ã‚¹ãƒˆè¨ˆç”»)
7. [æˆåŠŸåˆ¤å®šåŸºæº–](#7-æˆåŠŸåˆ¤å®šåŸºæº–)
8. [ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨ˆç”»](#8-ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨ˆç”»)

---

## 1. å®Ÿè£…æ¦‚è¦

### 1.1 ç›®çš„
- **ä¸»ç›®çš„**: æ‰‹è¡“å™¨å…·ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹
- **å‰¯ç›®çš„**: ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆé®è”½ï¼‰è€æ€§ã‚’å¼·åŒ–ã™ã‚‹
- **åˆ¶ç´„**: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã¯åŸºæœ¬çš„ã«å¤‰æ›´ã—ãªã„ï¼ˆãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®ã¿æ”¹å–„ï¼‰

### 1.2 å®Ÿè£…ç¯„å›²

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | å¤‰æ›´æœ‰ç„¡ | å¤‰æ›´å†…å®¹ |
|---------------|---------|---------|
| **ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰** | âŒ ãªã— | æ—¢å­˜UIã®ã¾ã¾ |
| **APIä»•æ§˜** | âŒ ãªã— | ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ»ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ç¶­æŒ |
| **SAM2ãƒˆãƒ©ãƒƒã‚«ãƒ¼** | âœ… ã‚ã‚Š | Video APIã¸ã®åˆ‡ã‚Šæ›¿ãˆ |
| **è§£æã‚µãƒ¼ãƒ“ã‚¹** | âœ… ã‚ã‚Š | æ–°ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã®çµ±åˆ |
| **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹** | âŒ ãªã— | ã‚¹ã‚­ãƒ¼ãƒå¤‰æ›´ãªã— |

### 1.3 æœŸå¾…åŠ¹æœ

```
ã€ç¾çŠ¶ã®å•é¡Œã€‘
â”œâ”€ ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½å‡¦ç† â†’ æ™‚é–“çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—
â”œâ”€ ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³å¼±ã„ â†’ é®è”½å¾Œã®å¾©å¸°å¤±æ•—
â””â”€ IDåˆ‡ã‚Šæ›¿ã‚ã‚Š â†’ åŒä¸€å™¨å…·ãŒåˆ¥IDã¨ã—ã¦æ¤œå‡º

ã€æ”¹å–„å¾Œã€‘
â”œâ”€ ãƒ“ãƒ‡ã‚ªå…¨ä½“å‡¦ç† â†’ æ™‚é–“çš„ä¸€è²«æ€§ç¢ºä¿
â”œâ”€ ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³å¼·ã„ â†’ ãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯ã§å¾©å¸°
â””â”€ IDå®‰å®šåŒ– â†’ ä¸€è²«ã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆID
```

---

## 2. ç¾çŠ¶åˆ†æã¨èª²é¡Œ

### 2.1 ç¾è¡Œå®Ÿè£…ã®å•é¡Œç‚¹

#### å•é¡Œ1: ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã®ç‹¬ç«‹å‡¦ç†

**ç¾çŠ¶ã‚³ãƒ¼ãƒ‰ï¼ˆ`backend/app/ai_engine/processors/sam2_tracker.py`ï¼‰:**
```python
# å„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å€‹åˆ¥ã«å‡¦ç†
for frame_idx, frame in enumerate(frames):
    # å‰å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ æƒ…å ±ã‚’ä½¿ã‚ãªã„
    result = self.predictor.predict(frame, points, labels)
    results.append(result)
```

**å•é¡Œç‚¹:**
- âœ— å„ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç‹¬ç«‹ â†’ æ™‚é–“çš„ä¸€è²«æ€§ãªã—
- âœ— å‰ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒã‚¹ã‚¯ã‚’æ´»ç”¨ã§ããªã„
- âœ— ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³ç™ºç”Ÿæ™‚ã«è¿½è·¡ãƒ­ã‚¹ãƒˆ

**ãƒ‡ãƒ¼ã‚¿:**
```
ãƒ•ãƒ¬ãƒ¼ãƒ 100: å™¨å…·æ¤œå‡º âœ“ (obj_id=1)
ãƒ•ãƒ¬ãƒ¼ãƒ 101: æ‰‹ã«é®è”½ âœ— (æ¤œå‡ºå¤±æ•—)
ãƒ•ãƒ¬ãƒ¼ãƒ 102: å™¨å…·å†å‡ºç¾ âœ“ (obj_id=2) â† æ–°ã—ã„IDã¨ã—ã¦èª¤æ¤œå‡º
```

#### å•é¡Œ2: ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³è€æ€§ã®æ¬ å¦‚

**ç¾è±¡:**
```
1. å™¨å…·ãŒæ‰‹ã§ä¸€æ™‚çš„ã«éš ã‚Œã‚‹
2. æ¤œå‡ºãŒé€”åˆ‡ã‚Œã‚‹
3. å†å‡ºç¾æ™‚ã«åˆ¥ã®å™¨å…·ã¨ã—ã¦èªè­˜
4. ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDãŒå¤‰ã‚ã‚‹
5. è»Œè·¡ãŒåˆ†æ–­ã•ã‚Œã‚‹
```

**å½±éŸ¿:**
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ãŒä¸æ­£ç¢ºï¼ˆè»Œè·¡ãŒé€”åˆ‡ã‚Œã‚‹ï¼‰
- å‹•ä½œè§£æã®ä¿¡é ¼æ€§ä½ä¸‹
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã®æ‚ªåŒ–

#### å•é¡Œ3: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®éæœ€é©æ€§

**ç¾çŠ¶:**
```python
# å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒ
frames = await self._extract_frames(video_path)  # 1000ãƒ•ãƒ¬ãƒ¼ãƒ  Ã— 1920Ã—1080 Ã— 3
# â†’ ç´„6GB ãƒ¡ãƒ¢ãƒªæ¶ˆè²»
```

---

## 3. æ”¹å–„æˆ¦ç•¥

### 3.1 SAM2 Video API ã®æ´»ç”¨

#### 3.1.1 Video API ã®ç‰¹å¾´

**sam2_tracerã‹ã‚‰ã®å­¦ç¿’:**
```python
from sam2.build_sam import build_sam2_video_predictor

# ãƒ“ãƒ‡ã‚ªå…¨ä½“ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å‡¦ç†
predictor = build_sam2_video_predictor(config, checkpoint, device)
inference_state = predictor.init_state(video_path=frames_dir)

# åˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ã§å¯¾è±¡ã‚’æŒ‡å®š
predictor.add_new_points_or_box(
    inference_state=inference_state,
    frame_idx=0,  # æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿
    obj_id=0,
    points=points,
    labels=labels
)

# å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã«è‡ªå‹•ä¼æ’­
for out_frame_idx, out_obj_ids, out_mask_logits in \
        predictor.propagate_in_video(inference_state):
    # ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®æ™‚é–“çš„é–¢ä¿‚ã‚’è€ƒæ…®ã—ãŸè¿½è·¡çµæœ
    video_segments[out_frame_idx] = process_masks(out_mask_logits)
```

**ä¸»è¦ãªåˆ©ç‚¹:**

| æ©Ÿèƒ½ | èª¬æ˜ | åŠ¹æœ |
|-----|------|-----|
| **Memory Bank** | éå»ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒã‚¹ã‚¯å±¥æ­´ã‚’ä¿æŒ | ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³å¾Œã®å¾©å¸° |
| **Temporal Context** | ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®æ™‚é–“çš„é–¢ä¿‚ã‚’å­¦ç¿’ | IDå®‰å®šåŒ– |
| **Propagation** | åˆæœŸãƒã‚¹ã‚¯ã‚’å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã«ä¼æ’­ | æ‰‹å‹•æŒ‡å®šã¯æœ€åˆã ã‘ |
| **Multi-Object** | è¤‡æ•°å™¨å…·ã‚’åŒæ™‚è¿½è·¡ | åŠ¹ç‡çš„ãªä¸¦åˆ—å‡¦ç† |

#### 3.1.2 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¯”è¼ƒ

```
ã€ç¾è¡Œã€‘ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½å‡¦ç†
Frame 0 â†’ SAM2 â†’ Mask 0
Frame 1 â†’ SAM2 â†’ Mask 1  (ç‹¬ç«‹)
Frame 2 â†’ SAM2 â†’ Mask 2  (ç‹¬ç«‹)
Frame 3 â†’ SAM2 â†’ Mask 3  (ç‹¬ç«‹)
  â†‘ å„ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç‹¬ç«‹ã€å‰å¾Œé–¢ä¿‚ãªã—

ã€æ”¹å–„å¾Œã€‘ãƒ“ãƒ‡ã‚ªå…¨ä½“å‡¦ç†
Frame 0: ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®š
  â†“ SAM2 Video API (Memory Bank)
Frame 0-999: è‡ªå‹•ä¼æ’­
  - Frameé–“ã®ä¸€è²«æ€§ä¿æŒ
  - ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³è£œé–“
  - IDå®‰å®šåŒ–
```

### 3.2 å®Ÿè£…ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

#### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒA: å®Œå…¨ç½®ãæ›ãˆï¼ˆæ¨å¥¨ï¼‰

**æ–¹é‡:**
- æ—¢å­˜ã® `SAM2Tracker` ã‚’ SAM2 Video API ãƒ™ãƒ¼ã‚¹ã«æ›¸ãæ›ãˆ
- ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¯ç¶­æŒï¼ˆAPIäº’æ›æ€§ç¢ºä¿ï¼‰

**åˆ©ç‚¹:**
- âœ… ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…
- âœ… ã‚³ãƒ¼ãƒ‰é‡è¤‡ãªã—
- âœ… ä¿å®ˆæ€§é«˜ã„

#### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒB: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰

**æ–¹é‡:**
- Video API ã‚’å„ªå…ˆä½¿ç”¨
- ã‚¨ãƒ©ãƒ¼æ™‚ã«ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

**åˆ©ç‚¹:**
- âœ… å®‰å®šæ€§æœ€å„ªå…ˆ
- âœ… ã‚¨ãƒ©ãƒ¼æ™‚ã®ä¿é™º

**å®Ÿè£…:**
```python
class SAM2Tracker:
    async def track_video(self, frames, instruments):
        try:
            # Video APIï¼ˆæ–°å®Ÿè£…ï¼‰
            return await self._track_with_video_api(frames, instruments)
        except Exception as e:
            logger.warning(f"Video API failed: {e}, falling back to frame-by-frame")
            # ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½å‡¦ç†ï¼ˆæ—¢å­˜å®Ÿè£…ï¼‰
            return await self._track_frame_by_frame(frames, instruments)
```

**æ¨å¥¨**: **ã‚¢ãƒ—ãƒ­ãƒ¼ãƒAï¼ˆå®Œå…¨ç½®ãæ›ãˆï¼‰**
- å®Ÿé¨“ç‰ˆãªã®ã§æ€ã„åˆ‡ã£ãŸå¤‰æ›´ãŒå¯èƒ½
- å•é¡ŒãŒã‚ã‚Œã°å®‰å®šç‰ˆã«åˆ‡ã‚Šæˆ»ã›ã°è‰¯ã„

---

## 4. æŠ€è¡“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### 4.1 ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆå›³

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ (å¤‰æ›´ãªã—)                    â”‚
â”‚  ãƒ»å‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰                                             â”‚
â”‚  ãƒ»è§£æé–‹å§‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆ                                           â”‚
â”‚  ãƒ»WebSocketã§é€²æ—å—ä¿¡                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP/WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI Backend (backend_experimental/)         â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   AnalysisServiceV2 (è»½å¾®ãªå¤‰æ›´)                      â”‚  â”‚
â”‚  â”‚   - ãƒˆãƒ©ãƒƒã‚«ãƒ¼åˆæœŸåŒ–ãƒ­ã‚¸ãƒƒã‚¯æ›´æ–°                        â”‚  â”‚
â”‚  â”‚   - WebSocketé€šçŸ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   SAM2TrackerVideo (æ–°å®Ÿè£…) â˜…                        â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚   [ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰]                                       â”‚  â”‚
â”‚  â”‚   ãƒ»initialize_from_frames()                          â”‚  â”‚
â”‚  â”‚     - inference stateä½œæˆ                             â”‚  â”‚
â”‚  â”‚     - å™¨å…·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç™»éŒ²                              â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚   ãƒ»propagate_in_video()                              â”‚  â”‚
â”‚  â”‚     - å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½è·¡ã‚’ä¼æ’­                          â”‚  â”‚
â”‚  â”‚     - Memory Bankã§æ™‚é–“çš„ä¸€è²«æ€§ç¢ºä¿                   â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚   ãƒ»extract_trajectories()                            â”‚  â”‚
â”‚  â”‚     - è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º                                â”‚  â”‚
â”‚  â”‚     - ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰äº’æ›å½¢å¼ã«å¤‰æ›                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   SAM2 Video Predictor (sam2ãƒ©ã‚¤ãƒ–ãƒ©ãƒª)               â”‚  â”‚
â”‚  â”‚   - Memory Bankæ©Ÿæ§‹                                   â”‚  â”‚
â”‚  â”‚   - Temporal Attention                                â”‚  â”‚
â”‚  â”‚   - Multi-object tracking                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

```
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ + è§£æé–‹å§‹
   â†“
2. AnalysisServiceV2.analyze_video()
   â”œâ”€ ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º (æ—¢å­˜)
   â”œâ”€ éª¨æ ¼æ¤œå‡º (æ—¢å­˜)
   â””â”€ å™¨å…·è¿½è·¡ â˜… æ–°å®Ÿè£…
   â†“
3. SAM2TrackerVideo.initialize_from_frames()
   â”œâ”€ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆã‹ã‚‰inference stateä½œæˆ
   â”œâ”€ Frame 0ã§å™¨å…·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç™»éŒ²
   â”‚  (ãƒã‚¤ãƒ³ãƒˆ/ãƒœãƒƒã‚¯ã‚¹/ãƒã‚¹ã‚¯)
   â””â”€ Memory BankåˆæœŸåŒ–
   â†“
4. SAM2TrackerVideo.propagate_in_video()
   â”œâ”€ SAM2ãŒå…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è‡ªå‹•è¿½è·¡
   â”œâ”€ æ™‚é–“çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®
   â”œâ”€ ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³è£œé–“
   â””â”€ ä¸€è²«ã—ãŸobj_idã‚’ç¶­æŒ
   â†“
5. SAM2TrackerVideo.extract_trajectories()
   â”œâ”€ å„obj_idã®è»Œè·¡ã‚’æŠ½å‡º
   â”œâ”€ ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã¨åº§æ¨™ã®ãƒãƒƒãƒ”ãƒ³ã‚°
   â””â”€ æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«å¤‰æ›
   â†“
6. AnalysisServiceV2ãŒçµæœã‚’ä¿å­˜
   â”œâ”€ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
   â””â”€ WebSocketã§å®Œäº†é€šçŸ¥
   â†“
7. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãŒçµæœã‚’è¡¨ç¤º
   (å¤‰æ›´ãªã—ã€æ—¢å­˜UIã§è¡¨ç¤º)
```

### 4.3 ãƒ‡ãƒ¼ã‚¿æ§‹é€ è¨­è¨ˆ

#### 4.3.1 å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤‰æ›´ãªã—ï¼‰

```python
# ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
{
    "video_id": "uuid",
    "instruments": [
        {
            "id": 0,
            "name": "forceps",
            "selection": {
                "type": "point",  # or "box", "mask"
                "data": [[x, y]]  # åº§æ¨™ãƒ‡ãƒ¼ã‚¿
            },
            "color": "#00FF00"
        }
    ]
}
```

#### 4.3.2 å†…éƒ¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼ˆæ–°è¨­è¨ˆï¼‰

```python
# SAM2 inference state
{
    "video_path": "frames_dir/",
    "frame_count": 1000,
    "obj_ids": [0, 1, 2],  # è¿½è·¡ä¸­ã®å™¨å…·ID
    "memory_bank": {
        # SAM2å†…éƒ¨ã§ç®¡ç†
        "feature_maps": [...],
        "mask_history": [...]
    }
}

# è¿½è·¡çµæœï¼ˆVideo APIå‡ºåŠ›ï¼‰
{
    "frame_idx": 100,
    "obj_ids": [0, 1, 2],
    "mask_logits": [
        np.ndarray,  # obj_id=0ã®ãƒã‚¹ã‚¯
        np.ndarray,  # obj_id=1ã®ãƒã‚¹ã‚¯
        np.ndarray   # obj_id=2ã®ãƒã‚¹ã‚¯
    ]
}
```

#### 4.3.3 å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤‰æ›´ãªã—ï¼‰

```python
# ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã¸ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰
{
    "analysis_id": "uuid",
    "status": "completed",
    "results": {
        "instrument_data": [
            {
                "instrument_id": 0,
                "name": "forceps",
                "trajectory": [
                    {
                        "frame_index": 0,
                        "center": [x, y],
                        "bbox": [x1, y1, x2, y2],
                        "confidence": 0.95
                    },
                    ...
                ]
            }
        ],
        "skeleton_data": [...],  # æ—¢å­˜ã®ã¾ã¾
        "metrics": {...}         # æ—¢å­˜ã®ã¾ã¾
    }
}
```

---

## 5. å®Ÿè£…è©³ç´°

### 5.1 æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
backend_experimental/
â””â”€â”€ app/
    â””â”€â”€ ai_engine/
        â””â”€â”€ processors/
            â”œâ”€â”€ sam2_tracker_video.py        â˜… æ–°è¦ä½œæˆ
            â”œâ”€â”€ sam2_tracker.py              (æ—¢å­˜ã€å‚è€ƒç”¨ã«æ®‹ã™)
            â””â”€â”€ skeleton_detector.py         (å¤‰æ›´ãªã—)
```

### 5.2 ã‚³ã‚¢å®Ÿè£…ï¼šSAM2TrackerVideo

#### 5.2.1 ã‚¯ãƒ©ã‚¹è¨­è¨ˆ

```python
"""
backend_experimental/app/ai_engine/processors/sam2_tracker_video.py

SAM2 Video API ã‚’æ´»ç”¨ã—ãŸå™¨å…·ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°å®Ÿè£…
"""

import cv2
import numpy as np
import torch
from typing import Dict, List, Any, Optional
import logging
from pathlib import Path

from sam2.build_sam import build_sam2_video_predictor

logger = logging.getLogger(__name__)


class SAM2TrackerVideo:
    """
    SAM2 Video API ã‚’ä½¿ã£ãŸå™¨å…·ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°

    ç‰¹å¾´:
    - ãƒ“ãƒ‡ã‚ªå…¨ä½“ã®æ™‚é–“çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®
    - Memory Bankã§ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³è€æ€§
    - ä¸€è²«ã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆID
    - è¤‡æ•°å™¨å…·ã®åŒæ™‚è¿½è·¡
    """

    def __init__(
        self,
        model_type: str = "small",
        checkpoint_path: Optional[str] = None,
        device: str = "cpu"
    ):
        """
        åˆæœŸåŒ–

        Args:
            model_type: "tiny", "small", "base_plus", "large"
            checkpoint_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            device: "cpu" or "cuda"
        """
        self.model_type = model_type
        self.device = device
        self.predictor = None
        self.inference_state = None

        # GPUæ¤œå‡º
        if device == "cuda" and not torch.cuda.is_available():
            logger.warning("CUDA not available, using CPU")
            self.device = "cpu"

        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        self._load_model(checkpoint_path)

    def _load_model(self, checkpoint_path: Optional[str]):
        """SAM2 Video Predictor ã‚’ãƒ­ãƒ¼ãƒ‰"""
        if checkpoint_path is None:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹
            model_files = {
                "tiny": "sam2.1_hiera_tiny.pt",
                "small": "sam2.1_hiera_small.pt",
                "base_plus": "sam2.1_hiera_base_plus.pt",
                "large": "sam2.1_hiera_large.pt"
            }
            checkpoint_path = Path(model_files[self.model_type])
            if not checkpoint_path.exists():
                checkpoint_path = Path("backend") / checkpoint_path

        config_files = {
            "tiny": "configs/sam2.1/sam2.1_hiera_t.yaml",
            "small": "configs/sam2.1/sam2.1_hiera_s.yaml",
            "base_plus": "configs/sam2.1/sam2.1_hiera_b+.yaml",
            "large": "configs/sam2.1/sam2.1_hiera_l.yaml"
        }
        config_path = config_files[self.model_type]

        logger.info(f"Loading SAM2 Video Predictor: {self.model_type}")

        self.predictor = build_sam2_video_predictor(
            config_path,
            str(checkpoint_path),
            device=self.device
        )

        logger.info("SAM2 Video Predictor loaded successfully")

    async def track_video(
        self,
        frames: List[np.ndarray],
        instruments: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ãƒ“ãƒ‡ã‚ªå…¨ä½“ã§å™¨å…·ã‚’è¿½è·¡

        Args:
            frames: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆ
            instruments: å™¨å…·æƒ…å ±
                [
                    {
                        "id": 0,
                        "name": "forceps",
                        "selection": {
                            "type": "point" | "box" | "mask",
                            "data": [...coordinates...]
                        }
                    }
                ]

        Returns:
            è¿½è·¡çµæœ
                {
                    "instruments": [
                        {
                            "instrument_id": 0,
                            "name": "forceps",
                            "trajectory": [
                                {
                                    "frame_index": 0,
                                    "center": [x, y],
                                    "bbox": [x1, y1, x2, y2],
                                    "confidence": 0.95,
                                    "mask": np.ndarray
                                },
                                ...
                            ]
                        }
                    ]
                }
        """
        logger.info(f"Starting video tracking: {len(frames)} frames, {len(instruments)} instruments")

        # 1. Inference stateåˆæœŸåŒ–
        self._initialize_state(frames)

        # 2. åˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆFrame 0ï¼‰ã§å™¨å…·ã‚’ç™»éŒ²
        self._register_instruments(instruments)

        # 3. å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½è·¡ã‚’ä¼æ’­
        video_segments = self._propagate_tracking()

        # 4. è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        trajectories = self._extract_trajectories(video_segments, instruments)

        logger.info("Video tracking completed")

        return {"instruments": trajectories}

    def _initialize_state(self, frames: List[np.ndarray]):
        """Inference stateã‚’åˆæœŸåŒ–"""
        logger.info("Initializing inference state...")

        with torch.inference_mode():
            # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆã‹ã‚‰stateã‚’ä½œæˆ
            # SAM2ã¯ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¾ãŸã¯ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆã‚’å—ã‘å…¥ã‚Œã‚‹
            self.inference_state = self.predictor.init_state(
                video_path=frames,
                async_loading_frames=False
            )

        logger.info(f"Inference state initialized with {len(frames)} frames")

    def _register_instruments(self, instruments: List[Dict[str, Any]]):
        """åˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ã§å™¨å…·ã‚’ç™»éŒ²"""
        logger.info(f"Registering {len(instruments)} instruments...")

        with torch.inference_mode():
            for inst in instruments:
                obj_id = inst["id"]
                selection = inst.get("selection", {})
                sel_type = selection.get("type")
                sel_data = selection.get("data")

                if sel_type == "point":
                    # ãƒã‚¤ãƒ³ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                    points = np.array(sel_data, dtype=np.float32)
                    labels = np.ones(len(points), dtype=np.int32)

                    self.predictor.add_new_points_or_box(
                        inference_state=self.inference_state,
                        frame_idx=0,
                        obj_id=obj_id,
                        points=points,
                        labels=labels
                    )
                    logger.info(f"Registered instrument {obj_id} with {len(points)} points")

                elif sel_type == "box":
                    # ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                    box = np.array(sel_data, dtype=np.float32)

                    self.predictor.add_new_points_or_box(
                        inference_state=self.inference_state,
                        frame_idx=0,
                        obj_id=obj_id,
                        box=box
                    )
                    logger.info(f"Registered instrument {obj_id} with box")

                elif sel_type == "mask":
                    # ãƒã‚¹ã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæœ€ã‚‚æ­£ç¢ºï¼‰
                    mask = np.array(sel_data, dtype=np.uint8)

                    self.predictor.add_new_mask(
                        inference_state=self.inference_state,
                        frame_idx=0,
                        obj_id=obj_id,
                        mask=mask
                    )
                    logger.info(f"Registered instrument {obj_id} with mask")

                else:
                    logger.warning(f"Unknown selection type: {sel_type}")

        logger.info("All instruments registered")

    def _propagate_tracking(self) -> Dict[int, Dict[int, np.ndarray]]:
        """å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½è·¡ã‚’ä¼æ’­"""
        logger.info("Propagating tracking across video...")

        video_segments = {}
        frame_count = 0

        with torch.inference_mode():
            # SAM2ãŒå…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è‡ªå‹•è¿½è·¡
            for out_frame_idx, out_obj_ids, out_mask_logits in \
                    self.predictor.propagate_in_video(self.inference_state):

                # ãƒã‚¹ã‚¯ã‚’ãƒã‚¤ãƒŠãƒªåŒ–
                masks = {
                    obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()
                    for i, obj_id in enumerate(out_obj_ids)
                }

                video_segments[out_frame_idx] = masks
                frame_count += 1

                # é€²æ—ãƒ­ã‚°ï¼ˆ100ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ï¼‰
                if frame_count % 100 == 0:
                    logger.info(f"Processed {frame_count} frames")

        logger.info(f"Tracking propagated to {frame_count} frames")
        return video_segments

    def _extract_trajectories(
        self,
        video_segments: Dict[int, Dict[int, np.ndarray]],
        instruments: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        logger.info("Extracting trajectories...")

        trajectories = []

        for inst in instruments:
            obj_id = inst["id"]
            trajectory = []

            for frame_idx in sorted(video_segments.keys()):
                masks = video_segments[frame_idx]

                if obj_id not in masks:
                    # ã“ã®å™¨å…·ãŒæ¤œå‡ºã•ã‚Œãªã‹ã£ãŸãƒ•ãƒ¬ãƒ¼ãƒ 
                    continue

                mask = masks[obj_id]

                # ãƒã‚¹ã‚¯ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
                if mask.sum() == 0:
                    # ç©ºã®ãƒã‚¹ã‚¯
                    continue

                # é‡å¿ƒè¨ˆç®—
                y_coords, x_coords = np.where(mask)
                center_x = float(np.mean(x_coords))
                center_y = float(np.mean(y_coords))

                # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹
                x_min, x_max = float(x_coords.min()), float(x_coords.max())
                y_min, y_max = float(y_coords.min()), float(y_coords.max())

                # ä¿¡é ¼åº¦ï¼ˆãƒã‚¹ã‚¯ã®é¢ç©ã‹ã‚‰æ¨å®šï¼‰
                area = mask.sum()
                confidence = min(1.0, area / (mask.shape[0] * mask.shape[1]))

                trajectory.append({
                    "frame_index": int(frame_idx),
                    "center": [center_x, center_y],
                    "bbox": [x_min, y_min, x_max, y_max],
                    "confidence": float(confidence),
                    "mask": mask  # å¿…è¦ã«å¿œã˜ã¦ä¿å­˜
                })

            trajectories.append({
                "instrument_id": obj_id,
                "name": inst.get("name", f"instrument_{obj_id}"),
                "trajectory": trajectory
            })

            logger.info(f"Extracted trajectory for {inst['name']}: {len(trajectory)} frames")

        return trajectories
```

### 5.3 çµ±åˆï¼šAnalysisServiceV2ã®ä¿®æ­£

```python
# backend_experimental/app/services/analysis_service_v2.py

from app.ai_engine.processors.sam2_tracker_video import SAM2TrackerVideo

class AnalysisServiceV2:

    async def _run_detection(
        self,
        frames: List[np.ndarray],
        video_type: str,
        video_id: str,
        instruments: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """æ¤œå‡ºå‡¦ç†ï¼ˆå™¨å…·è¿½è·¡ã‚’æ–°å®Ÿè£…ã«å¤‰æ›´ï¼‰"""

        detection_results = {
            "skeleton_data": [],
            "instrument_data": []
        }

        # 1. éª¨æ ¼æ¤œå‡ºï¼ˆæ—¢å­˜ã®ã¾ã¾ï¼‰
        if video_type in ["external", "external_no_instruments", "external_with_instruments"]:
            detector = HandSkeletonDetector(...)
            for frame in frames:
                result = detector.detect_from_frame(frame)
                detection_results["skeleton_data"].append(result)

        # 2. å™¨å…·è¿½è·¡ï¼ˆæ–°å®Ÿè£…ï¼‰â˜…
        if video_type in ["internal", "external_with_instruments"] and instruments:
            logger.info("[EXPERIMENTAL] Using SAM2 Video API for instrument tracking")

            # SAM2 Video TrackeråˆæœŸåŒ–
            tracker = SAM2TrackerVideo(
                model_type="small",  # or from config
                device="cpu"         # or "cuda" if available
            )

            # ãƒ“ãƒ‡ã‚ªå…¨ä½“ã‚’è¿½è·¡
            tracking_result = await tracker.track_video(frames, instruments)

            detection_results["instrument_data"] = tracking_result["instruments"]

            logger.info(f"Tracked {len(tracking_result['instruments'])} instruments")

        return detection_results
```

---

## 6. ãƒ†ã‚¹ãƒˆè¨ˆç”»

### 6.1 ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ

```python
# backend_experimental/tests/test_sam2_tracker_video.py

import pytest
import numpy as np
from app.ai_engine.processors.sam2_tracker_video import SAM2TrackerVideo


class TestSAM2TrackerVideo:

    @pytest.fixture
    def tracker(self):
        return SAM2TrackerVideo(model_type="tiny", device="cpu")

    @pytest.fixture
    def sample_frames(self):
        # 10ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
        return [np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8) for _ in range(10)]

    @pytest.fixture
    def sample_instruments(self):
        return [
            {
                "id": 0,
                "name": "forceps",
                "selection": {
                    "type": "point",
                    "data": [[320, 240]]  # ä¸­å¤®ä»˜è¿‘
                }
            }
        ]

    @pytest.mark.asyncio
    async def test_initialization(self, tracker):
        """åˆæœŸåŒ–ãŒæ­£å¸¸ã«å®Œäº†ã™ã‚‹ã‹"""
        assert tracker.predictor is not None
        assert tracker.model_type == "tiny"

    @pytest.mark.asyncio
    async def test_track_video(self, tracker, sample_frames, sample_instruments):
        """ãƒ“ãƒ‡ã‚ªè¿½è·¡ãŒå‹•ä½œã™ã‚‹ã‹"""
        result = await tracker.track_video(sample_frames, sample_instruments)

        assert "instruments" in result
        assert len(result["instruments"]) == 1
        assert result["instruments"][0]["instrument_id"] == 0
        assert len(result["instruments"][0]["trajectory"]) > 0

    @pytest.mark.asyncio
    async def test_trajectory_format(self, tracker, sample_frames, sample_instruments):
        """è»Œè·¡ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ã„å½¢å¼ã‹"""
        result = await tracker.track_video(sample_frames, sample_instruments)

        trajectory = result["instruments"][0]["trajectory"]
        first_point = trajectory[0]

        assert "frame_index" in first_point
        assert "center" in first_point
        assert "bbox" in first_point
        assert "confidence" in first_point
        assert isinstance(first_point["center"], list)
        assert len(first_point["center"]) == 2
        assert 0.0 <= first_point["confidence"] <= 1.0
```

### 6.2 çµ±åˆãƒ†ã‚¹ãƒˆ

```python
# backend_experimental/tests/test_analysis_integration.py

import pytest
from app.services.analysis_service_v2 import AnalysisServiceV2
from app.models import SessionLocal, Video, AnalysisResult


@pytest.mark.asyncio
async def test_full_analysis_with_sam2_video():
    """å®Ÿéš›ã®å‹•ç”»ã§å®Œå…¨ãªè§£æãƒ•ãƒ­ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆ"""

    # ãƒ†ã‚¹ãƒˆå‹•ç”»ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    service = AnalysisServiceV2()

    # è§£æå®Ÿè¡Œ
    result = await service.analyze_video(
        video_id="test_video_id",
        analysis_id="test_analysis_id",
        instruments=[
            {
                "id": 0,
                "name": "test_instrument",
                "selection": {
                    "type": "point",
                    "data": [[100, 100]]
                }
            }
        ]
    )

    # çµæœæ¤œè¨¼
    assert result["status"] == "success"
    assert "detection_results" in result
    assert "instrument_data" in result["detection_results"]

    # è»Œè·¡ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã‹
    instruments = result["detection_results"]["instrument_data"]
    assert len(instruments) > 0
    assert len(instruments[0]["trajectory"]) > 0
```

### 6.3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

```python
# scripts/benchmark_sam2_video.py

import time
import numpy as np
from app.ai_engine.processors.sam2_tracker_video import SAM2TrackerVideo


async def benchmark():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š"""

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    frames = [np.random.randint(0, 255, (720, 1280, 3), dtype=np.uint8) for _ in range(100)]
    instruments = [{"id": 0, "name": "test", "selection": {"type": "point", "data": [[640, 360]]}}]

    tracker = SAM2TrackerVideo(model_type="small", device="cpu")

    # æ¸¬å®šé–‹å§‹
    start = time.time()
    result = await tracker.track_video(frames, instruments)
    elapsed = time.time() - start

    print(f"Processing time: {elapsed:.2f}s")
    print(f"Frames per second: {len(frames) / elapsed:.2f}")
    print(f"Trajectory points: {len(result['instruments'][0]['trajectory'])}")


if __name__ == "__main__":
    import asyncio
    asyncio.run(benchmark())
```

---

## 7. æˆåŠŸåˆ¤å®šåŸºæº–

### 7.1 å®šé‡çš„æŒ‡æ¨™

| æŒ‡æ¨™ | ç›®æ¨™å€¤ | æ¸¬å®šæ–¹æ³• |
|-----|--------|----------|
| **ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ç²¾åº¦** | > 85% | æ‰‹å‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã®ä¸€è‡´ç‡ |
| **IDå®‰å®šæ€§** | > 95% | åŒä¸€å™¨å…·ãŒåŒä¸€IDã§è¿½è·¡ã•ã‚Œã‚‹å‰²åˆ |
| **ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³å¾©å¸°** | > 80% | é®è”½å¾Œã®å†æ¤œå‡ºæˆåŠŸç‡ |
| **å‡¦ç†æ™‚é–“** | < 2å€ | å®‰å®šç‰ˆã¨ã®æ¯”è¼ƒ |
| **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** | < 1.5å€ | å®‰å®šç‰ˆã¨ã®æ¯”è¼ƒ |

### 7.2 å®šæ€§çš„è©•ä¾¡

```markdown
## è©•ä¾¡ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### ç²¾åº¦
- [ ] å™¨å…·ãŒæ­£ã—ãè¿½è·¡ã•ã‚Œã¦ã„ã‚‹
- [ ] é®è”½å¾Œã‚‚è¿½è·¡ãŒç¶™ç¶šã—ã¦ã„ã‚‹
- [ ] IDãŒé€”ä¸­ã§å¤‰ã‚ã‚‰ãªã„
- [ ] è»Œè·¡ãŒæ»‘ã‚‰ã‹ã§ã‚ã‚‹

### å®‰å®šæ€§
- [ ] ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãªã„
- [ ] å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã§å‡¦ç†ãŒå®Œäº†ã™ã‚‹
- [ ] ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãªã„

### ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“
- [ ] å‡¦ç†æ™‚é–“ãŒè¨±å®¹ç¯„å›²å†…
- [ ] çµæœã®å¯è¦–åŒ–ãŒæ­£å¸¸
- [ ] ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§å•é¡Œãªãè¡¨ç¤º
```

### 7.3 æ¯”è¼ƒãƒ†ã‚¹ãƒˆ

```bash
# åŒã˜å‹•ç”»ã§ä¸¡ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ãƒ†ã‚¹ãƒˆ

# å®‰å®šç‰ˆ
curl -X POST http://localhost:8000/api/v1/analysis/{video_id}/analyze

# å®Ÿé¨“ç‰ˆ
curl -X POST http://localhost:8001/api/v1/analysis/{video_id}/analyze

# çµæœã‚’æ¯”è¼ƒ
python scripts/compare_versions.py --stable-id xxx --experimental-id yyy
```

---

## 8. ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨ˆç”»

### 8.1 å³åº§ã®åˆ‡ã‚Šæˆ»ã—ï¼ˆ10ç§’ä»¥å†…ï¼‰

```bash
# ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®æ¥ç¶šå…ˆã‚’å®‰å®šç‰ˆã«æˆ»ã™
cd frontend
copy /Y .env.local.stable .env.local

# ãƒ–ãƒ©ã‚¦ã‚¶ã‚’ãƒªãƒ­ãƒ¼ãƒ‰
```

### 8.2 å•é¡Œç™ºç”Ÿæ™‚ã®å¯¾å¿œãƒ•ãƒ­ãƒ¼

```
1. å•é¡Œæ¤œçŸ¥
   â”œâ”€ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç¢ºèª
   â”œâ”€ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
   â””â”€ å‡¦ç†æ™‚é–“ç¢ºèª

2. ç·Šæ€¥åº¦åˆ¤å®š
   â”œâ”€ [é«˜] ã‚·ã‚¹ãƒ†ãƒ ãƒ€ã‚¦ãƒ³ â†’ å³åº§ã«åˆ‡ã‚Šæˆ»ã—
   â”œâ”€ [ä¸­] ç²¾åº¦ä½ä¸‹ â†’ ãƒ‡ãƒ¼ã‚¿åé›†å¾Œã«åˆ¤æ–­
   â””â”€ [ä½] å‡¦ç†æ™‚é–“å¢— â†’ è¨±å®¹ç¯„å›²å†…ã‹ç¢ºèª

3. åˆ‡ã‚Šæˆ»ã—å®Ÿè¡Œ
   â”œâ”€ ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰è¨­å®šå¤‰æ›´
   â”œâ”€ å®‰å®šç‰ˆã¸ã®æ¥ç¶šç¢ºèª
   â””â”€ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®é€šçŸ¥

4. åŸå› èª¿æŸ»
   â”œâ”€ ãƒ­ã‚°åˆ†æ
   â”œâ”€ ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œ
   â””â”€ ä¿®æ­£è¨ˆç”»ç«‹æ¡ˆ
```

### 8.3 ãƒ‡ãƒ¼ã‚¿ä¿å…¨

```python
# å®Ÿé¨“ç‰ˆã§ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã¯åˆ¥DBã«ä¿å­˜
# aimotion_experimental.db

# å®‰å®šç‰ˆã«æˆ»ã—ã¦ã‚‚æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¯å½±éŸ¿ãªã—
# aimotion.dbï¼ˆå®‰å®šç‰ˆã®ãƒ‡ãƒ¼ã‚¿ï¼‰
```

---

## 9. å®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

### ãƒ•ã‚§ãƒ¼ã‚º1: ç’°å¢ƒæ§‹ç¯‰ï¼ˆ1æ—¥ï¼‰

- [ ] backend_experimental/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
- [ ] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£ï¼ˆãƒãƒ¼ãƒˆã€DBï¼‰
- [ ] èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
- [ ] ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰åˆ‡ã‚Šæ›¿ãˆãƒ„ãƒ¼ãƒ«ä½œæˆ

### ãƒ•ã‚§ãƒ¼ã‚º2: ã‚³ã‚¢å®Ÿè£…ï¼ˆ2-3æ—¥ï¼‰

- [ ] SAM2TrackerVideoå®Ÿè£…
  - [ ] åˆæœŸåŒ–å‡¦ç†
  - [ ] inference stateä½œæˆ
  - [ ] å™¨å…·ç™»éŒ²
  - [ ] ä¼æ’­å‡¦ç†
  - [ ] è»Œè·¡æŠ½å‡º
- [ ] AnalysisServiceV2çµ±åˆ
- [ ] ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆä½œæˆ

### ãƒ•ã‚§ãƒ¼ã‚º3: ãƒ†ã‚¹ãƒˆï¼ˆ1-2æ—¥ï¼‰

- [ ] ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- [ ] çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
- [ ] å®Ÿéš›ã®æ‰‹è¡“å‹•ç”»ã§ãƒ†ã‚¹ãƒˆ

### ãƒ•ã‚§ãƒ¼ã‚º4: è©•ä¾¡ï¼ˆ1æ—¥ï¼‰

- [ ] ç²¾åº¦æ¸¬å®š
- [ ] å‡¦ç†æ™‚é–“æ¸¬å®š
- [ ] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š
- [ ] æˆåŠŸåˆ¤å®šåŸºæº–ã¨ã®ç…§åˆ

### ãƒ•ã‚§ãƒ¼ã‚º5: æœ¬ç•ªæŠ•å…¥åˆ¤æ–­ï¼ˆ1æ—¥ï¼‰

- [ ] è©•ä¾¡çµæœã®ãƒ¬ãƒ“ãƒ¥ãƒ¼
- [ ] Go/No-Goåˆ¤æ–­
- [ ] æœ¬ç•ªæŠ•å…¥ or ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯

**åˆè¨ˆ: 5-8æ—¥**

---

## 10. ãƒªã‚¹ã‚¯ç®¡ç†

### 10.1 æŠ€è¡“çš„ãƒªã‚¹ã‚¯

| ãƒªã‚¹ã‚¯ | å½±éŸ¿åº¦ | ç™ºç”Ÿç¢ºç‡ | å¯¾ç­– |
|-------|--------|---------|------|
| SAM2ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®äº’æ›æ€§å•é¡Œ | é«˜ | ä½ | äº‹å‰ã«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèªã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£… |
| ãƒ¡ãƒ¢ãƒªä¸è¶³ | é«˜ | ä¸­ | ãƒ•ãƒ¬ãƒ¼ãƒ æ•°åˆ¶é™ã€ãƒãƒƒãƒå‡¦ç† |
| å‡¦ç†æ™‚é–“è¶…é | ä¸­ | ä¸­ | ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã€éåŒæœŸå‡¦ç† |
| GPU/CPUæ€§èƒ½å·® | ä¸­ | ä½ | ãƒ‡ãƒã‚¤ã‚¹è‡ªå‹•æ¤œå‡ºã€æœ€é©åŒ– |

### 10.2 é‹ç”¨ãƒªã‚¹ã‚¯

| ãƒªã‚¹ã‚¯ | å½±éŸ¿åº¦ | ç™ºç”Ÿç¢ºç‡ | å¯¾ç­– |
|-------|--------|---------|------|
| å®‰å®šç‰ˆã¸ã®åˆ‡ã‚Šæˆ»ã—å¤±æ•— | é«˜ | æ¥µä½ | æ‰‹é †æ›¸æ•´å‚™ã€å®šæœŸçš„ãªåˆ‡ã‚Šæˆ»ã—ãƒ†ã‚¹ãƒˆ |
| ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆ | ä¸­ | ä½ | åˆ¥DBã§ç®¡ç†ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— |
| ãƒ¦ãƒ¼ã‚¶ãƒ¼æ··ä¹± | ä½ | ä¸­ | ç’°å¢ƒãƒãƒƒã‚¸è¡¨ç¤ºã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™ |

---

## 11. ä»˜éŒ²

### 11.1 å‚è€ƒè³‡æ–™

- [SAM2 å…¬å¼ãƒªãƒã‚¸ãƒˆãƒª](https://github.com/facebookresearch/sam2)
- [SAM2 è«–æ–‡](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/)
- sam2_tracerå®Ÿè£…ï¼ˆå‚è€ƒã‚³ãƒ¼ãƒ‰ï¼‰

### 11.2 ç”¨èªé›†

| ç”¨èª | èª¬æ˜ |
|-----|------|
| **Video API** | SAM2ã®ãƒ“ãƒ‡ã‚ªè¿½è·¡ç”¨APIï¼ˆ`build_sam2_video_predictor`ï¼‰ |
| **inference state** | ãƒ“ãƒ‡ã‚ªå…¨ä½“ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿æŒã™ã‚‹çŠ¶æ…‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ |
| **Memory Bank** | éå»ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒã‚¹ã‚¯æƒ…å ±ã‚’ä¿æŒã™ã‚‹æ©Ÿæ§‹ |
| **Propagation** | åˆæœŸãƒã‚¹ã‚¯ã‚’å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã«ä¼æ’­ã•ã›ã‚‹å‡¦ç† |
| **ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³** | å™¨å…·ãŒæ‰‹ãªã©ã§ä¸€æ™‚çš„ã«éš ã‚Œã‚‹ç¾è±¡ |

---

**æ›´æ–°å±¥æ­´:**
- 2025-10-11: åˆç‰ˆä½œæˆ
