"""
CUDAç’°å¢ƒã®æ¤œè¨¼ï¼ˆRTX 3060å¯¾å¿œï¼‰
"""
import sys

print("=" * 70)
print("CUDAç’°å¢ƒãƒã‚§ãƒƒã‚¯ï¼ˆRTX 3060ï¼‰")
print("=" * 70)
print()

# PyTorchã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
try:
    import torch
    print(f"âœ… PyTorch imported successfully")
    print(f"   Version: {torch.__version__}")
except ImportError as e:
    print(f"âŒ PyTorch not installed: {e}")
    print("   Please install: pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
    sys.exit(1)

print()
print("=" * 70)
print("CUDA Availability")
print("=" * 70)

if not torch.cuda.is_available():
    print("âŒ CUDA is NOT available")
    print()
    print("Possible causes:")
    print("1. PyTorch CPUç‰ˆãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹")
    print("   è§£æ±ºç­–: pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
    print("2. NVIDIA ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãŒå¤ã„")
    print("   è§£æ±ºç­–: https://www.nvidia.com/Download/index.aspx ã‹ã‚‰ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’æ›´æ–°")
    print("3. CUDA ToolkitãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„")
    print("   è§£æ±ºç­–: PyTorch CUDAç‰ˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆä¸Šè¨˜ã‚³ãƒãƒ³ãƒ‰ï¼‰")
    print()
    sys.exit(1)

print(f"âœ… CUDA is available")
print(f"   CUDA version (compiled): {torch.version.cuda}")
print(f"   cuDNN version: {torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else 'N/A'}")
print()

print("=" * 70)
print("GPU Information")
print("=" * 70)

gpu_count = torch.cuda.device_count()
print(f"GPU count: {gpu_count}")
print()

for i in range(gpu_count):
    props = torch.cuda.get_device_properties(i)
    print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
    print(f"   Total VRAM: {props.total_memory / 1024**3:.2f} GB")
    print(f"   CUDA Compute Capability: {props.major}.{props.minor}")
    print(f"   Multi-processors: {props.multi_processor_count}")
    print(f"   Max threads per block: {props.max_threads_per_block}")
    print()

# ç¾åœ¨ã®GPU
current_device = torch.cuda.current_device()
print(f"Current GPU: {current_device} ({torch.cuda.get_device_name(current_device)})")
print()

print("=" * 70)
print("VRAM Status")
print("=" * 70)

allocated = torch.cuda.memory_allocated() / 1024**2
reserved = torch.cuda.memory_reserved() / 1024**2
total = torch.cuda.get_device_properties(0).total_memory / 1024**2

print(f"Allocated: {allocated:.1f} MB")
print(f"Reserved: {reserved:.1f} MB")
print(f"Total: {total:.1f} MB ({total/1024:.2f} GB)")
print(f"Free: {total - reserved:.1f} MB")
print()

print("=" * 70)
print("GPU Test")
print("=" * 70)

try:
    # ãƒ†ã‚¹ãƒˆãƒ†ãƒ³ã‚½ãƒ«ã‚’GPUã«é…ç½®
    test_tensor = torch.randn(1000, 1000).cuda()
    result = torch.matmul(test_tensor, test_tensor)

    print(f"âœ… GPU computation test passed")
    print(f"   Test tensor device: {test_tensor.device}")
    print(f"   Test tensor shape: {test_tensor.shape}")
    print(f"   Computation result shape: {result.shape}")

    # VRAMä½¿ç”¨é‡ç¢ºèª
    allocated_after = torch.cuda.memory_allocated() / 1024**2
    print(f"   VRAM used by test: {allocated_after - allocated:.1f} MB")

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    del test_tensor, result
    torch.cuda.empty_cache()

except Exception as e:
    print(f"âŒ GPU computation test failed: {e}")
    sys.exit(1)

print()
print("=" * 70)
print("SAM vit_h Requirements Check")
print("=" * 70)

total_vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
required_vram_gb = 3.0  # SAM vit_hæ¨å¥¨

print(f"Total VRAM: {total_vram_gb:.2f} GB")
print(f"Required for SAM vit_h: {required_vram_gb:.1f} GB")

if total_vram_gb >= required_vram_gb:
    print(f"âœ… Sufficient VRAM for SAM vit_h")
    print(f"   ä½™è£•: {total_vram_gb - required_vram_gb:.2f} GB")
else:
    print(f"âš ï¸ VRAM may be insufficient")
    print(f"   ä¸è¶³: {required_vram_gb - total_vram_gb:.2f} GB")
    print(f"   æ¨å¥¨: vit_l ã¾ãŸã¯ vit_b ã‚’ä½¿ç”¨")

print()
print("=" * 70)
print("ğŸ‰ CUDAç’°å¢ƒã¯æ­£å¸¸ã§ã™ï¼")
print("   RTX 3060ã§SAM vit_h GPUã‚’å®Ÿè¡Œå¯èƒ½")
print("=" * 70)
